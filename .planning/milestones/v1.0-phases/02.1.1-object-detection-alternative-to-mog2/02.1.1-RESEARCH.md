# Phase 02.1.1: Object-detection alternative to MOG2 - Research

**Researched:** 2026-02-20
**Domain:** YOLOv8 object detection training, Label Studio annotation workflow, pipeline integration
**Confidence:** HIGH

<user_constraints>
## User Constraints (from CONTEXT.md)

### Locked Decisions

**Detector choice:**
- YOLOv8 (nano or small variant) via ultralytics
- Bounding-box detection only (not instance segmentation)
- Fine-tune from COCO pretrained weights — no zero-shot baseline needed
- Single-class detection: "fish"

**Training data strategy:**
- 150 manually annotated frames with bounding boxes
- Camera distribution: 10 frames per ring camera (12 × 10 = 120) + 30 frames from center camera
- MOG2-guided frame sampling: pick frames with varying fish counts (0, 1, 2, many) to ensure hard cases are represented
- Annotation tool: Label Studio (already set up from Phase 02.1)
- Train/val split stratified by camera: 80% train / 20% val per camera (8 train + 2 val per ring, 24 train + 6 val center → 120 train / 30 val total)

**Pipeline integration:**
- MOG2 and YOLO are runtime-configurable alternatives — user chooses one or the other
- YOLODetector class lives in `segmentation/detector.py` alongside MOG2Detector, sharing the Detection dataclass
- YOLO boxes feed into SAM2 as box prompts for mask generation (same role MOG2 plays today)
- Full pipeline wiring included in this phase: YOLO → SAM2 → Mask R-CNN path must be usable end-to-end

**Evaluation criteria:**
- Primary metrics: recall and precision (both tracked, recall is most important)
- Success bar: YOLO must beat MOG2's recall on the same val set (relative comparison, not absolute threshold)
- Visual comparison: side-by-side annotated frames showing YOLO boxes vs MOG2 boxes on the same frames
- Evaluation on stratified val set (30 frames, 2 per ring camera + 6 center)

### Claude's Discretion

- Exact YOLO model variant (v8n vs v8s) based on training results. Start with v8n
- Training hyperparameters (epochs, augmentation, learning rate)
- MOG2-guided sampling algorithm (how to select diverse frames)
- Frame export script design for Label Studio import
- Runtime configuration mechanism (config file, CLI flag, etc.)

### Deferred Ideas (OUT OF SCOPE)

None — discussion stayed within phase scope
</user_constraints>

---

## Summary

This phase adds YOLOv8 (via the `ultralytics` package) as a runtime-configurable alternative to MOG2 for first-stage fish localization. The work breaks into four sequential activities: (1) frame sampling and annotation, (2) YOLO training and evaluation, (3) `YOLODetector` class implementation wired into `segmentation/detector.py`, and (4) a diagnostic script that runs the full YOLO → SAM2 path end-to-end and compares against MOG2.

The codebase is already well-structured for this addition. The `Detection` dataclass in `detector.py` was intentionally designed with a `confidence` placeholder to accommodate downstream detectors. The `SAMPseudoLabeler.predict()` method already accepts `list[Detection]` and uses `det.bbox` as a box prompt — so YOLO detections slot directly in. The main new work is: YOLO dataset construction (YOLO-format labels from Label Studio bounding boxes), training, `YOLODetector` class, and a comparison eval script.

Label Studio already exports bounding boxes in YOLO text format (one `.txt` per image, normalized `class x_center y_center w h` on each line). This means the existing Label Studio workflow extends cleanly — the user annotates bounding boxes in Label Studio (using a RectangleLabels tag) and exports in YOLO format, which `model.train()` consumes directly via a dataset YAML.

**Primary recommendation:** Use `ultralytics` `YOLO('yolov8n.pt')` as the base, train on 150 frames with the standard YOLO dataset directory layout, add `YOLODetector` to `detector.py` following the `MOG2Detector` interface contract, and gate the choice between detectors via a string argument (`detector="mog2"` or `detector="yolo"`).

---

## Standard Stack

### Core

| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| ultralytics | >=8.0 (current stable ~8.3) | YOLOv8 training, inference, validation | Official Ultralytics package; single unified API for train/val/predict |
| PyTorch | >=2.0 (already in env) | Backend for YOLO | Already a project dependency; YOLO uses torch internally |
| opencv-python | >=4.8 (already in env) | Frame extraction from video | Already a project dependency |

### Supporting

| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| PyYAML | bundled with ultralytics | Dataset config YAML | Required by `model.train(data=...)` |
| label-studio-converter | already in env | Export/import Label Studio annotations | For bounding box export to YOLO format |

### Alternatives Considered

| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| ultralytics YOLOv8n | RT-DETR, YOLOv5, DETR | ultralytics is locked by user decision; YOLOv8n has better small-object recall than YOLOv5n at this scale |
| Label Studio bbox annotation | Roboflow, CVAT | Label Studio already set up in project from Phase 02.1; no switching cost |

**Installation (not yet in pyproject.toml):**
```bash
pip install ultralytics
```

Note: Add `"ultralytics"` to `pyproject.toml` `dependencies` array when implementing. The `ultralytics` package brings in PyYAML, Pillow, matplotlib, seaborn, tqdm — all acceptable additions.

---

## Architecture Patterns

### YOLO Dataset Directory Layout

The `ultralytics` trainer expects this directory structure:

```
data/yolo_fish/
├── dataset.yaml          # Dataset config consumed by model.train(data=...)
├── images/
│   ├── train/            # 120 training images (.jpg)
│   └── val/              # 30 val images (.jpg)
└── labels/
    ├── train/            # 120 label .txt files (parallel to images/train/)
    └── val/              # 30 label .txt files (parallel to images/val/)
```

The `dataset.yaml` for single-class fish detection:
```yaml
path: /absolute/path/to/data/yolo_fish  # absolute path required
train: images/train
val: images/val
nc: 1
names:
  0: fish
```

Each label `.txt` file has one line per fish bounding box:
```
0 x_center y_center width height
```
All values normalized to [0, 1] by image dimensions. Empty `.txt` files represent frames with no fish (negative examples).

### Pattern 1: Label Studio Bounding Box Export to YOLO Format

**What:** Label Studio exports YOLO-format annotations directly. The user creates a Label Studio project with `RectangleLabels` tag (not `BrushLabels` used for masks), annotates bounding boxes, then exports as "YOLO" format. The download contains:
- `classes.txt` (one class name per line)
- `images/` directory (source images)
- `labels/` directory (one `.txt` per image in YOLO format)

**When to use:** This is the primary annotation path for this phase. No custom conversion script needed — Label Studio handles the normalization.

**Key detail:** Label Studio's YOLO export uses the `label-studio-converter` under the hood, which is already installed. The export is triggered from the Label Studio web UI: Export → YOLO format.

### Pattern 2: YOLODetector Class Interface

**What:** `YOLODetector` must implement the same interface as `MOG2Detector` so it is interchangeable for downstream SAM2 prompting.

```python
# Source: analysis of existing detector.py and pseudo_labeler.py

class YOLODetector:
    """Fish detector using YOLOv8 object detection.

    Args:
        model_path: Path to trained YOLOv8 .pt weights.
        conf_threshold: Minimum confidence score (default 0.25).
        padding_fraction: Fraction of bbox dimension to add as padding.
    """

    def __init__(
        self,
        model_path: str | Path,
        conf_threshold: float = 0.25,
        padding_fraction: float = 0.15,
    ) -> None:
        from ultralytics import YOLO
        self._model = YOLO(str(model_path))
        self._conf = conf_threshold
        self._padding_fraction = padding_fraction

    def detect(self, frame: np.ndarray) -> list[Detection]:
        """Detect fish in a frame, returning bounding boxes.

        Args:
            frame: BGR image as uint8 array of shape (H, W, 3).

        Returns:
            List of Detection objects, one per fish.
        """
        h_frame, w_frame = frame.shape[:2]
        results = self._model.predict(
            frame, conf=self._conf, verbose=False
        )
        detections: list[Detection] = []
        for r in results:
            for box in r.boxes:
                x1, y1, x2, y2 = box.xyxy[0].tolist()
                conf = float(box.conf[0])
                # Pad bbox
                bw, bh = x2 - x1, y2 - y1
                pad_x = bw * self._padding_fraction
                pad_y = bh * self._padding_fraction
                rx1 = max(0, int(x1 - pad_x))
                ry1 = max(0, int(y1 - pad_y))
                rx2 = min(w_frame, int(x2 + pad_x))
                ry2 = min(h_frame, int(y2 + pad_y))
                bbox = (rx1, ry1, rx2 - rx1, ry2 - ry1)
                # Build full-frame mask from bbox (for SAM2 compatibility)
                mask = np.zeros((h_frame, w_frame), dtype=np.uint8)
                mask[ry1:ry2, rx1:rx2] = 255
                area = (rx2 - rx1) * (ry2 - ry1)
                detections.append(
                    Detection(bbox=bbox, mask=mask, area=area, confidence=conf)
                )
        return detections
```

**Key compatibility note:** `SAMPseudoLabeler.predict()` uses `det.bbox` (as box prompt) and `det.mask` (as mask logit prompt). `YOLODetector` must populate both fields. The `mask` for a YOLO detection is a filled bbox rectangle (not a silhouette), which is a valid but cruder mask prompt than MOG2's foreground mask. This is acceptable — SAM2's box prompt already provides spatial constraints; the mask prompt is supplemental.

### Pattern 3: YOLO Training API

```python
# Source: ultralytics official docs (verified)
from ultralytics import YOLO

model = YOLO("yolov8n.pt")  # loads COCO pretrained weights
results = model.train(
    data="data/yolo_fish/dataset.yaml",
    epochs=100,
    imgsz=1600,     # match actual camera resolution (1600x1200)
    batch=4,        # small batch for large resolution + limited GPU VRAM
    device=0,       # CUDA GPU 0; use "cpu" if no GPU
    patience=20,    # early stopping
    single_cls=True,  # forces all classes to be treated as one
    augment=True,   # enables Albumentations augmentation (recommended for small datasets)
)
```

**Image size note:** Camera frames are 1600×1200. Training at native resolution (`imgsz=1600`) is correct for fish detection — down-scaling to 640 would make small fish undetectable. This increases VRAM requirement; v8n with `batch=4` at 1600px needs approximately 6-8 GB VRAM.

**Alternative if VRAM is limited:** Use `imgsz=640` at the cost of small-fish recall. Start with 1600; fall back to 640 if OOM.

### Pattern 4: Validation Metrics Access

```python
# Source: ultralytics docs, confirmed via community discussion
metrics = model.val(data="data/yolo_fish/dataset.yaml")
precision = metrics.box.P      # scalar or per-class array
recall = metrics.box.R         # scalar or per-class array
map50 = metrics.box.map50      # mAP at IoU=0.5
map50_95 = metrics.box.map     # mAP at IoU=0.5:0.95
```

For the phase evaluation, `recall` is the primary metric. Report both `P` and `R`; success criterion is YOLO recall > MOG2 recall on the same 30 validation frames.

### Pattern 5: Runtime Detector Selection

The decision (Claude's discretion) is to use a string argument pattern, consistent with how the project already parametrizes choices (e.g., `SAMPseudoLabeler` accepts `model_variant` as a string):

```python
def make_detector(kind: str, **kwargs) -> MOG2Detector | YOLODetector:
    """Factory: 'mog2' or 'yolo'."""
    if kind == "mog2":
        return MOG2Detector(**kwargs)
    elif kind == "yolo":
        model_path = kwargs.pop("model_path")
        return YOLODetector(model_path=model_path, **kwargs)
    raise ValueError(f"Unknown detector kind: {kind!r}")
```

This avoids config file overhead for what is currently a two-option choice. Scripts pass `--detector mog2|yolo` as a CLI flag.

### Anti-Patterns to Avoid

- **Do not use `single_cls=False` with `nc=1`:** YOLO handles this automatically; mixing these is a common source of confusion. Use `single_cls=True` explicitly for clarity.
- **Do not train at 640px on 1600×1200 footage:** Fish may occupy 50-200 pixels. At 640px input, downscaling loses fine-grained detail and harms recall.
- **Do not pass the MOG2 mask shape to YOLODetector:** `YOLODetector.detect()` must return a mask matching the full frame shape `(H, W)`, not a cropped region. The existing `SAMPseudoLabeler` code calls `det.mask` expecting full-frame dimensions.
- **Do not call `model.predict()` on frames one at a time in a tight loop without `verbose=False`:** YOLO prints per-image results to stdout by default. Set `verbose=False`.
- **Do not import `ultralytics` at module level in `detector.py`:** Use lazy import inside `YOLODetector.__init__` (consistent with how `sam2` is lazily imported in `pseudo_labeler.py`). This prevents import failure when `ultralytics` is not installed.

---

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| YOLO-format label file export | Custom label normalization code | Label Studio "Export → YOLO" button | Label Studio already handles % normalization, file structure, classes.txt |
| Bounding box NMS / confidence filtering | Custom box filtering | `model.predict(conf=0.25)` | Built-in confidence threshold parameter; YOLO internally applies NMS |
| Dataset augmentation for small training set | Custom augment pipeline | `model.train(augment=True)` | Ultralytics uses Albumentations internally; covers flips, scale, HSV jitter, mosaic |
| mAP / recall computation | Custom metric code | `model.val()` → `metrics.box.R` | Returns per-class and aggregate recall; matches COCO evaluation protocol |
| Mosaic / copy-paste augmentation | Custom collation | Ultralytics default `mosaic=1.0` training | Mosaic augmentation is on by default in YOLOv8; valuable for multi-fish frames |

**Key insight:** `ultralytics` is a batteries-included framework. Nearly all training concerns (augmentation, scheduler, NMS, metric logging) are handled internally. The integration work is primarily: dataset prep (directory structure + YAML), and `YOLODetector` class wrapping `model.predict()`.

---

## Common Pitfalls

### Pitfall 1: Image Path Mismatch (labels not found)

**What goes wrong:** YOLO trainer silently skips labels when the `images/` and `labels/` paths don't mirror each other exactly. Images in `images/train/cam_A_frame_000100.jpg` require a label at `labels/train/cam_A_frame_000100.txt` (same stem, `.txt` extension, same subdirectory).

**Why it happens:** The ultralytics trainer substitutes `images` → `labels` in the path string. Any deviation (extra subfolder, different naming) causes labels to be treated as missing (treated as negatives).

**How to avoid:** Name image files exactly matching label files. Use the script-generated structure (see dataset prep plan). Verify with a quick `assert` counting files in both directories.

**Warning signs:** Training loss drops to near-zero immediately (all samples treated as negatives), or `model.val()` reports 0.0 recall.

### Pitfall 2: Full-resolution Training OOM

**What goes wrong:** At `imgsz=1600`, YOLOv8n with `batch=4` may exceed GPU VRAM (8-12 GB needed). Windows with CUDA can give cryptic OOM or silently crash the training process.

**Why it happens:** 1600×1200 RGB frames are 4× the area of the standard 640px benchmark. YOLOv8n's FPN intermediate feature maps scale with input resolution.

**How to avoid:** Start with `batch=2` and `imgsz=1600`. If OOM, reduce to `imgsz=1280` then `imgsz=960`. Do not use `imgsz=640` as a first resort — the primary concern is recall on small fish.

**Warning signs:** CUDA OOM error during training; Python process killed.

### Pitfall 3: Label Studio YOLO Export Format Mismatch

**What goes wrong:** Label Studio's YOLO export creates a flat `images/` + `labels/` structure with a `classes.txt`. The `dataset.yaml` must reference the correct root path and subdirectory names. If the dataset root is wrong, training silently finds no images.

**Why it happens:** Label Studio exports to a zip that, when extracted, has a specific layout that may differ from the manual layout described in docs.

**How to avoid:** After extracting the Label Studio YOLO export, verify the structure matches the layout in the Architecture section. May need a small reorganization script to add `train/` and `val/` subdirectories and split frames per the 80/20 stratified split.

**Warning signs:** `model.train()` reports 0 images found.

### Pitfall 4: `YOLODetector.mask` Field — Wrong Shape or Missing

**What goes wrong:** `SAMPseudoLabeler.predict()` calls `_mask_to_logits(det.mask)` expecting `det.mask` to be a full-frame `(H, W)` uint8 array of shape matching the input image. If `YOLODetector` returns a cropped mask or `None`, SAM2 will error.

**Why it happens:** YOLO's detect task returns bounding boxes only, not pixel masks. The `mask` field must be explicitly synthesized (filled bbox rectangle) by `YOLODetector`.

**How to avoid:** In `YOLODetector.detect()`, always construct `mask = np.zeros((h_frame, w_frame), dtype=np.uint8)` and fill the bbox region with 255 before appending to detections.

**Warning signs:** `AttributeError` or shape mismatch error in `pseudo_labeler.py` during end-to-end integration test.

### Pitfall 5: Small Dataset Overfitting

**What goes wrong:** With only 120 training images, YOLOv8n can overfit quickly. Training recall on train set looks good but val recall is poor.

**Why it happens:** 120 images is small for fine-tuning a detection model. Val loss diverges from train loss after ~30 epochs.

**How to avoid:** Use `augment=True`, `mosaic=1.0` (default), `patience=20` for early stopping. Monitor val loss during training (ultralytics logs to `runs/detect/` by default). If val recall plateaus, stop early.

**Warning signs:** Val loss increasing while train loss decreasing after epoch 30.

### Pitfall 6: `single_cls` Flag Required

**What goes wrong:** If `nc=1` but `single_cls=False`, some metrics pathways in ultralytics may report per-class breakdowns that are misleading for this use case.

**How to avoid:** Always pass `single_cls=True` to `model.train()` for single-class fish detection.

---

## Code Examples

### Training Script Skeleton

```python
# Source: ultralytics official docs (verified Feb 2026)
from ultralytics import YOLO
from pathlib import Path

model = YOLO("yolov8n.pt")  # COCO pretrained weights downloaded automatically

results = model.train(
    data=str(Path("data/yolo_fish/dataset.yaml").resolve()),
    epochs=100,
    imgsz=1600,
    batch=2,
    device=0,
    patience=20,
    single_cls=True,
    augment=True,
    project="output/yolo_fish",
    name="train_v1",
)

# Weights saved to: output/yolo_fish/train_v1/weights/best.pt
```

### Evaluation Script Skeleton

```python
# Source: ultralytics official docs + community issue #8709 (verified)
from ultralytics import YOLO

model = YOLO("output/yolo_fish/train_v1/weights/best.pt")
metrics = model.val(
    data=str(Path("data/yolo_fish/dataset.yaml").resolve()),
    split="val",
)

recall = float(metrics.box.R)      # scalar for single-class
precision = float(metrics.box.P)
map50 = float(metrics.box.map50)
print(f"Recall: {recall:.3f}, Precision: {precision:.3f}, mAP50: {map50:.3f}")
```

### Predict on a Single Frame

```python
# Source: ultralytics official docs (verified Feb 2026)
import cv2
from ultralytics import YOLO

model = YOLO("output/yolo_fish/train_v1/weights/best.pt")
frame = cv2.imread("frame.jpg")  # BGR

results = model.predict(frame, conf=0.25, verbose=False)
for r in results:
    for box in r.boxes:
        x1, y1, x2, y2 = box.xyxy[0].tolist()
        conf = float(box.conf[0])
        print(f"  Fish bbox: ({x1:.0f}, {y1:.0f}, {x2:.0f}, {y2:.0f}), conf={conf:.2f}")
```

### MOG2-Guided Frame Sampling (diversity selection)

The user decision says "pick frames with varying fish counts (0, 1, 2, many)." Implementation approach:

```python
# Run MOG2 on all candidate frames, bin by detection count, sample uniformly per bin
from collections import defaultdict

def sample_diverse_frames(frames_with_counts: list[dict], target_per_bin: int = 5) -> list[dict]:
    """Sample frames across fish-count bins: 0, 1, 2, 3+.

    Args:
        frames_with_counts: List of dicts with keys frame_path, camera_id,
            frame_idx, mog2_count (int detection count from MOG2).
        target_per_bin: Number of frames to sample from each bin.

    Returns:
        Sampled subset.
    """
    bins: dict[int, list[dict]] = defaultdict(list)
    for f in frames_with_counts:
        count = f["mog2_count"]
        bin_key = min(count, 3)  # 0, 1, 2, 3+
        bins[bin_key].append(f)

    sampled = []
    for key, candidates in sorted(bins.items()):
        sampled.extend(candidates[:target_per_bin])
    return sampled
```

---

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Background subtraction (MOG2) | Learned object detection (YOLO) | ~2016 onward | Handles stationary fish, scene changes, shadow artifacts that confuse MOG2 |
| Separate detect.py / train.py scripts | Unified `ultralytics` API | YOLOv8 release Jan 2023 | Single package, consistent API for train/val/predict |
| YOLOv5 as standard | YOLOv8 as standard | 2023 | Better recall on small objects; improved anchor-free head |
| Full frame inference at 640px | Native resolution (1600px) for high-res cameras | Ongoing community practice | Required for small-object detection on 1600×1200 aquarium footage |

**Newer YOLO versions available:**
- YOLO11, YOLO26 exist in the ultralytics package as of 2025-2026. They share the same API (`YOLO("yolo11n.pt")`). YOLOv8 remains well-documented and stable. The user locked decision is "YOLOv8" — use `yolov8n.pt`/`yolov8s.pt` only.

---

## Implementation Plan for Planner

This section maps work into a natural plan breakdown:

**Plan A — Data collection and annotation**
- Frame extraction script: run MOG2 across all 13 cameras, sample diverse frames (binned by detection count), export JPEG frames per camera
- Label Studio project creation with RectangleLabels tag (bounding boxes, not masks)
- Checkpoint: human annotates 150 frames in Label Studio (bounding boxes)
- Label Studio YOLO export → dataset directory structure creation

**Plan B — YOLO training and evaluation**
- Add `ultralytics` to `pyproject.toml` dependencies
- Add `YOLODetector` class to `segmentation/detector.py` with lazy import
- Update `segmentation/__init__.py` and `__all__`
- Training script: `scripts/train_yolo.py` (thin wrapper over `model.train()`)
- Evaluation script: `scripts/eval_yolo.py` (recall, precision, visual comparison vs MOG2)
- Add unit tests for `YOLODetector` to `tests/unit/segmentation/test_detector.py`
- Checkpoint: YOLO recall > MOG2 recall on val set

**Plan C — Pipeline wiring and comparison**
- End-to-end integration: YOLO → SAM2 path works using existing `SAMPseudoLabeler`
- Runtime selection mechanism (`make_detector()` factory or equivalent)
- Diagnostic script: side-by-side YOLO boxes vs MOG2 boxes on same frames

---

## Open Questions

1. **GPU VRAM on dev machine**
   - What we know: torch 2.10+cu130 is installed (from STATE.md); machine has Windows 11
   - What's unclear: exact GPU VRAM available — affects whether `imgsz=1600` is feasible
   - Recommendation: Start training at `imgsz=1600, batch=2`. If OOM, fall back to `imgsz=1280`. Document the resolution used in the training script.

2. **Label Studio project reuse vs new project**
   - What we know: Phase 02.1 used Label Studio with BrushLabels (masks); this phase needs RectangleLabels (bboxes)
   - What's unclear: Whether the user wants to reuse the same Label Studio instance or create a new project
   - Recommendation: Create a new Label Studio project with RectangleLabels tag. BrushLabels and RectangleLabels are incompatible annotation types in the same project.

3. **MOG2 recall baseline value**
   - What we know: MOG2 was validated on 2 cameras (Phase 02.1-01); full 13-camera recall not yet measured
   - What's unclear: The exact recall number YOLO needs to beat
   - Recommendation: The comparison eval script should run MOG2 and YOLO on the same 30 val frames and report both recall numbers. No absolute threshold needed — just relative comparison.

4. **`imgsz` must be divisible by 32 for YOLOv8**
   - What we know: ultralytics requires imgsz to be divisible by 32
   - What's unclear: 1600 is divisible by 32 (1600/32=50), so this is fine. 1200 is not (1200/32=37.5) — the trainer handles non-square images by padding to the next multiple of 32.
   - Recommendation: Use `imgsz=1600` (the long edge); ultralytics will letterbox-pad the 1200 dimension automatically.

---

## Sources

### Primary (HIGH confidence)

- Ultralytics official docs (train): https://docs.ultralytics.com/modes/train/ — training API, parameters, data YAML format
- Ultralytics official docs (predict): https://docs.ultralytics.com/modes/predict/ — predict API, results.boxes, conf parameter
- Ultralytics official docs (val): https://docs.ultralytics.com/modes/val/ — metrics.box.map50, .map, .P, .R
- Ultralytics official GitHub model docs: https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolov8.md — model names (yolov8n.pt, yolov8s.pt)
- Label Studio docs (export): https://labelstud.io/guide/export — YOLO format export structure
- Codebase analysis: `src/aquapose/segmentation/detector.py`, `pseudo_labeler.py`, `label_studio.py` — existing interface contracts

### Secondary (MEDIUM confidence)

- Ultralytics community issue #8709 — precision/recall access via `metrics.box.P` and `metrics.box.R`
- Label Studio blog (YOLO integration): https://labelstud.io/blog/quickly-create-datasets-for-training-yolo-object-detection-with-label-studio/ — YOLO export workflow
- Medium: YOLO + SAM2 integration pattern — https://medium.com/@genet.gessessew/supercharging-object-detection-combining-yolo-and-sam2-for-enhanced-video-segmentation-3a8c5e603430

### Tertiary (LOW confidence)

- YOLO26 model names from ultralytics quickstart docs page — the page appeared to use "yolo26n.pt" as model names, which may indicate the docs were showing newer model generation. YOLOv8 names (yolov8n.pt) confirmed separately from the GitHub model docs. Use yolov8n.pt as specified in user decisions.

---

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH — ultralytics on PyPI, API verified from official docs
- Architecture (YOLO API): HIGH — verified from official docs and GitHub model page
- Architecture (YOLODetector integration): HIGH — derived from direct codebase analysis of existing interface
- Pitfalls: MEDIUM — derived from ultralytics community issues and known small-dataset patterns
- Label Studio YOLO export: MEDIUM — verified from official docs; exact zip structure may vary by version

**Research date:** 2026-02-20
**Valid until:** 2026-03-20 (ultralytics API is stable; Label Studio export format is stable)
