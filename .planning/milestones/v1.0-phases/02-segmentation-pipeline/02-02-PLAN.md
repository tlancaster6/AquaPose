---
phase: 02-segmentation-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/aquapose/segmentation/pseudo_labeler.py
  - src/aquapose/segmentation/dataset.py
  - src/aquapose/segmentation/__init__.py
  - tests/unit/segmentation/test_pseudo_labeler.py
  - tests/unit/segmentation/test_dataset.py
  - scripts/build_training_data.py
autonomous: true
requirements:
  - SEG-01
  - SEG-02
  - SEG-03
  - SEG-04

must_haves:
  truths:
    - "SAM2 uses box-only prompting (no mask prompt) and keeps the largest area mask"
    - "Quality filtering rejects masks by confidence, fill ratio, min area, and keeps only largest connected component"
    - "Training dataset is COCO JSON with per-camera stratified 80/20 train/val split"
    - "~10% negative examples (background crops with no fish) included in dataset"
    - "Temporal sampling selects frames from video at configurable stride"
    - "Crop dimensions match bbox dimensions (width and height vary independently)"
  artifacts:
    - path: "src/aquapose/segmentation/pseudo_labeler.py"
      provides: "SAMPseudoLabeler with box-only mode, quality filtering, to_coco_dataset"
    - path: "src/aquapose/segmentation/dataset.py"
      provides: "CropDataset supporting variable-size crops with per-camera stratified split"
    - path: "scripts/build_training_data.py"
      provides: "CLI for generating COCO training data from raw videos"
  key_links:
    - from: "scripts/build_training_data.py"
      to: "SAMPseudoLabeler.predict"
      via: "detect -> SAM2 -> filter -> COCO export pipeline"
      pattern: "labeler\\.predict"
    - from: "src/aquapose/segmentation/dataset.py"
      to: "COCO JSON annotations"
      via: "CropDataset.__getitem__ loads variable-size crops"
      pattern: "cv2\\.resize.*crop_size"
---

<objective>
Update the pseudo-labeling pipeline and dataset to match CONTEXT.md decisions: box-only SAM2, quality filtering, variable crop sizes, stratified splits, and negative examples.

Purpose: The original Phase 2 code used mask prompts and fixed 256x256 crops. CONTEXT.md specifies box-only SAM2 (dramatically better masks), quality filtering, variable crop dimensions matching bbox, per-camera stratified splits, and ~10% negative examples. These changes produce higher-quality training data for Mask R-CNN.

Output: Updated pseudo_labeler.py, dataset.py, and build_training_data.py that produce a COCO dataset ready for Mask R-CNN training.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-segmentation-pipeline/02-CONTEXT.md
@.planning/phases/02-segmentation-pipeline/02-RESEARCH.md
@.planning/phases/02-segmentation-pipeline/02-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update SAMPseudoLabeler for box-only mode and quality filtering</name>
  <files>
    src/aquapose/segmentation/pseudo_labeler.py
    tests/unit/segmentation/test_pseudo_labeler.py
    src/aquapose/segmentation/__init__.py
  </files>
  <action>
Update `SAMPseudoLabeler` in `pseudo_labeler.py`:

1. **Remove mask prompt support**: Delete the `use_mask_prompt` parameter from `predict()`. Remove the `_mask_to_logits` helper function entirely. SAM2 should always use box-only prompting per CONTEXT.md ("SAM2 uses crop+box-only prompt (no mask prompt) — dramatically better masks").

2. **Handle multiple SAM2 masks**: When SAM2 produces multiple masks (if `multimask_output=True` for single-box calls), keep the largest area mask. Add a `_select_largest_mask(masks: np.ndarray) -> np.ndarray` helper that selects the mask with the most nonzero pixels. For the batch case (multiple boxes), continue using `multimask_output=False`.

3. **Add quality filtering**: Add a `filter_mask()` classmethod or standalone function with these parameters:
   - `mask`: binary mask (uint8, 0/255)
   - `detection`: Detection object
   - `min_conf`: float = 0.3 (minimum YOLO detection confidence)
   - `min_fill`: float = 0.15 (minimum mask fill ratio of bbox area)
   - `max_fill`: float = 0.85 (maximum mask fill ratio of bbox area)
   - `min_area`: int = 150 (minimum mask pixel area)
   Returns `np.ndarray | None` (None if filtered out).

   Quality filtering steps:
   a. Check detection confidence >= min_conf
   b. Keep only the largest connected component (`cv2.connectedComponentsWithStats`)
   c. Check mask area >= min_area
   d. Compute fill ratio = mask_area / bbox_area, reject if outside [min_fill, max_fill]

   Note: `build_training_data.py` already has `_largest_connected_component` and `_filter_mask` — extract and formalize these into the module. The script versions are the reference implementation.

4. **Add visualization flag**: Add `draw_pseudolabels: bool = False` parameter to `predict()`. When True, return a list of annotated images alongside masks (overlay mask contours on the crop). This is for developer debugging, not user-facing. Store as a second return value only when the flag is True (use overload or optional return).

   Actually, keep it simpler per CONTEXT.md "Configurable visualization flag: draw_pseudolabels=true/false": Add it as a constructor parameter `draw_pseudolabels: bool = False` on SAMPseudoLabeler. When True, `predict()` saves annotated images to a `debug/` subdirectory next to output. This avoids changing the return signature.

5. **Export new symbols** from `__init__.py`: Export `filter_mask` if made standalone.

6. Update tests:
   - Remove tests that use `use_mask_prompt=True`
   - Add test for `filter_mask` with edge cases (low confidence, bad fill ratio, small area, empty mask)
   - Add test for largest connected component selection
   - Test that `_mask_to_logits` no longer exists (import error)
  </action>
  <verify>
- `hatch run test` passes
- `hatch run lint` passes
- `hatch run typecheck` passes
- `use_mask_prompt` parameter no longer exists on `predict()`
- `_mask_to_logits` function no longer exists
- `filter_mask` function importable from `aquapose.segmentation`
  </verify>
  <done>SAMPseudoLabeler uses box-only prompting. Quality filtering function available. Mask prompt code removed. Tests updated.</done>
</task>

<task type="auto">
  <name>Task 2: Update CropDataset for variable crop sizes and stratified split, plus build_training_data.py</name>
  <files>
    src/aquapose/segmentation/dataset.py
    tests/unit/segmentation/test_dataset.py
    scripts/build_training_data.py
  </files>
  <action>
**CropDataset changes** (`dataset.py`):

1. **Variable crop sizes**: Remove the fixed `crop_size=256` resize. Crops should match the bbox dimensions from the COCO annotation. Change `__getitem__` to:
   - Load the crop image at its native resolution (already cropped by the generation script)
   - Resize to a configurable `max_size` (default 512) only if either dimension exceeds it, preserving aspect ratio
   - Pad to a square if needed (using zero-padding) for batching compatibility, OR keep variable sizes and rely on the collate_fn (Mask R-CNN handles variable sizes natively via its FPN)

   Actually, torchvision Mask R-CNN handles variable-sized inputs natively (the FPN + RoI pooling handles this). So: load crop at native resolution, normalize to float [0,1], return as-is. The collate_fn already returns a tuple (not a stacked batch), so variable sizes work.

2. **Per-camera stratified split**: Add a `stratified_split(dataset, val_fraction=0.2, seed=42)` function that:
   - Groups images by `camera_id` field in the COCO JSON
   - For each camera, randomly selects `val_fraction` of its images for validation
   - Returns `(train_indices, val_indices)` as lists of ints
   - This ensures each camera is proportionally represented in both train and val

3. **Negative examples support**: CropDataset should handle images with zero annotations (COCO image entry exists but no annotations reference it). The current code already returns empty tensors for this case — verify it works correctly with native-resolution images.

4. Update tests:
   - Test variable-size crop loading (create test COCO JSON with crops of different sizes)
   - Test stratified_split returns proportional camera representation
   - Test negative example handling (empty annotations)

**build_training_data.py changes** (`scripts/build_training_data.py`):

5. Update the `generate` subcommand to:
   - Use `filter_mask` from `aquapose.segmentation` instead of the local `_filter_mask`
   - Remove the local `_largest_connected_component` and `_filter_mask` functions (now in the module)
   - Add `--neg-fraction` argument (default 0.1) controlling the fraction of negative examples
   - After generating all positive crops, add random background crops: sample random bbox-sized regions from frames with no detections (or from the margins of frames with detections), save as crop images, add to COCO JSON with empty annotations
   - Use `stratified_split` from `dataset.py` to split and save separate `train.json` / `val.json` COCO files alongside the combined `coco_annotations.json`
  </action>
  <verify>
- `hatch run test` passes
- `hatch run lint` passes
- `hatch run typecheck` passes
- CropDataset loads variable-size crops without forced resize to 256x256
- `stratified_split` function importable from `aquapose.segmentation`
- build_training_data.py `--help` shows `--neg-fraction` argument
  </verify>
  <done>CropDataset handles variable-size crops. Per-camera stratified split available. build_training_data.py generates negative examples and split JSON files.</done>
</task>

</tasks>

<verification>
1. `hatch run check` (lint + typecheck) passes
2. `hatch run test` passes
3. SAM2 predict() has no use_mask_prompt parameter
4. filter_mask importable from aquapose.segmentation
5. CropDataset works with variable-size images
6. stratified_split produces proportional camera representation
7. build_training_data.py generates train.json and val.json with negative examples
</verification>

<success_criteria>
- SAM2 pseudo-labeler uses box-only prompting exclusively
- Quality filtering is a reusable function in the segmentation module
- CropDataset supports variable-size crops (not fixed 256x256)
- Per-camera stratified 80/20 split implemented
- ~10% negative examples generated in training data
- build_training_data.py is the single entry point for dataset generation
</success_criteria>

<output>
After completion, create `.planning/phases/02-segmentation-pipeline/02-02-SUMMARY.md`
</output>
