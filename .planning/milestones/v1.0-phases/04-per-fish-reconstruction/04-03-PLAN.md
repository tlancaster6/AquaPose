---
phase: 04-per-fish-reconstruction
plan: 03
type: execute
wave: 3
depends_on: [04-02]
files_modified:
  - src/aquapose/optimization/validation.py
  - src/aquapose/optimization/__init__.py
  - tests/unit/optimization/test_validation.py
  - scripts/run_reconstruction.py
autonomous: false
requirements: [RECON-05]
must_haves:
  truths:
    - "Cross-view holdout validation computes IoU for each held-out camera by fitting on N-1 cameras and evaluating the rendered silhouette on the excluded camera"
    - "Global average holdout IoU is reported with per-camera breakdown showing no camera below 0.60 floor"
    - "Visual overlay renders the optimized mesh silhouette onto the original camera frame for qualitative QA"
    - "A reconstruction script processes a real video clip end-to-end: detect fish, initialize state, optimize per frame, validate holdout"
  artifacts:
    - path: "src/aquapose/optimization/validation.py"
      provides: "Holdout IoU evaluation and visual overlay rendering"
      exports: ["evaluate_holdout_iou", "run_holdout_validation", "render_overlay"]
    - path: "scripts/run_reconstruction.py"
      provides: "End-to-end reconstruction CLI on real data"
    - path: "tests/unit/optimization/test_validation.py"
      provides: "Tests for holdout IoU computation and overlay rendering"
  key_links:
    - from: "src/aquapose/optimization/validation.py"
      to: "src/aquapose/optimization/optimizer.py"
      via: "run_holdout_validation calls FishOptimizer with camera subsets"
      pattern: "FishOptimizer|optimize_first_frame|optimize_frame"
    - from: "src/aquapose/optimization/validation.py"
      to: "src/aquapose/optimization/renderer.py"
      via: "evaluate_holdout_iou renders mesh into held-out camera"
      pattern: "renderer\\.render\\("
    - from: "scripts/run_reconstruction.py"
      to: "src/aquapose/optimization/"
      via: "Orchestrates full pipeline: detection -> init -> optimize -> validate"
      pattern: "from aquapose\\.optimization"
---

<objective>
Implement cross-view holdout validation and run end-to-end reconstruction on real data, achieving the target holdout IoU.

Purpose: Validate that the analysis-by-synthesis system generalizes beyond the cameras it was fit on — holdout IoU is the primary success metric for Phase 4. The reconstruction script provides the complete pipeline for real data.

Output: `validation.py` (holdout IoU evaluation + visual overlays), `run_reconstruction.py` (end-to-end CLI), quantitative IoU results, and visual QA overlays.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-per-fish-reconstruction/04-RESEARCH.md
@.planning/phases/04-per-fish-reconstruction/04-01-SUMMARY.md
@.planning/phases/04-per-fish-reconstruction/04-02-SUMMARY.md

Key upstream APIs:
- FishOptimizer.optimize_first_frame, optimize_frame, optimize_sequence (Plan 02)
- RefractiveSilhouetteRenderer.render (Plan 01)
- soft_iou_loss (Plan 01)
- UNetSegmentor or YOLODetector for detection (Phase 2)
- init_fish_states_from_masks (Phase 3)
- load_calibration_data (Phase 1)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement holdout validation and visual overlay utilities</name>
  <files>
    src/aquapose/optimization/validation.py
    src/aquapose/optimization/__init__.py
    tests/unit/optimization/test_validation.py
  </files>
  <action>
  **Step 1: Create `src/aquapose/optimization/validation.py`.**

  Implement `evaluate_holdout_iou(state, held_out_camera, held_out_mask, renderer, crop_region=None) -> float`:
  - Build mesh from state: `meshes = build_fish_mesh([state])`
  - Render into held-out camera with `torch.no_grad()`
  - Compute IoU: `1.0 - float(soft_iou_loss(alpha, held_out_mask, crop_region))`
  - Return IoU as Python float

  Implement `run_holdout_validation(states, frames_data, cameras, camera_ids, renderer, optimizer) -> dict`:
  - For each frame, for each camera: hold that camera out, optimize on remaining N-1 cameras, evaluate IoU on held-out camera
  - Optionally: rotate held-out camera across frames to save compute (hold out camera k on frame k % N)
  - Return dict with: `"global_mean_iou"`, `"per_camera_iou"` (dict[str, float] mean per camera), `"per_frame_iou"` (list of per-frame results), `"min_camera_iou"` (worst-performing camera)
  - Print summary: global mean, per-camera breakdown, whether 0.80/0.60 targets are met

  Implement `render_overlay(frame_bgr, alpha_map, crop_region=None, color=(0, 255, 0), opacity=0.4) -> np.ndarray`:
  - Overlay the rendered silhouette (alpha map) onto the original camera frame as a colored transparent overlay
  - If crop_region provided, paste alpha into full frame first
  - Use cv2 for blending: `overlay = frame * (1 - alpha * opacity) + color * alpha * opacity`
  - Return BGR image with overlay

  **Step 2: Create `tests/unit/optimization/test_validation.py`.**
  - `test_evaluate_holdout_iou_perfect`: alpha == target → IoU near 1.0
  - `test_evaluate_holdout_iou_no_overlap`: disjoint → IoU near 0.0
  - `test_render_overlay_shape`: output shape matches input frame shape
  - `test_render_overlay_modifies_pixels`: output differs from input where alpha > 0
  - `test_run_holdout_validation_structure`: mock optimizer and renderer, verify returned dict has expected keys

  **Step 3: Update `src/aquapose/optimization/__init__.py`** adding `evaluate_holdout_iou`, `run_holdout_validation`, `render_overlay` to `__all__`.
  </action>
  <verify>
  ```bash
  hatch run test tests/unit/optimization/test_validation.py -v
  hatch run test  # all tests pass
  ```
  </verify>
  <done>
  - evaluate_holdout_iou computes IoU between rendered mesh and held-out camera mask
  - run_holdout_validation rotates held-out cameras and reports per-camera and global metrics
  - render_overlay produces visual QA overlays of mesh on camera frames
  - All validation tests pass
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: End-to-end reconstruction on real data with holdout validation</name>
  <files>
    scripts/run_reconstruction.py
  </files>
  <action>
  Create `scripts/run_reconstruction.py` — an end-to-end CLI script that:
  1. Loads calibration data for all cameras via `load_calibration_data()`
  2. Reads video frames from `--video-dir`, one frame at a time across all cameras
  3. Detects fish in each camera view using YOLO (default) or MOG2 detector, producing binary masks
  4. Initializes 3D fish state from multi-view masks via `init_fish_states_from_masks()`
  5. Optimizes per-fish pose via `FishOptimizer` (2-start first frame, warm-start subsequent)
  6. Runs cross-view holdout validation via `run_holdout_validation()` and reports IoU metrics
  7. Saves visual overlay images via `render_overlay()` for qualitative QA

  CLI arguments:
  - `--video-dir`: path to raw video directory (required)
  - `--calibration-json`: path to AquaCal calibration file (required)
  - `--output-dir`: where to write results, default `output/reconstruction`
  - `--n-frames`: number of frames to process, default 100
  - `--detector`: "yolo" or "mog2", default "yolo"
  - `--yolo-weights`: path to YOLO weights file
  - `--sigma`: silhouette sigma hyperparameter, default 1e-4
  - `--lr`: optimizer learning rate, default 1e-3

  Script creates:
  - `{output-dir}/overlays/` — visual overlay images per frame per camera
  - `{output-dir}/metrics.json` — holdout IoU per camera, global mean, pass/fail status

  Then run the script on a short clip (~50 frames) and report results for human verification.
  </action>
  <verify>
  ```bash
  python scripts/run_reconstruction.py --help
  ```
  </verify>
  <done>
  - Script runs end-to-end on real video data without errors
  - Holdout IoU metrics and overlay images are produced
  </done>
  <what-built>
  End-to-end reconstruction script processing real video data through the full pipeline: detection, initialization, optimization, holdout validation, and visual overlay output.
  </what-built>
  <how-to-verify>
  1. Run reconstruction on a short clip:
     ```bash
     python scripts/run_reconstruction.py \
       --video-dir "C:/Users/tucke/Desktop/Aqua/AquaPose/raw_videos" \
       --calibration-json "path/to/calibration.json" \
       --output-dir output/reconstruction \
       --n-frames 50
     ```
  2. Check console output for holdout IoU metrics:
     - Global mean IoU should be >= 0.80
     - No individual camera should be below 0.60
  3. Open overlay images in `output/reconstruction/overlays/` — the green mesh silhouette should align with the actual fish in the camera frame
  4. If holdout IoU is below target, review per-camera breakdown to identify problem cameras/frames

  **If targets are NOT met:**
  - Review visual overlays to diagnose: is the mesh misaligned? wrong fish? wrong scale?
  - Consider sigma/gamma tuning (try sigma=5e-4 or 1e-3 for softer boundaries)
  - Consider learning rate adjustments
  - Report findings and we'll create a gap-closure plan
  </how-to-verify>
  <resume-signal>Report holdout IoU results. Type "approved" if >= 0.80 global mean with no camera below 0.60, or describe issues for gap closure.</resume-signal>
</task>

</tasks>

<verification>
```bash
# Unit tests pass
hatch run test tests/unit/optimization/ -v

# Full test suite
hatch run test

# End-to-end reconstruction produces results
python scripts/run_reconstruction.py --help
```
</verification>

<success_criteria>
- Cross-view holdout validation achieves >= 0.80 global mean IoU
- No individual camera holdout IoU below 0.60
- Visual overlays show mesh silhouettes aligned with actual fish in camera views
- Reconstruction script processes real video data end-to-end without errors
- All unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/04-per-fish-reconstruction/04-03-SUMMARY.md`
</output>
