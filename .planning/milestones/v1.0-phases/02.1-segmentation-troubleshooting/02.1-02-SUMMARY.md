---
phase: 02.1-segmentation-troubleshooting
plan: 02
subsystem: segmentation
tags: [sam2, yolo, pseudo-labels, evaluation, mask-iou]

# Dependency graph
requires:
  - phase: 02.1.1-object-detection-alternative-to-mog2
    provides: YOLODetector + run_pseudo_labels.py --detector yolo pipeline
  - phase: 02-segmentation-pipeline
    provides: SAMPseudoLabeler, label_studio export utilities
provides:
  - SAM2 evaluation script (scripts/test_sam2.py) comparing YOLO-sourced pseudo-labels to manual GT
  - Per-frame mask IoU metrics and side-by-side visual overlays
affects:
  - 02.1-03 (next plan, using verified pseudo-labels for Label Studio import)
  - Phase 05 training pipeline (pseudo-label quality gates)

# Tech tracking
tech-stack:
  added: []
  patterns:
    - Greedy IoU matching for multi-fish per-frame evaluation (sort pairwise matrix descending, assign one-to-one)
    - Label Studio brush RLE decode inline (no label_studio_converter dependency in evaluation script)

key-files:
  created: []
  modified:
    - scripts/test_sam2.py

key-decisions:
  - "Default --pseudo-labels-dir is output/pseudo_labels (matches run_pseudo_labels.py YOLO output, not verify_pseudo_labels)"
  - "YOLO traceability note printed in summary output for audit trail"

patterns-established:
  - "test_sam2.py: empty GT dir exits 0 with guidance message (not error)"
  - "test_sam2.py: unmatched GT masks count as IoU=0; unmatched SAM2 masks reported as FP but do not penalise score"

requirements-completed:
  - SEG-2
  - SEG-3

# Metrics
duration: 15min
completed: 2026-02-20
---

# Phase 02.1 Plan 02: SAM2 Evaluation Script Summary

**SAM2 pseudo-label evaluation script using YOLO-sourced detections with per-frame IoU, greedy matching, and side-by-side visual comparisons**

## Performance

- **Duration:** ~15 min
- **Started:** 2026-02-20T20:12:00Z
- **Completed:** 2026-02-20T20:27:43Z
- **Tasks:** 1 of 2 complete (Task 2 is a human-verify checkpoint)
- **Files modified:** 1

## Accomplishments
- `scripts/test_sam2.py` updated to default to `output/pseudo_labels` (YOLO pipeline output directory)
- Added YOLO traceability note in summary output for audit trail
- Updated module docstring with 3-step usage workflow (generate -> annotate -> evaluate)
- Script handles empty GT directory gracefully (informative message, exit 0)

## Task Commits

1. **Task 1: Create SAM2 evaluation script using YOLO-sourced pseudo-labels** - `62252e6` (feat)

_Task 2 (human-verify checkpoint) requires user to generate YOLO pseudo-labels, annotate GT, and run evaluation._

## Files Created/Modified
- `scripts/test_sam2.py` - SAM2 evaluation script: loads Label Studio tasks JSON, decodes brush RLE masks, matches to GT binary PNGs via greedy IoU, saves side-by-side comparisons, prints PASS/FAIL summary

## Decisions Made
- Default `--pseudo-labels-dir` changed from `output/verify_pseudo_labels` to `output/pseudo_labels` to align with `run_pseudo_labels.py --detector yolo` output path
- YOLO traceability note added to summary output: "Note: SAM2 detections sourced from YOLO (not MOG2)."

## Deviations from Plan

### Auto-fixed Issues

**1. [Rule 1 - Bug] Corrected default --pseudo-labels-dir path**
- **Found during:** Task 1 (reviewing existing script vs plan spec)
- **Issue:** Existing script defaulted to `output/verify_pseudo_labels` (old verify pipeline path), but plan specifies `output/pseudo_labels` (YOLO pipeline output)
- **Fix:** Updated default path and help text to match plan spec
- **Files modified:** scripts/test_sam2.py
- **Verification:** --help output shows correct default
- **Committed in:** 62252e6 (Task 1 commit)

---

**Total deviations:** 1 auto-fixed (1 bug â€” wrong default path)
**Impact on plan:** Necessary correctness fix. Script would silently look in wrong directory without this.

## Issues Encountered
- `scripts/test_sam2.py` already existed (from prior work). Updated to match plan spec rather than creating from scratch.

## User Setup Required
None - no external service configuration required.

## Next Phase Readiness
- Task 2 (human-verify checkpoint) is blocking: user must generate YOLO pseudo-labels and provide GT annotations before evaluation can run
- Once Task 2 passes (mean IoU >= 0.70), Phase 02.1 Plan 03 can proceed

## Self-Check: PASSED

- FOUND: scripts/test_sam2.py
- FOUND: commit 62252e6

---
*Phase: 02.1-segmentation-troubleshooting*
*Completed: 2026-02-20*
