---
phase: 19-alpha-refactor-audit
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - tools/smoke_test.py
  - tests/e2e/test_smoke.py
autonomous: true
requirements: [AUDIT]

must_haves:
  truths:
    - "A reusable smoke test script exercises aquapose run across all modes and backends"
    - "Reproducibility is verified by running the pipeline twice and comparing outputs with np.allclose"
    - "The smoke test is flexible to future changes (new modes, new backends)"
  artifacts:
    - path: "tools/smoke_test.py"
      provides: "Reusable smoke test script for pipeline verification"
    - path: "tests/e2e/test_smoke.py"
      provides: "Pytest wrapper for smoke test (marked @slow)"
  key_links:
    - from: "tools/smoke_test.py"
      to: "src/aquapose/cli.py"
      via: "subprocess call to aquapose run"
      pattern: "aquapose run"
---

<objective>
Create a reusable smoke test script that exercises the full pipeline across all modes, backends, and data types, and verifies reproducibility.

Purpose: CONTEXT.md mandates full pipeline verification: each mode (production, diagnostic, synthetic, benchmark), each swappable backend (triangulation vs curve_optimizer), both real and synthetic data. Stop at frame 10 for speed. Reproducibility verified by running twice and comparing at the data level. Packaged as a reusable tool.

Output: Standalone smoke test script and pytest integration.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/19-alpha-refactor-audit/19-CONTEXT.md
@src/aquapose/cli.py
@src/aquapose/engine/config.py
@src/aquapose/engine/pipeline.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build reusable smoke test script</name>
  <files>tools/smoke_test.py</files>
  <action>
Create `tools/smoke_test.py` — a standalone script that exercises the pipeline. Design for reusability (CONTEXT.md: "a test we'll run rarely but will run again").

**Test dimensions (each tested independently with others at defaults):**

1. **Mode presets:** production, diagnostic, synthetic, benchmark
   - For each mode: run `aquapose run --mode {mode} --config {config}` with frame limit 10
   - Assert exit code 0
   - Assert expected output artifacts exist (mode-dependent)

2. **Swappable backends:**
   - Reconstruction: triangulation (default), curve_optimizer (if available)
   - Detection: YOLO (default)
   - Run with each non-default backend and assert exit code 0

3. **Data sources:**
   - Real data (core_videos + calibration)
   - Synthetic mode (no external data needed)

4. **Reproducibility check:**
   - Run pipeline twice with identical config and seed
   - Load HDF5 outputs from both runs
   - Compare all numeric arrays with `np.allclose(atol=1e-10, rtol=1e-10)`
   - Report any arrays that differ

**Script structure:**
```python
class SmokeTestRunner:
    def __init__(self, config_path, video_dir, calibration_path, output_base, frame_limit=10):
        ...

    def run_mode_tests(self) -> list[TestResult]
    def run_backend_tests(self) -> list[TestResult]
    def run_reproducibility_test(self) -> TestResult
    def run_all(self) -> SmokeTestReport

@dataclass
class TestResult:
    name: str
    passed: bool
    duration_seconds: float
    error: str | None
    artifacts: list[str]

@dataclass
class SmokeTestReport:
    results: list[TestResult]
    total_duration: float
    passed: int
    failed: int
```

**CLI interface:**
```
python tools/smoke_test.py --config path.yaml --output-dir ./smoke_results
python tools/smoke_test.py --config path.yaml --only modes      # just mode tests
python tools/smoke_test.py --config path.yaml --only backends   # just backend tests
python tools/smoke_test.py --config path.yaml --only repro      # just reproducibility
```

**Flexibility requirements:**
- Mode list and backend list are defined as constants at top of file (easy to extend)
- Config path and data paths are CLI arguments (not hardcoded)
- Each test is independent — failure of one doesn't block others
- Output is structured (JSON report + human-readable summary)
- Frame limit is configurable

**Error handling:**
- Subprocess timeout: 5 minutes per run
- Missing data files: skip test with warning (not failure)
- Pipeline failure: capture stderr, report as test failure
  </action>
  <verify>
    <automated>cd C:/Users/tucke/PycharmProjects/AquaPose && python tools/smoke_test.py --help</automated>
  </verify>
  <done>The smoke test script is runnable, accepts CLI args, and prints help. Its test dimensions cover modes, backends, and reproducibility.</done>
</task>

<task type="auto">
  <name>Task 2: Create pytest wrapper and run synthetic-only smoke test</name>
  <files>tests/e2e/test_smoke.py</files>
  <action>
1. Create `tests/e2e/test_smoke.py` that wraps the smoke test for pytest integration:

```python
import pytest
from tools.smoke_test import SmokeTestRunner

@pytest.mark.slow
@pytest.mark.e2e
class TestSmoke:
    """Smoke tests for pipeline verification.

    Run with: hatch run test-all -k smoke
    These tests require external data and are excluded from fast test runs.
    """

    def test_synthetic_mode(self, tmp_path):
        """Synthetic mode needs no external data — always runnable."""
        runner = SmokeTestRunner(output_base=tmp_path, frame_limit=5)
        result = runner.run_single_mode("synthetic")
        assert result.passed, f"Synthetic mode failed: {result.error}"

    def test_benchmark_mode(self, tmp_path):
        """Benchmark mode with synthetic data."""
        runner = SmokeTestRunner(output_base=tmp_path, frame_limit=5)
        result = runner.run_single_mode("benchmark")
        assert result.passed, f"Benchmark mode failed: {result.error}"
```

2. Run the synthetic-only test to verify basic pipeline health:
```bash
hatch run test tests/e2e/test_smoke.py::TestSmoke::test_synthetic_mode -x
```

3. If synthetic mode doesn't work without external data (needs at minimum a config), create a minimal test config fixture that works with synthetic data only.

Note: Tests requiring real video data are marked @slow and skipped when data paths aren't available. The synthetic test should always be runnable.
  </action>
  <verify>
    <automated>cd C:/Users/tucke/PycharmProjects/AquaPose && python -c "import tools.smoke_test; print('import ok')" 2>&1 || python -c "import importlib.util; spec = importlib.util.spec_from_file_location('smoke_test', 'tools/smoke_test.py'); print('file exists' if spec else 'not found')"</automated>
  </verify>
  <done>Pytest wrapper exists. Synthetic mode smoke test can be invoked. The smoke test tool is reusable for future audit runs.</done>
</task>

</tasks>

<verification>
1. `python tools/smoke_test.py --help` prints usage
2. The script defines SmokeTestRunner with mode, backend, and reproducibility test methods
3. The script supports `--only` filtering for partial runs
4. `tests/e2e/test_smoke.py` exists and can be collected by pytest
</verification>

<success_criteria>
- Smoke test script covers all 4 modes, backend variations, and reproducibility
- Script is CLI-driven with configurable paths and frame limits
- Pytest wrapper provides @slow-marked tests for CI integration
- Report output is structured (JSON + human-readable)
- Flexible to future additions (new modes, new backends)
</success_criteria>

<output>
After completion, create `.planning/phases/19-alpha-refactor-audit/19-02-SUMMARY.md`
</output>
