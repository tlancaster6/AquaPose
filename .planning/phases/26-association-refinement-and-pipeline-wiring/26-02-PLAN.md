---
phase: 26-association-refinement-and-pipeline-wiring
plan: 02
type: execute
wave: 2
depends_on: [26-01]
files_modified:
  - src/aquapose/core/midline/stage.py
  - src/aquapose/core/midline/orientation.py
  - src/aquapose/core/midline/__init__.py
  - src/aquapose/engine/config.py
  - tests/unit/core/midline/test_orientation.py
autonomous: true
requirements: [PIPE-02]

must_haves:
  truths:
    - "Midline extraction runs only on detections belonging to confirmed tracklet groups (not all detections)"
    - "Head-tail orientation is resolved using three signals: cross-camera geometric vote, velocity alignment, and temporal prior"
    - "Velocity signal is gated by a configurable speed threshold — below threshold, velocity is ignored"
    - "When a camera disagrees with consensus orientation, its midline points are flipped (not excluded)"
    - "Skeleton backend resolves head-tail before emitting annotated_detections"
  artifacts:
    - path: "src/aquapose/core/midline/orientation.py"
      provides: "Head-tail orientation resolver combining three signals"
      exports: ["resolve_orientation"]
    - path: "src/aquapose/core/midline/stage.py"
      provides: "MidlineStage processing only confirmed tracklet detections"
      contains: "tracklet_groups"
    - path: "tests/unit/core/midline/test_orientation.py"
      provides: "Unit tests for orientation resolver"
      min_lines: 80
  key_links:
    - from: "src/aquapose/core/midline/stage.py"
      to: "src/aquapose/core/association/types.py"
      via: "MidlineStage reads tracklet_groups to determine which detections to process"
      pattern: "tracklet_groups"
    - from: "src/aquapose/core/midline/orientation.py"
      to: "src/aquapose/calibration/luts.py"
      via: "ForwardLUT.cast_ray() for triangulating both head-tail orientations"
      pattern: "cast_ray"
---

<objective>
Wire MidlineStage (Stage 4) to process only detections from confirmed tracklet groups, and implement head-tail orientation resolution using cross-camera geometry, velocity alignment, and temporal prior.

Purpose: In the v2.1 pipeline, midline extraction runs AFTER association (Stage 3), so it knows which detections belong to which fish across cameras. This knowledge enables true head-tail disambiguation by triangulating both orientations and picking the geometrically consistent one.

Output: Updated `MidlineStage.run()`, new `orientation.py` module, MidlineConfig additions for orientation thresholds.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/26-association-refinement-and-pipeline-wiring/26-CONTEXT.md
@.planning/phases/26-association-refinement-and-pipeline-wiring/26-01-PLAN.md

@src/aquapose/core/midline/stage.py
@src/aquapose/core/midline/types.py
@src/aquapose/core/midline/backends/segment_then_extract.py
@src/aquapose/core/association/types.py
@src/aquapose/core/tracking/types.py
@src/aquapose/core/context.py
@src/aquapose/engine/config.py
@src/aquapose/calibration/luts.py

<interfaces>
<!-- Key types the executor needs -->

From src/aquapose/core/context.py:
```python
@dataclass
class PipelineContext:
    detections: list | None = None        # list[dict[str, list[Detection]]]
    tracks_2d: dict | None = None         # dict[str, list[Tracklet2D]]
    tracklet_groups: list | None = None   # list[TrackletGroup]
    annotated_detections: list | None = None  # list[dict[str, list]]
```

From src/aquapose/core/association/types.py:
```python
@dataclass(frozen=True)
class TrackletGroup:
    fish_id: int
    tracklets: tuple       # tuple[Tracklet2D, ...]
    confidence: float | None = None
    per_frame_confidence: tuple | None = None  # added in 26-01
```

From src/aquapose/core/tracking/types.py:
```python
@dataclass(frozen=True)
class Tracklet2D:
    camera_id: str
    track_id: int
    frames: tuple          # tuple[int, ...]
    centroids: tuple       # tuple[tuple[float, float], ...]
    bboxes: tuple
    frame_status: tuple    # "detected" | "coasted" | "interpolated"
```

MidlineStage currently processes ALL detections from context.detections.
After this plan, it will only process detections that belong to tracklet groups.
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add orientation config and implement orientation resolver</name>
  <files>
    src/aquapose/engine/config.py
    src/aquapose/core/midline/orientation.py
    src/aquapose/core/midline/__init__.py
    tests/unit/core/midline/test_orientation.py
  </files>
  <action>
  1. **Config additions** in `MidlineConfig`:
     - `speed_threshold: float = 2.0` — minimum pixel-per-frame speed for velocity signal (below this, velocity is ignored and temporal prior is used alone). Fish centroid movement < 2px/frame is near-stationary.
     - `orientation_weight_geometric: float = 1.0` — weight for cross-camera geometric vote.
     - `orientation_weight_velocity: float = 0.5` — weight for velocity alignment signal.
     - `orientation_weight_temporal: float = 0.3` — weight for temporal prior.

  2. **Create `src/aquapose/core/midline/orientation.py`** with:

     ```python
     def resolve_orientation(
         midline_points_by_cam: dict[str, np.ndarray],
         forward_luts: dict[str, ForwardLUT],
         velocity: tuple[float, float] | None,
         prev_orientation: int | None,
         speed: float,
         config: OrientationConfigLike,
     ) -> tuple[dict[str, np.ndarray], int]:
     ```

     **Parameters:**
     - `midline_points_by_cam`: camera_id -> (N, 2) midline points for one fish in one frame. N=15 points from head to tail (or tail to head — ambiguous).
     - `forward_luts`: per-camera ForwardLUT for ray back-projection.
     - `velocity`: OC-SORT Kalman-filtered velocity vector (du/dt, dv/dt) in pixels/frame from the primary camera tracklet. None if unavailable.
     - `prev_orientation`: +1 (as-is) or -1 (flipped) from previous frame. None if first frame.
     - `speed`: magnitude of velocity in pixels/frame.
     - `config`: orientation-related config fields.

     **Returns:** (corrected midline_points_by_cam, orientation) where orientation is +1 or -1.

     **Algorithm:**
     a. **Cross-camera geometric vote** (signal 1):
        - Triangulate head point (index 0) across all cameras with >=2 views: compute mean pairwise ray-ray distance for point[0] across cameras.
        - Triangulate tail point (index -1) across all cameras.
        - Flip all midlines (reverse point order) and repeat.
        - Score = total ray convergence error. Lower = better. vote_geometric = +1 if original is better, -1 if flipped.
        - If only 1 camera available, skip this signal (weight=0).

     b. **Velocity alignment** (signal 2):
        - If `speed >= config.speed_threshold` and `velocity` is not None:
          - Compute midline direction vector from point[0] to point[-1] in pixel space (use primary camera).
          - Compute alignment = dot(midline_direction, velocity) / (|midline_direction| * |velocity|).
          - vote_velocity = +1 if alignment > 0 (midline head aligns with direction of travel), -1 otherwise.
        - If speed < threshold: skip this signal (weight=0).

     c. **Temporal prior** (signal 3):
        - If `prev_orientation` is not None: vote_temporal = prev_orientation.
        - If None (first frame): skip this signal (weight=0).

     d. **Combine**:
        - weighted_sum = w_geo * vote_geo + w_vel * vote_vel + w_temp * vote_temp (only include signals that are active).
        - orientation = +1 if weighted_sum >= 0, -1 otherwise.
        - If all signals skipped (single camera, no velocity, first frame): default +1.

     e. **Apply per-camera flip**: After consensus orientation is determined, check each camera's midline direction individually against the consensus. For each camera whose midline is anti-aligned with consensus (i.e., its geometric vote disagrees), flip (reverse) that camera's midline point array. Do NOT apply a global flip to all cameras — only flip the ones that disagree with the consensus orientation.

  3. **OrientationConfigLike Protocol** (IB-003 pattern): `speed_threshold`, `orientation_weight_geometric`, `orientation_weight_velocity`, `orientation_weight_temporal`.

  4. **Update `__init__.py`**: Export `resolve_orientation`.

  5. **Tests in `test_orientation.py`**:
     - Test geometric vote: 3 cameras with consistent midlines -> no flip.
     - Test geometric vote: flipped midline in one camera produces flip consensus.
     - Test velocity gating: below speed threshold, velocity signal ignored.
     - Test temporal prior: maintains previous orientation when other signals weak.
     - Test single camera: falls back to velocity+temporal only.
     - Test first frame with single camera: defaults to +1.
     - Use mock ForwardLUTs with predictable rays.
  </action>
  <verify>
    <automated>hatch run test -- tests/unit/core/midline/test_orientation.py -x -v</automated>
  </verify>
  <done>
  - `resolve_orientation()` combines 3 signals with configurable weights
  - Velocity signal correctly gated by speed threshold
  - Temporal prior maintained across frames
  - All orientation tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Update MidlineStage to process only tracklet-group detections with orientation</name>
  <files>
    src/aquapose/core/midline/stage.py
  </files>
  <action>
  Rewrite `MidlineStage.run()` to use tracklet_groups for selective processing:

  1. **Read tracklet_groups** from context (alongside detections and camera_ids).

  2. **Build a frame-indexed lookup** from tracklet_groups:
     For each frame_idx, for each TrackletGroup, collect which (camera_id, detection_index) pairs belong to this fish. Use the tracklet's frames and frame_status to map: for a given frame, the tracklet's centroid corresponds to the detection at that frame in the detections list. Detection matching: find the detection in `detections[frame_idx][cam_id]` whose centroid is closest to the tracklet centroid (within a small tolerance, e.g. 5px). Record the detection index.

  3. **Process only matched detections**: For each frame, only run the midline backend on detections that matched a tracklet in a tracklet group. Skip unmatched detections (no fish identity — wasteful to process).

  4. **Apply orientation resolution**: After midline extraction, call `resolve_orientation()` for each fish (TrackletGroup) in each frame:
     - Collect the extracted midline points for this fish across cameras.
     - Get velocity from the primary camera's tracklet (camera with most detected frames, or first camera).
     - Get previous frame's orientation from a running dict keyed by fish_id.
     - Compute speed from centroid displacement.
     - Call `resolve_orientation()` and apply flips.

  5. **Populate annotated_detections**: Same structure as before (`list[dict[str, list]]`), but only containing detections from tracklet groups, with orientation-corrected midlines.

  6. **Handle the case** where `tracklet_groups` is empty or None: fall back to the original behavior (process all detections, no orientation). Log a warning.

  7. **Forward LUT loading**: Load ForwardLUTs at the start of run() using `load_forward_luts()` from `aquapose.calibration.luts`. This requires access to `calibration_path` and `lut` config. Add `lut_config` and `calibration_path` as optional constructor parameters to MidlineStage (or accept the full pipeline config). Since MidlineStage already takes `calibration_path`, just add an optional `lut_config` parameter (default None). When None, skip orientation (no LUTs available).

  8. **Update `build_stages()` in pipeline.py**: Pass `lut_config=config.lut` when constructing MidlineStage, so it can load ForwardLUTs. This is a small change to pipeline.py — add the kwarg.

  IMPORTANT: The `MidlineStage.__init__` constructor changes must be backward-compatible. The new `lut_config` parameter defaults to None, so existing construction sites (tests, etc.) continue to work without modification.
  </action>
  <verify>
    <automated>hatch run test -- tests/unit/core/midline/ -x -v</automated>
  </verify>
  <done>
  - MidlineStage.run() reads tracklet_groups and processes only matched detections
  - Orientation resolver is called per fish per frame using 3-signal combination
  - annotated_detections contains orientation-corrected midlines
  - Fallback behavior preserved when tracklet_groups is empty/None
  </done>
</task>

</tasks>

<verification>
```bash
# Midline tests
hatch run test -- tests/unit/core/midline/ -x -v

# Orientation tests specifically
hatch run test -- tests/unit/core/midline/test_orientation.py -x -v

# Full association + midline test sweep
hatch run test -- tests/unit/core/ -x -q

# Type checking
hatch run typecheck

# Lint
hatch run lint
```
</verification>

<success_criteria>
- MidlineStage processes only detections belonging to tracklet groups
- Head-tail orientation is resolved using 3 weighted signals
- Velocity signal is correctly gated by speed threshold
- Camera disagreeing with consensus has its midline flipped, not excluded
- annotated_detections carry orientation-corrected midlines
- All existing and new tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/26-association-refinement-and-pipeline-wiring/26-02-SUMMARY.md`
</output>
