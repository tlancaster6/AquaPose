---
phase: 14-golden-data-and-verification-framework
plan: 02
type: execute
wave: 2
depends_on:
  - 14-01
files_modified:
  - tests/golden/__init__.py
  - tests/golden/conftest.py
  - tests/golden/test_stage_harness.py
autonomous: true
requirements:
  - VER-02

must_haves:
  truths:
    - "A test can instantiate any Stage, call stage.run(context), and assert output fields in PipelineContext"
    - "Structural assertions verify correct keys, shapes, and dtypes in stage output"
    - "Numerical assertions verify output values match golden data within tolerance (atol=1e-3)"
    - "Tests are marked @slow and skipped in normal CI runs"
  artifacts:
    - path: "tests/golden/__init__.py"
      provides: "Golden test package init"
    - path: "tests/golden/conftest.py"
      provides: "Shared fixtures for golden data loading and PipelineContext construction"
      min_lines: 40
    - path: "tests/golden/test_stage_harness.py"
      provides: "Interface test harness with one test per v1.0 stage"
      min_lines: 100
  key_links:
    - from: "tests/golden/conftest.py"
      to: "tests/golden/*.pt"
      via: "torch.load loads golden fixture files"
      pattern: "torch\\.load"
    - from: "tests/golden/test_stage_harness.py"
      to: "src/aquapose/engine/stages.py"
      via: "Tests construct PipelineContext and verify stage outputs"
      pattern: "PipelineContext"
    - from: "tests/golden/test_stage_harness.py"
      to: "tests/golden/conftest.py"
      via: "Pytest fixtures provide golden data and pre-populated contexts"
      pattern: "golden_|loaded_"
---

<objective>
Create the interface test harness that loads golden data fixtures and asserts that stage outputs match the frozen reference. Each test instantiates a stage concept (or calls the v1.0 stage function), feeds it the golden input context, and verifies both structural correctness and numerical equivalence.

Purpose: This harness is the regression safety net for Phase 15 stage migrations. When stages are ported to the new engine, the same tests will verify the ported implementations produce equivalent results.
Output: `tests/golden/test_stage_harness.py` with structural + numerical tests for all 5 stages.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-golden-data-and-verification-framework/14-CONTEXT.md
@.planning/phases/14-golden-data-and-verification-framework/14-01-SUMMARY.md
@src/aquapose/pipeline/stages.py (v1.0 stage functions — what tests will call)
@src/aquapose/engine/stages.py (PipelineContext — what tests will assert against)

<interfaces>
<!-- Key interfaces for the test harness -->

From src/aquapose/engine/stages.py:
```python
@dataclass
class PipelineContext:
    frame_count: int | None = None
    camera_ids: list[str] | None = None
    detections: list[dict[str, list]] | None = None
    masks: list[dict[str, list]] | None = None
    tracks: list[list] | None = None
    midline_sets: list | None = None
    midlines_3d: list[dict] | None = None
    stage_timing: dict[str, float] = field(default_factory=dict)
    def get(self, field_name: str) -> object: ...
```

From src/aquapose/pipeline/stages.py:
```python
def run_detection(video_set, stop_frame=None, detector_kind="mog2", **detector_kwargs) -> list[dict[str, list[Detection]]]
def run_segmentation(detections_per_frame, video_set, segmentor, stop_frame=None) -> list[dict[str, list[tuple[np.ndarray, CropRegion]]]]
def run_tracking(detections_per_frame, models, tracker) -> list[list[FishTrack]]
def run_midline_extraction(tracks_per_frame, masks_per_frame, detections_per_frame, extractor) -> list[MidlineSet]
def run_triangulation(midline_sets, models) -> list[dict[int, Midline3D]]
```

Golden data files (from Plan 14-01):
- tests/golden/golden_detection.pt — list[dict[str, list[Detection]]]
- tests/golden/golden_segmentation.pt — list[dict[str, list[tuple[ndarray, CropRegion]]]]
- tests/golden/golden_tracking.pt — list[list[FishTrack]]
- tests/golden/golden_midline_extraction.pt — list[MidlineSet]
- tests/golden/golden_triangulation.pt — list[dict[int, Midline3D]]
- tests/golden/metadata.pt — dict with seed, versions, camera_ids, frame_count
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create golden test fixtures and conftest</name>
  <files>
    tests/golden/__init__.py
    tests/golden/conftest.py
  </files>
  <action>
Create `tests/golden/__init__.py` with a module docstring: `"""Golden data regression tests for v1.0 pipeline equivalence."""`

Create `tests/golden/conftest.py` with shared pytest fixtures:

1. **`GOLDEN_DIR`** — module-level constant: `Path(__file__).parent` (the tests/golden/ directory).

2. **`_check_golden_data_exists()`** — helper function that checks if golden `.pt` files exist. Returns True if all expected files are present. Used by fixtures to skip tests gracefully.

3. **`golden_metadata` fixture** (session scope):
   - Load `metadata.pt` from GOLDEN_DIR via `torch.load(..., weights_only=False)`
   - If file does not exist, `pytest.skip("Golden data not generated — run scripts/generate_golden_data.py first")`
   - Return the metadata dict

4. **`golden_detections` fixture** (session scope):
   - Load `golden_detection.pt`, skip if missing
   - Return the detections list

5. **`golden_masks` fixture** (session scope):
   - Load `golden_segmentation.pt`, skip if missing
   - Return the masks list

6. **`golden_tracks` fixture** (session scope):
   - Load `golden_tracking.pt`, skip if missing
   - Return the tracks list

7. **`golden_midlines` fixture** (session scope):
   - Load `golden_midline_extraction.pt`, skip if missing
   - Return the midline_sets list

8. **`golden_triangulation` fixture** (session scope):
   - Load `golden_triangulation.pt`, skip if missing
   - Return the midlines_3d list

9. **`DEFAULT_ATOL` constant** — `1e-3` (the default numerical tolerance from CONTEXT.md decisions).

All fixtures use `weights_only=False` in `torch.load()` because the golden data contains custom objects (Detection, CropRegion, FishTrack, Midline3D, etc.).

Mark all fixtures with `@pytest.fixture(scope="session")` for efficiency — golden data is read-only and expensive to load.
  </action>
  <verify>
    <automated>cd C:/Users/tucke/PycharmProjects/AquaPose && python -c "import ast; ast.parse(open('tests/golden/conftest.py').read()); print('conftest syntax OK')"</automated>
  </verify>
  <done>tests/golden/ package exists with conftest.py providing session-scoped fixtures for all golden data files.</done>
</task>

<task type="auto">
  <name>Task 2: Create interface test harness for all 5 stages</name>
  <files>
    tests/golden/test_stage_harness.py
  </files>
  <action>
Create `tests/golden/test_stage_harness.py` with structural and numerical regression tests for each v1.0 stage output.

**All tests** must be decorated with `@pytest.mark.slow` so they are skipped in normal test runs (`hatch run test`) and only run via `hatch run test-all`.

**Test structure per stage — both structural and numerical assertions:**

**1. `test_detection_structure(golden_detections, golden_metadata)`**
- Assert `golden_detections` is a list
- Assert length equals `golden_metadata["frame_count"]`
- Assert each element is a dict mapping str keys (camera IDs) to lists
- Assert camera IDs in first frame match `golden_metadata["camera_ids"]` (or a subset — some cameras may have 0 detections for some frames, which is normal)
- For each Detection in the first frame's first camera that has detections:
  - Assert it has `bbox` attribute (list of 4 floats) and `confidence` attribute (float)

**2. `test_segmentation_structure(golden_masks, golden_metadata)`**
- Assert `golden_masks` is a list of length `golden_metadata["frame_count"]`
- Assert each element is a dict mapping str keys to lists
- For each (mask, crop_region) tuple in the first camera's first frame:
  - Assert mask is an ndarray with dtype uint8
  - Assert mask has 2 dimensions (height, width)
  - Assert crop_region has `x`, `y`, `width`, `height` attributes (all ints)

**3. `test_tracking_structure(golden_tracks, golden_metadata)`**
- Assert `golden_tracks` is a list of length `golden_metadata["frame_count"]`
- Assert each element is a list (of FishTrack objects)
- For each FishTrack in the first non-empty frame:
  - Assert it has `fish_id` attribute (int) and `centroid_3d` attribute (ndarray of shape (3,))

**4. `test_midline_extraction_structure(golden_midlines, golden_metadata)`**
- Assert `golden_midlines` is a list of length `golden_metadata["frame_count"]`
- Each element is a MidlineSet: `dict[int, dict[str, Midline2D]]`
- For each fish_id in the first non-empty MidlineSet:
  - Assert the inner dict maps camera_id (str) to Midline2D objects
  - Assert each Midline2D has a `points` attribute (ndarray of shape (N, 2))

**5. `test_triangulation_structure(golden_triangulation, golden_metadata)`**
- Assert `golden_triangulation` is a list of length `golden_metadata["frame_count"]`
- Each element is a dict mapping int (fish_id) to Midline3D
- For each Midline3D in the first non-empty frame:
  - Assert it has `control_points` attribute (ndarray of shape (K, 3))
  - Assert it has `camera_ids` attribute (list of str)

**6. `test_detection_numerical_stability(golden_detections)`**
- Verify golden detections contain valid numerical data:
  - All bbox coordinates are finite (no NaN/Inf)
  - All confidence values are in [0, 1]
  - At least one frame has at least one detection

**7. `test_segmentation_numerical_stability(golden_masks)`**
- Verify golden masks contain valid data:
  - All mask values are either 0 or 255
  - At least one mask has non-zero pixels

**8. `test_triangulation_numerical_stability(golden_triangulation)`**
- Verify golden triangulation contains valid 3D data:
  - All control points are finite
  - Control points are within reasonable tank bounds (no coordinates > 10m for a fish tank)
  - At least one frame has at least one triangulated midline

**9. `test_metadata_completeness(golden_metadata)`**
- Assert all required keys exist: `seed`, `stop_frame`, `detector_kind`, `max_fish`, `torch_version`, `numpy_version`, `camera_ids`, `frame_count`, `generation_timestamp`
- Assert `seed` is an int, `frame_count` > 0, `camera_ids` is a non-empty list

**Important notes:**
- These tests validate the GOLDEN DATA itself (structure and numerical sanity). They are NOT yet testing ported Stage implementations — that comes in Phase 15-16 when the harness is reused.
- The harness is designed so that in Phase 15, the same structural assertions can be applied to the output of `NewStage.run(context)` vs the golden reference.
- Use `numpy.testing.assert_allclose` with `atol=DEFAULT_ATOL` for any numerical comparisons.
- When iterating over detections/masks/tracks, find the first non-empty frame (not all frames may have data for all cameras — this is normal behavior per project memory).
- Import `numpy as np` and `pytest`. Import domain types inside test functions (not at module level) to avoid import failures if the golden data format changes.
  </action>
  <verify>
    <automated>cd C:/Users/tucke/PycharmProjects/AquaPose && python -c "import ast; ast.parse(open('tests/golden/test_stage_harness.py').read()); print('harness syntax OK')"</automated>
  </verify>
  <done>Test harness exists with 9 tests (5 structural + 3 numerical stability + 1 metadata completeness), all marked @slow. Tests skip gracefully if golden data not yet generated.</done>
</task>

</tasks>

<verification>
1. `tests/golden/__init__.py` and `conftest.py` exist with proper fixtures
2. `test_stage_harness.py` has 9 tests all marked `@pytest.mark.slow`
3. Tests skip gracefully when golden data files are missing
4. When golden data exists: `hatch run test-all tests/golden/ -x` passes all tests
5. Structural assertions check keys, shapes, dtypes — not just "is not None"
6. Numerical assertions verify finite values and reasonable ranges
</verification>

<success_criteria>
- Test harness can load golden data via session-scoped fixtures
- Each of the 5 stages has structural validation tests
- Numerical stability tests verify golden data is sane
- All tests are @slow-marked for CI compatibility
- Tests gracefully skip when golden data hasn't been generated yet
</success_criteria>

<output>
After completion, create `.planning/phases/14-golden-data-and-verification-framework/14-02-SUMMARY.md`
</output>
