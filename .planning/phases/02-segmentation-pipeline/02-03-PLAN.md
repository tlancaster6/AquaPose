---
phase: 02-segmentation-pipeline
plan: 03
type: execute
wave: 3
depends_on: ["02-01", "02-02"]
files_modified:
  - src/aquapose/segmentation/model.py
  - src/aquapose/segmentation/training.py
  - src/aquapose/segmentation/__init__.py
  - tests/unit/segmentation/test_model.py
  - tests/unit/segmentation/test_training.py
autonomous: true
requirements:
  - SEG-04
  - SEG-05

must_haves:
  truths:
    - "Inference pipeline has separate detect(), crop(), segment() callable stages"
    - "segment() returns crop-space mask + bbox/crop metadata, not full-frame mask"
    - "Batch frame API accepts multiple frames for GPU throughput"
    - "Default confidence threshold is 0.1 for high recall"
    - "Training uses per-camera stratified split from dataset.py"
    - "ImageNet-pretrained ResNet-50 backbone with standard augmentation"
  artifacts:
    - path: "src/aquapose/segmentation/model.py"
      provides: "MaskRCNNSegmentor with separate segment() returning crop-space masks + CropRegion metadata"
    - path: "src/aquapose/segmentation/training.py"
      provides: "Training script using stratified_split, variable-size crops"
    - path: "src/aquapose/segmentation/__init__.py"
      provides: "Updated public API with all inference stage functions"
  key_links:
    - from: "MaskRCNNSegmentor.segment"
      to: "crop.py paste_mask"
      via: "caller reconstructs full-frame using returned CropRegion"
      pattern: "paste_mask.*region"
    - from: "training.py train()"
      to: "dataset.py stratified_split"
      via: "per-camera val split"
      pattern: "stratified_split"
---

<objective>
Update Mask R-CNN model and training to use variable-size crops, separate inference stages, and crop-space output.

Purpose: CONTEXT.md specifies separate callable stages (detect, crop, segment), crop-space mask output with metadata for caller reconstruction, batch frame API, and variable crop dimensions. The training pipeline needs to use per-camera stratified splits and support the new variable-size dataset.

Output: Updated model.py and training.py with a clean inference API and training pipeline matching all CONTEXT.md decisions.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-segmentation-pipeline/02-CONTEXT.md
@.planning/phases/02-segmentation-pipeline/02-RESEARCH.md
@.planning/phases/02-segmentation-pipeline/02-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Refactor MaskRCNNSegmentor for separate stages and crop-space output</name>
  <files>
    src/aquapose/segmentation/model.py
    src/aquapose/segmentation/__init__.py
    tests/unit/segmentation/test_model.py
  </files>
  <action>
Update `MaskRCNNSegmentor` in `model.py`:

1. **Update SegmentationResult**: Change to return crop-space mask + metadata:
   ```python
   @dataclass
   class SegmentationResult:
       bbox: tuple[int, int, int, int]  # (x1, y1, x2, y2) in crop-space coords
       mask: np.ndarray  # binary mask in crop-space (uint8, 0/255), NOT full-frame
       confidence: float
       label: int
       crop_region: CropRegion  # metadata for full-frame reconstruction via paste_mask()
   ```
   Remove `mask_rle` field. Callers who need RLE can encode themselves. The crop_region allows callers to reconstruct full-frame masks via `paste_mask(result.mask, result.crop_region)`.

2. **Add `segment()` method** — the primary inference entry point:
   ```python
   def segment(
       self,
       crops: list[np.ndarray],
       crop_regions: list[CropRegion],
   ) -> list[list[SegmentationResult]]:
   ```
   Accepts pre-cropped images (from `extract_crop()`) and their associated CropRegion metadata. Returns per-crop detection results with crop-space masks. This is the "segment" stage in the detect -> crop -> segment pipeline.

3. **Update `predict()` for backward compatibility**: Keep `predict(images)` but have it internally call `segment()`. When called without crop_regions, create trivial CropRegion covering the full image.

4. **Batch frame API**: The `segment()` method already accepts a list (batch). Ensure it processes all crops in a single forward pass through the model for GPU throughput. Handle the case where crops have different sizes (torchvision Mask R-CNN handles this natively via FPN).

5. **Confidence threshold**: Already defaults to 0.1 — verify and keep.

6. **Import CropRegion**: Add `from .crop import CropRegion` to model.py.

7. **Update __init__.py** exports: Ensure `SegmentationResult` with new fields is exported.

8. Update tests:
   - Test `segment()` with mock model output, verify crop-space masks returned
   - Test that `crop_region` is correctly attached to results
   - Test batch inference (multiple crops of different sizes)
   - Test backward-compatible `predict()` still works
  </action>
  <verify>
- `hatch run test` passes
- `hatch run lint` passes
- `hatch run typecheck` passes
- `SegmentationResult` has `mask` (ndarray) and `crop_region` (CropRegion) fields, no `mask_rle`
- `segment()` method exists on MaskRCNNSegmentor
- `predict()` still works for backward compatibility
  </verify>
  <done>MaskRCNNSegmentor has separate segment() method returning crop-space masks with CropRegion metadata. Batch inference works with variable-size crops.</done>
</task>

<task type="auto">
  <name>Task 2: Update training pipeline for stratified split and variable crops</name>
  <files>
    src/aquapose/segmentation/training.py
    tests/unit/segmentation/test_training.py
  </files>
  <action>
Update `training.py`:

1. **Use stratified_split**: Replace `random_split` with `stratified_split` from `dataset.py` (created in Plan 02). Import and call:
   ```python
   from .dataset import CropDataset, stratified_split
   train_indices, val_indices = stratified_split(full_dataset, val_fraction=val_split, seed=42)
   ```
   Use `torch.utils.data.Subset` with these indices.

2. **Variable-size crop handling**: The DataLoader already uses `_collate_fn` returning tuples (not stacked tensors), so variable-size crops are handled. Verify this works correctly when crops have different aspect ratios.

3. **Add training configuration dataclass** (optional but clean):
   ```python
   @dataclass
   class TrainConfig:
       epochs: int = 40
       batch_size: int = 4
       lr: float = 0.005
       val_split: float = 0.2
       device: str | None = None
   ```
   This makes the `train()` signature cleaner. Claude's discretion on whether to implement this — if it adds too much churn, keep the current kwarg style.

4. **Augmentation**: Keep current augmentation (flips, rotations, brightness/contrast jitter) in CropDataset. Ensure augmentation handles variable-size images correctly (the existing numpy-based augmentation already does).

5. **Evaluation update**: Update `evaluate()` to return results compatible with the new `SegmentationResult` format. The evaluation still computes IoU between predicted and ground-truth masks — ensure it works with variable-size crops.

6. **Train/val COCO split files**: Update `train()` to accept optional `train_json` and `val_json` paths. If provided, load separate datasets from these files (generated by build_training_data.py). If not provided, fall back to `stratified_split` on the single COCO JSON.

7. Update tests:
   - Test training with variable-size crops (synthetic COCO with different image dimensions)
   - Test that stratified_split is called (mock or verify split proportions)
   - Test train/val separate JSON file loading
  </action>
  <verify>
- `hatch run test` passes
- `hatch run lint` passes
- `hatch run typecheck` passes
- `train()` uses `stratified_split` instead of `random_split`
- `train()` accepts optional `train_json`/`val_json` parameters
- Training works with variable-size crop images
  </verify>
  <done>Training pipeline uses per-camera stratified split, handles variable-size crops, and accepts pre-split COCO JSON files.</done>
</task>

</tasks>

<verification>
1. `hatch run check` (lint + typecheck) passes
2. `hatch run test` passes
3. MaskRCNNSegmentor.segment() returns crop-space masks with CropRegion
4. Training uses stratified_split (not random_split)
5. Variable-size crops work end-to-end (dataset -> training -> inference)
6. Confidence threshold defaults to 0.1
</verification>

<success_criteria>
- Separate detect/crop/segment stages available as callable functions
- segment() returns crop-space mask + CropRegion metadata
- Callers reconstruct full-frame via paste_mask(result.mask, result.crop_region)
- Batch frame API processes multiple crops in single forward pass
- Training uses per-camera stratified split
- All tests pass with variable-size crop images
</success_criteria>

<output>
After completion, create `.planning/phases/02-segmentation-pipeline/02-03-SUMMARY.md`
</output>
