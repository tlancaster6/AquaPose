---
phase: 02.1-segmentation-troubleshooting
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/test_mog2.py
autonomous: true
requirements:
  - SEG-1
must_haves:
  truths:
    - "Side-by-side annotated stills exist for every sampled frame showing detections + raw MOG2 mask"
    - "Detection count per frame is printed for visual cross-reference"
  artifacts:
    - path: "scripts/test_mog2.py"
      provides: "Consolidated MOG2 diagnostic script with numeric metrics and visual output"
      contains: "MOG2Detector"
  key_links:
    - from: "scripts/test_mog2.py"
      to: "src/aquapose/segmentation/detector.py"
      via: "MOG2Detector import and detect() calls"
      pattern: "from aquapose.segmentation.detector import MOG2Detector"
---

<objective>
Consolidate the two existing MOG2 diagnostic scripts into a single test script, run it on 2 default cameras (center + one side), and produce annotated stills for visual quality assessment.

Purpose: MOG2 is the foundation of the segmentation pipeline. If it misses fish, SAM2 never sees them and Mask R-CNN training data is incomplete. This plan validates the existing implementation on real data with visual output for human review.

Output: `scripts/test_mog2.py` producing annotated stills; output images in `output/test_mog2/`
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02.1-segmentation-troubleshooting/02.1-RESEARCH.md

@scripts/diagnose_mog2.py
@scripts/verify_mog2_recall.py
@src/aquapose/segmentation/detector.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Consolidate MOG2 diagnostic scripts and run on all 13 cameras</name>
  <files>scripts/test_mog2.py</files>
  <action>
Create `scripts/test_mog2.py` by merging logic from `scripts/diagnose_mog2.py` and `scripts/verify_mog2_recall.py`. The consolidated script must:

1. **Per-camera sampling** (from verify_mog2_recall.py):
   - Accept `--data-root`, `--output-dir`, `--warmup-frames` (default 500), `--sample-count` (default 10), `--cameras` (default `e3v8250,e3v83eb`) CLI args
   - Discover camera MP4 files in `{data-root}/raw_videos/` matching the `--cameras` list (default: center camera e3v8250 + side camera e3v83eb; pass `all` to run every camera)
   - For each camera: warm up MOG2, feed intermediate frames to keep model updated, sample `sample_count` evenly-spaced frames after warmup
   - Call `detector.detect(frame)` on each sampled frame

2. **Side-by-side visualization** (from diagnose_mog2.py):
   - For each sampled frame, produce a side-by-side image:
     - Left panel: original frame with detection bbox overlays (green rectangles) and detection count text
     - Right panel: colorized raw MOG2 mask (green=foreground 255, blue-ish=shadow 127, colored outlines=detection contours)
   - Use `detector._mog2.apply(frame, learningRate=0)` for the raw mask visualization (as diagnose_mog2.py already does — this is acceptable in a diagnostic script)
   - Scale both panels to half-size and hstack; save as JPEG to `{output-dir}/{camera_id}/frame_{idx:06d}.jpg`

3. **Detection count logging**:
   - Print detection count per frame as the script runs (e.g., `e3v8250 frame 000042: 7 detections`)
   - At the end, print a simple per-camera summary: avg/min/max detections across sampled frames
   - No automated pass/fail — quality is assessed visually from the output stills

4. **Use default MOG2Detector parameters** (history=500, var_threshold=12, min_area=200) — these are the production defaults. Do NOT use the tuned parameters from verify_pseudo_labels.py (those were specific to that script's needs).

After creating the script, run it with defaults (2 cameras):
```
hatch run python scripts/test_mog2.py --data-root "C:/Users/tucke/Desktop/Aqua/AquaPose" --output-dir output/test_mog2
```

Document in the plan summary: (a) the existing 2-stage shadow fix in detector.py and what it does, (b) detection count observations, (c) whether any parameter changes were needed.
  </action>
  <verify>
- `scripts/test_mog2.py` exists and runs without error
- Output images exist in `output/test_mog2/{camera_id}/` for both default cameras
- Per-camera detection count summary is printed to stdout
  </verify>
  <done>
MOG2 detection validated on real data with annotated side-by-side stills for default cameras. Detection counts logged per frame.
  </done>
</task>

</tasks>

<verification>
- `hatch run python scripts/test_mog2.py --help` runs without import errors
- Output directory contains camera subdirectories with JPEG stills for both default cameras
- Detection count summary printed to stdout
</verification>

<success_criteria>
Annotated side-by-side stills produced for default cameras (e3v8250, e3v83eb) with detection counts logged. Quality assessed visually by user.
</success_criteria>

<output>
After completion, create `.planning/phases/02.1-segmentation-troubleshooting/02.1-01-SUMMARY.md`
</output>
