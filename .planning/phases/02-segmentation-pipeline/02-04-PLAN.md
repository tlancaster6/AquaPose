---
phase: 02-segmentation-pipeline
plan: 04
type: execute
wave: 3
depends_on: ["02-02", "02-03"]
files_modified:
  - scripts/build_training_data.py
  - src/aquapose/segmentation/__init__.py
autonomous: false
requirements:
  - SEG-01
  - SEG-02
  - SEG-04
  - SEG-05

must_haves:
  truths:
    - "Full pipeline runs end-to-end: video -> YOLO detect -> SAM2 pseudo-label -> filter -> COCO dataset -> Mask R-CNN train -> inference"
    - "Separate detect/crop/segment stages work independently"
    - "Pipeline accepts N fish per frame (variable-length detection lists)"
    - "Crop dimensions match bbox (not fixed 256x256)"
  artifacts:
    - path: "scripts/build_training_data.py"
      provides: "Complete CLI: generate subcommand produces COCO dataset, train subcommand trains Mask R-CNN"
    - path: "src/aquapose/segmentation/__init__.py"
      provides: "Clean public API with all required symbols"
  key_links:
    - from: "scripts/build_training_data.py generate"
      to: "SAMPseudoLabeler.predict + filter_mask + to_coco_dataset"
      via: "pipeline chain"
      pattern: "labeler\\.predict.*filter_mask.*to_coco_dataset"
    - from: "scripts/build_training_data.py train"
      to: "training.train + training.evaluate"
      via: "model training and evaluation"
      pattern: "train\\(.*\\).*evaluate\\("
---

<objective>
Integrate all updated components and verify the full pipeline works end-to-end with a human checkpoint.

Purpose: Plans 01-03 modified multiple components independently. This plan wires them together in build_training_data.py, ensures the full pipeline chain works, and gets human verification.

Output: Working build_training_data.py that orchestrates the complete segmentation pipeline from raw video to trained Mask R-CNN.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-segmentation-pipeline/02-CONTEXT.md
@.planning/phases/02-segmentation-pipeline/02-01-SUMMARY.md
@.planning/phases/02-segmentation-pipeline/02-02-SUMMARY.md
@.planning/phases/02-segmentation-pipeline/02-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire updated components into build_training_data.py and finalize API</name>
  <files>
    scripts/build_training_data.py
    src/aquapose/segmentation/__init__.py
  </files>
  <action>
1. **Update build_training_data.py `generate` subcommand** to use the module's `filter_mask` function instead of local implementation. Remove any locally duplicated functions that now exist in the module (`_largest_connected_component`, `_filter_mask`).

2. **Integrate negative example generation**: After processing all cameras, add background crop generation:
   - Count positive crops generated
   - Calculate target negative count = positive_count * neg_fraction / (1 - neg_fraction) to achieve ~10% negatives in final dataset
   - Sample random regions from frames with zero detections, or from frame margins away from any detection bbox
   - Save as crop images with no mask annotations in COCO JSON

3. **Integrate stratified split**: After generating the full COCO JSON:
   - Import `stratified_split` from `aquapose.segmentation.dataset`
   - Split into train/val indices
   - Write `train.json` and `val.json` alongside `coco_annotations.json`

4. **Update `train` subcommand**: Accept `--train-json` and `--val-json` optional arguments. If provided, pass to `train()` as separate train/val COCO files. If not provided, pass the combined JSON and let `train()` split internally.

5. **Add `--draw-pseudolabels` flag** to the `generate` subcommand: passes through to SAMPseudoLabeler constructor for debug visualization.

6. **Final __init__.py audit**: Verify all public symbols are exported:
   - From detector: `Detection`, `MOG2Detector`, `YOLODetector`, `make_detector`
   - From pseudo_labeler: `SAMPseudoLabeler`, `FrameAnnotation`, `AnnotatedFrame`, `filter_mask`, `to_coco_dataset`
   - From model: `MaskRCNNSegmentor`, `SegmentationResult`
   - From crop: `CropRegion`, `compute_crop_region`, `extract_crop`, `paste_mask`
   - From dataset: `CropDataset`, `stratified_split`
   - From training: `train`, `evaluate`

7. Run full test suite: `hatch run test` and `hatch run check`.
  </action>
  <verify>
- `hatch run test` passes
- `hatch run check` (lint + typecheck) passes
- `python scripts/build_training_data.py generate --help` shows all expected arguments including `--neg-fraction` and `--draw-pseudolabels`
- `python scripts/build_training_data.py train --help` shows `--train-json` and `--val-json` arguments
- All symbols listed in step 6 are importable from `aquapose.segmentation`
  </verify>
  <done>build_training_data.py wired to all updated module components. Negative examples, stratified split, and visualization flag integrated. Public API complete.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Human verification of full pipeline</name>
  <files>scripts/build_training_data.py</files>
  <action>Human verifies the full pipeline by running build_training_data.py generate and train subcommands on real data. No code changes in this task â€” purely verification.</action>
  <verify>
1. **Generate training data** (requires GPU + YOLO weights + raw videos):
   ```bash
   python scripts/build_training_data.py generate \
     --video-dir "C:/Users/tucke/Desktop/Aqua/AquaPose/raw_videos" \
     --output-dir output/training_data/ \
     --yolo-weights runs/detect/output/yolo_fish/train_v1/weights/best.pt \
     --max-frames 5 \
     --neg-fraction 0.1 \
     --draw-pseudolabels
   ```
2. **Inspect output**: Check `output/training_data/images/` for crop images (variable sizes, not 256x256). Check `output/training_data/coco_annotations.json`, `train.json`, `val.json` exist. Verify negative examples present (images with no annotations).
3. **Train Mask R-CNN** (short test):
   ```bash
   python scripts/build_training_data.py train \
     --coco-json output/training_data/coco_annotations.json \
     --image-root output/training_data/images/ \
     --output-dir output/maskrcnn/ \
     --epochs 5
   ```
4. **Verify training completes** without errors and outputs best_model.pth.
5. **Test inference** (optional quick check in Python):
   ```python
   from aquapose.segmentation import MaskRCNNSegmentor, compute_crop_region, extract_crop, paste_mask
   seg = MaskRCNNSegmentor(weights_path="output/maskrcnn/best_model.pth")
   # Load a test crop, run seg.segment([crop], [region])
   ```
  </how-to-verify>
  </verify>
  <done>Human confirms pipeline generates correct pseudo-labels, trains successfully, and produces plausible masks.</done>
  <resume-signal>Type "approved" if pipeline works, or describe issues found.</resume-signal>
</task>

</tasks>

<verification>
1. `hatch run check` passes
2. `hatch run test` passes
3. build_training_data.py runs end-to-end on real data
4. COCO JSON contains variable-size images and negative examples
5. Mask R-CNN trains and produces model checkpoint
6. Inference returns crop-space masks with CropRegion metadata
</verification>

<success_criteria>
- Full pipeline executable from a single CLI script (build_training_data.py)
- Human verifies: SAM2 pseudo-labels look correct, training converges, inference produces plausible masks
- All module symbols properly exported and documented
</success_criteria>

<output>
After completion, create `.planning/phases/02-segmentation-pipeline/02-04-SUMMARY.md`
</output>
