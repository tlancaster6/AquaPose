---
phase: 01-calibration-and-refractive-geometry
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/aquapose/calibration/uncertainty.py
  - src/aquapose/calibration/__init__.py
  - tests/unit/calibration/test_uncertainty.py
autonomous: true

must_haves:
  truths:
    - "Z-uncertainty report quantifies X/Y/Z reconstruction error as a function of tank depth for the 13-camera top-down geometry"
    - "Report shows error curves at uniform depth intervals across the full tank range"
    - "X and Y errors are substantially smaller than Z errors for top-down cameras (expected geometric property)"
    - "Report includes embedded matplotlib plots showing error vs. depth for X, Y, Z separately"
  artifacts:
    - path: "src/aquapose/calibration/uncertainty.py"
      provides: "Z-uncertainty analytical characterization"
      contains: "compute_triangulation_uncertainty"
    - path: "tests/unit/calibration/test_uncertainty.py"
      provides: "Unit tests for uncertainty calculations"
      contains: "test_compute_triangulation_uncertainty"
    - path: "docs/reports/z_uncertainty_report.md"
      provides: "Z-uncertainty characterization report for paper and optimizer weighting"
      contains: "X error|Y error|Z error|anisotropy"
  key_links:
    - from: "src/aquapose/calibration/uncertainty.py"
      to: "src/aquapose/calibration/projection.py"
      via: "Uses RefractiveProjectionModel.project() and cast_ray()"
      pattern: "RefractiveProjectionModel"
    - from: "src/aquapose/calibration/uncertainty.py"
      to: "aquacal.datasets.synthetic"
      via: "Uses generate_real_rig_array() for synthetic 13-camera rig geometry"
      pattern: "from aquacal\\.datasets\\.synthetic import"
---

<objective>
Implement the Z-uncertainty analytical characterization (CALIB-04) — a ray-simulation report quantifying X/Y/Z reconstruction error vs. tank depth for the 13-camera top-down geometry.

Purpose: Informs downstream optimizer weighting (Phase 4) by documenting how much worse Z estimation is than X/Y for top-down cameras. Also serves as a paper-quality figure for the methods section.

Output: `uncertainty.py` module with functions to compute and plot triangulation uncertainty, plus a generated markdown report with embedded plots.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-calibration-and-refractive-geometry/01-RESEARCH.md
@.planning/phases/01-calibration-and-refractive-geometry/01-01-SUMMARY.md

# Uses the projection module from Plan 01
@src/aquapose/calibration/projection.py
@src/aquapose/calibration/loader.py

# AquaCal synthetic rig generation — use for build_synthetic_rig()
@C:/Users/tucke/PycharmProjects/AquaCal/src/aquacal/datasets/synthetic.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement uncertainty computation and report generation</name>
  <files>
    src/aquapose/calibration/uncertainty.py
    src/aquapose/calibration/__init__.py
    tests/unit/calibration/test_uncertainty.py
  </files>
  <action>
Create the Z-uncertainty analytical characterization module. This is entirely new code (not ported from AquaMVS), using the ray-simulation approach described in the research doc.

**uncertainty.py** — Create with these functions:

`compute_triangulation_uncertainty(models, depths, pixel_noise=0.5) -> UncertaintyResult`:
- `models`: list of RefractiveProjectionModel instances (one per camera)
- `depths`: 1D tensor or array of Z values to evaluate (underwater depths, Z > water_z)
- `pixel_noise`: half-pixel perturbation magnitude (default 0.5px)
- For each depth Z:
  1. Create a ground-truth 3D point at (0, 0, Z) — tank center
  2. Project the point into all cameras using model.project()
  3. For each camera that sees the point (valid==True):
     - Add ±pixel_noise perturbation in X and Y pixel directions (4 perturbed pixels per camera)
     - Cast rays from perturbed pixels using model.cast_ray()
  4. Triangulate 3D point from all perturbed rays using least-squares (closest point to all rays — use SVD-based ray intersection)
  5. Measure |X_est - X_gt|, |Y_est - Y_gt|, |Z_est - Z_gt| error
  6. Also perturb individual cameras and measure per-camera contribution
- Return UncertaintyResult dataclass with: depths, x_errors, y_errors, z_errors, n_cameras_visible (arrays, one entry per depth)
- Also test at off-center points (e.g., at 0.1m, 0.2m offsets in X and Y) to capture spatial variation

`triangulate_rays(origins, directions) -> Tensor`:
- Least-squares closest point to multiple 3D rays
- Standard SVD approach: build the system sum_i (I - d_i @ d_i^T) @ p = sum_i (I - d_i @ d_i^T) @ o_i, solve with torch.linalg.lstsq
- Returns (3,) point tensor

`generate_uncertainty_report(result, output_dir) -> Path`:
- Takes UncertaintyResult and output directory
- Generates 3 matplotlib/seaborn plots:
  1. X/Y/Z error vs. depth (all three on one plot, depth on X-axis, error in mm on Y-axis)
  2. Z/X error ratio vs. depth (shows how much worse Z is than XY)
  3. Number of cameras visible vs. depth
- Saves plots as PNG files in output_dir
- Writes a markdown report summarizing: min/max/mean errors per axis, worst-case depth, Z/XY anisotropy ratio, camera visibility
- Use clean matplotlib styling (no grid clutter, labeled axes with units, legend)

`build_synthetic_rig() -> list[RefractiveProjectionModel]`:
- Use AquaCal's `aquacal.datasets.synthetic.generate_real_rig_array()` to create the 13-camera rig matching the actual aquarium hardware geometry (1 center + 6 inner ring + 6 outer ring, 1600x1200, ~750mm above water)
- Convert AquaCal's intrinsics/extrinsics/water_zs dicts into RefractiveProjectionModel instances using the same pattern as load_calibration_data
- AquaCal handles the realistic camera placement (ring radii, FOV, pointing angles) — do NOT hand-roll camera geometry
- Returns list of RefractiveProjectionModel instances
- This enables running the uncertainty analysis without a real calibration JSON file

**__init__.py** — Add exports: `compute_triangulation_uncertainty`, `generate_uncertainty_report`, `UncertaintyResult`, `triangulate_rays`

**tests/unit/calibration/test_uncertainty.py**:
- Test triangulate_rays with known geometry: two rays that intersect at a known point → returns that point within tolerance
- Test triangulate_rays with parallel rays: should return midpoint or handle gracefully
- Test build_synthetic_rig returns 13 models (uses aquacal.datasets.synthetic.generate_real_rig_array), each can project a point at tank center
- Test compute_triangulation_uncertainty returns results with correct shapes
- Test that X/Y errors are smaller than Z errors for the top-down geometry (this is the expected geometric property — top-down cameras have poor Z observability)
- Test that all depths in the input appear in the output
- Do NOT test plot generation (matplotlib output is visual, not unit-testable)
  </action>
  <verify>
Run: `hatch run test tests/unit/calibration/test_uncertainty.py` — all tests pass.

Run: `python -c "from aquapose.calibration.uncertainty import build_synthetic_rig, compute_triangulation_uncertainty; models = build_synthetic_rig(); import torch; result = compute_triangulation_uncertainty(models, torch.linspace(1.1, 1.5, 5)); print(f'Z errors: {result.z_errors}')"` — prints Z error values, confirming the pipeline runs end to end.

Run: `hatch run lint src/aquapose/calibration/uncertainty.py tests/unit/calibration/test_uncertainty.py` — no lint errors.
  </verify>
  <done>
uncertainty.py provides compute_triangulation_uncertainty (ray-simulation approach), triangulate_rays (SVD least-squares), generate_uncertainty_report (markdown + plots), and build_synthetic_rig (13-camera reference geometry). Tests verify: ray triangulation accuracy, correct output shapes, Z > XY error anisotropy for top-down cameras. The module runs end-to-end on synthetic rig data without requiring a real calibration file.
  </done>
</task>

<task type="auto">
  <name>Task 2: Generate the Z-uncertainty characterization report</name>
  <files>
    docs/reports/z_uncertainty_report.md
    docs/reports/z_uncertainty_xyz_error.png
    docs/reports/z_uncertainty_ratio.png
    docs/reports/z_uncertainty_cameras.png
  </files>
  <action>
Run the uncertainty analysis on the synthetic 13-camera rig and generate the final report artifact.

**Generate the report:**
- Use build_synthetic_rig() to create the 13-camera ring geometry
- Sample depths at 5cm intervals from water_z + 0.05 to water_z + 0.50 (approximately 10 depth samples covering the full usable tank range)
- Run compute_triangulation_uncertainty() with default 0.5px noise
- Call generate_uncertainty_report() to produce the markdown + plots

**Report location:** `docs/reports/z_uncertainty_report.md` with plot PNGs alongside it.

**Report content must include:**
- Table: depth (m) | X error (mm) | Y error (mm) | Z error (mm) | Z/XY ratio | cameras visible
- Summary statistics: best-case depth, worst-case depth, mean Z/XY anisotropy
- Interpretation: "Z uncertainty is ~Nx worse than XY for this top-down geometry, confirming that the optimizer should weight Z errors less aggressively than XY errors" (with actual N from the data)
- Three plots embedded as relative image links

**This task can be done as a simple script execution** — write a small runner script or use python -c to invoke the report generator. The report is a one-time artifact, not a recurring computation.
  </action>
  <verify>
Verify the report file exists: `ls docs/reports/z_uncertainty_report.md docs/reports/z_uncertainty_xyz_error.png`

Verify the markdown has actual data (not placeholders): `grep "error" docs/reports/z_uncertainty_report.md` should show numeric values.

Verify Z > XY anisotropy is documented: `grep -i "ratio\|anisotropy\|worse" docs/reports/z_uncertainty_report.md` should find the interpretation paragraph.
  </verify>
  <done>
A Z-uncertainty characterization report exists at docs/reports/z_uncertainty_report.md with: error-vs-depth data table at 5cm intervals, three embedded plots (XYZ error, Z/XY ratio, camera visibility), summary statistics, and interpretation paragraph documenting the Z/XY anisotropy ratio for downstream optimizer weighting decisions.
  </done>
</task>

</tasks>

<verification>
1. `hatch run test tests/unit/calibration/test_uncertainty.py` — unit tests pass
2. `ls docs/reports/z_uncertainty_report.md` — report exists
3. Report contains numeric error data at multiple depths
4. Report documents Z/XY error anisotropy with interpretation
5. `hatch run lint src/aquapose/calibration/uncertainty.py` — no lint errors
</verification>

<success_criteria>
- Z-uncertainty report exists with error quantification at 5cm depth intervals across the tank range
- X and Y errors are demonstrably smaller than Z errors (expected for top-down geometry)
- Report includes embedded plots showing error vs. depth curves
- Report includes interpretation paragraph suitable for a methods section
- triangulate_rays and compute_triangulation_uncertainty are unit-tested and importable from aquapose.calibration
</success_criteria>

<output>
After completion, create `.planning/phases/01-calibration-and-refractive-geometry/01-02-SUMMARY.md`
</output>
