---
phase: 02.1.1-object-detection-alternative-to-mog2
plan: 01
subsystem: segmentation
tags: [yolo, mog2, frame-sampling, label-studio, annotation, dataset]

# Dependency graph
requires:
  - phase: 02.1-segmentation-troubleshooting
    provides: MOG2Detector with watershed-split and shadow-merge logic
provides:
  - scripts/sample_yolo_frames.py for MOG2-guided diverse frame export
  - Documented workflow for Label Studio annotation and YOLO dataset assembly
affects:
  - 02.1.1-02 (YOLO training — consumes data/yolo_fish/ dataset)

# Tech tracking
tech-stack:
  added: []
  patterns:
    - "MOG2-guided diversity sampling: bin by detection count (0,1,2,3+), allocate at least 1 per non-empty bin, fill remainder proportionally"

key-files:
  created:
    - scripts/sample_yolo_frames.py
  modified: []

key-decisions:
  - "sample_yolo_frames.py uses MOG2Detector.detect() (not apply()) on strided frames to classify diversity; detector.apply() is called on skipped frames to keep background model current"
  - "Ring cameras: 10 frames each (12 cameras = 120); center camera: 30 frames; total 150 frames"
  - "Stratified bin sampling: quota >= 1 per non-empty bin, surplus distributed proportional to bin size"

patterns-established:
  - "Frame diversity via MOG2 count bins (0, 1, 2, 3+): ensures hard cases (stationary/absent fish) are included in annotation set"

requirements-completed:
  - SEG-01

# Metrics
duration: 5min
completed: 2026-02-20
---

# Phase 02.1.1 Plan 01: YOLO Frame Sampling - Summary

**MOG2-guided frame sampling script (`scripts/sample_yolo_frames.py`) that exports 150 diverse frames binned by fish-detection count, ready for Label Studio bounding box annotation.**

## Performance

- **Duration:** 5 min
- **Started:** 2026-02-20T15:15:04Z
- **Completed:** 2026-02-20T15:20:00Z
- **Tasks:** 1 of 2 (stopped at checkpoint:human-verify)
- **Files modified:** 1

## Accomplishments
- Frame sampling script with MOG2 detection-count-based bin diversity (0, 1, 2, 3+ fish)
- Balanced quota allocation: guarantees at least 1 frame per non-empty bin, fills remainder proportionally to bin size
- Per-camera summary table printed at completion
- `--center-camera` flag distinguishes center camera (30 frames) from ring cameras (10 each)
- Random seed support for reproducible sampling

## Task Commits

Each task was committed atomically:

1. **Task 1: Build MOG2-guided frame sampling script** - `3627a48` (feat)

**Plan metadata:** (pending — created at checkpoint)

## Files Created/Modified
- `scripts/sample_yolo_frames.py` - CLI script: scans per-camera videos with MOG2, bins by detection count, exports diverse JPEG frames for YOLO annotation

## Decisions Made
- `detect()` called on every strided frame (not `apply()`) so detection count drives bin assignment; `apply()` is still called on skipped frames to keep background model accurate
- Script does NOT automate Label Studio import — user imports JPEGs manually via web UI (as specified in plan)
- `--ring-frames` and `--center-frames` exposed as separate CLI args (not hardcoded) for flexibility

## Deviations from Plan

### Auto-fixed Issues

**1. [Rule 1 - Bug] Removed unused variable `frame_idx` to satisfy ruff F841**
- **Found during:** Task 1 (pre-commit hook lint check)
- **Issue:** Variable assigned but never used after code was restructured to use `cap.get(CAP_PROP_POS_FRAMES)` for actual frame position
- **Fix:** Removed the stale assignment
- **Files modified:** scripts/sample_yolo_frames.py
- **Verification:** `hatch run python scripts/sample_yolo_frames.py --help` passes; ruff passes pre-commit
- **Committed in:** 3627a48 (Task 1 commit)

---

**Total deviations:** 1 auto-fixed (1 lint bug)
**Impact on plan:** Trivial cleanup. No scope creep.

## Issues Encountered
- Pre-commit ruff formatter reformatted long dict comprehension lines (expected, no impact on behavior)

## User Setup Required

**Task 2 awaits human annotation workflow:**

1. Run the sampling script:
   ```bash
   python scripts/sample_yolo_frames.py --video-dir /path/to/videos --center-camera e3v8340
   ```
   Adjust `--center-camera` to match your center camera ID.

2. Verify ~150 frames exported to `data/yolo_fish_raw/images/`.

3. Create Label Studio project with RectangleLabels config:
   ```xml
   <View>
     <Image name="image" value="$image"/>
     <RectangleLabels name="label" toName="image">
       <Label value="fish" background="green"/>
     </RectangleLabels>
   </View>
   ```

4. Import frames into Label Studio, annotate all with fish bounding boxes.

5. Export as "YOLO" format, extract zip.

6. Organize into YOLO dataset structure with stratified 80/20 split per camera:
   - `data/yolo_fish/images/train/` (~120 images)
   - `data/yolo_fish/images/val/` (~30 images)
   - `data/yolo_fish/labels/train/` and `labels/val/` (matching .txt files)

7. Create `data/yolo_fish/dataset.yaml`:
   ```yaml
   path: /absolute/path/to/AquaPose/data/yolo_fish
   train: images/train
   val: images/val
   nc: 1
   names:
     0: fish
   ```

## Next Phase Readiness
- `scripts/sample_yolo_frames.py` is ready to run — user needs video files accessible
- After Task 2 annotation workflow completes, Plan 02 (YOLO training) can proceed immediately
- Plan 02 consumes `data/yolo_fish/dataset.yaml` via `model.train(data=...)`

---
*Phase: 02.1.1-object-detection-alternative-to-mog2*
*Completed: 2026-02-20*
