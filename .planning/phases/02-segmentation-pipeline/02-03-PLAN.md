---
phase: 02-segmentation-pipeline
plan: 03
type: execute
wave: 3
depends_on: ["02-02"]
files_modified:
  - src/aquapose/segmentation/dataset.py
  - src/aquapose/segmentation/model.py
  - src/aquapose/segmentation/training.py
  - tests/unit/segmentation/test_dataset.py
  - tests/unit/segmentation/test_model.py
autonomous: true

must_haves:
  truths:
    - "Dataset loads COCO-format annotations and produces 256x256 crops with masks"
    - "Mask R-CNN model predicts binary masks on 256x256 input crops"
    - "Training script fine-tunes from ImageNet-pretrained backbone"
    - "Model returns RLE-encoded masks with bounding boxes and confidence scores"
  artifacts:
    - path: "src/aquapose/segmentation/dataset.py"
      provides: "CropDataset for training"
      exports: ["CropDataset"]
    - path: "src/aquapose/segmentation/model.py"
      provides: "MaskRCNNSegmentor for inference"
      exports: ["MaskRCNNSegmentor"]
    - path: "src/aquapose/segmentation/training.py"
      provides: "Training entry point"
      exports: ["train"]
    - path: "tests/unit/segmentation/test_dataset.py"
      provides: "Dataset unit tests"
      contains: "test_"
    - path: "tests/unit/segmentation/test_model.py"
      provides: "Model unit tests"
      contains: "test_"
  key_links:
    - from: "src/aquapose/segmentation/dataset.py"
      to: "src/aquapose/segmentation/label_studio.py"
      via: "consumes COCO JSON from to_coco_dataset"
      pattern: "coco"
    - from: "src/aquapose/segmentation/model.py"
      to: "torchvision.models.detection"
      via: "wraps maskrcnn_resnet50_fpn_v2"
      pattern: "maskrcnn_resnet50_fpn_v2"
---

<objective>
Implement Mask R-CNN training dataset, model wrapper, and training script using torchvision (not Detectron2 — per research, Detectron2 is unmaintained and Windows-incompatible).

Purpose: The trained Mask R-CNN replaces MOG2+SAM for production inference, achieving ≥0.90 mean mask IoU (≥0.85 females). Per user decision: 256x256 crops, single "fish" class, ImageNet-pretrained ResNet-50, standard augmentations, 80/20 train/val split.

Output: CropDataset, MaskRCNNSegmentor, training script, unit tests.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-segmentation-pipeline/02-RESEARCH.md
@.planning/phases/02-segmentation-pipeline/02-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Training dataset and model wrapper</name>
  <files>
    src/aquapose/segmentation/dataset.py
    src/aquapose/segmentation/model.py
    tests/unit/segmentation/test_dataset.py
    tests/unit/segmentation/test_model.py
  </files>
  <action>
    **dataset.py — CropDataset class (torch.utils.data.Dataset):**

    Constructor: `CropDataset(coco_json: Path, image_root: Path, crop_size: int = 256, augment: bool = False)`
    - Loads COCO JSON annotations (images, annotations, categories)
    - Builds index: image_id -> list of annotation dicts
    - Stores crop_size and augment flag

    `__getitem__(idx) -> tuple[Tensor, dict]`:
    - Load image from image_root / images[idx]["file_name"]
    - Get all annotations for this image
    - For each annotation:
      - Decode RLE mask via pycocotools.mask.decode()
      - Extract bbox [x, y, w, h] and convert to [x1, y1, x2, y2] (torchvision format)
    - Resize image and masks to crop_size x crop_size (cv2.resize, INTER_LINEAR for image, INTER_NEAREST for masks)
    - If augment=True, apply random:
      - Horizontal flip (50%)
      - Vertical flip (50%)
      - Random rotation (0, 90, 180, 270)
      - Brightness/contrast jitter (torchvision.transforms.ColorJitter, brightness=0.3, contrast=0.3)
    - Convert to tensors:
      - image: float32 [C, H, W] normalized to [0, 1]
      - target dict with:
        - "boxes": float32 [N, 4] in [x1, y1, x2, y2]
        - "labels": int64 [N] all 1 (fish class)
        - "masks": uint8 [N, H, W] binary masks
    - For negative frames (no annotations): return image with empty tensors for boxes/labels/masks
    - Handle all augmentations consistently across image, boxes, and masks

    `__len__()`: return number of images

    **model.py — MaskRCNNSegmentor class:**

    Constructor: `MaskRCNNSegmentor(num_classes: int = 2, weights_path: Path | None = None, confidence_threshold: float = 0.1)`
    - Builds torchvision maskrcnn_resnet50_fpn_v2:
      - Load with MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT (ImageNet pretrained)
      - Replace box predictor: FastRCNNPredictor(model.roi_heads.box_predictor.cls_score.in_features, num_classes)
      - Replace mask predictor: MaskRCNNPredictor(model.roi_heads.mask_predictor.conv5_mask.in_channels, 256, num_classes)
    - If weights_path provided, load state_dict from file
    - Store confidence_threshold

    Method: `predict(images: list[np.ndarray]) -> list[list[SegmentationResult]]`
    - Accepts batch of numpy images (H, W, 3 uint8)
    - Converts to list of float32 tensors [C, H, W] normalized [0, 1]
    - Runs model in eval mode with torch.no_grad()
    - For each image output, filter by confidence_threshold
    - For each surviving detection:
      - Threshold mask at 0.5 to get binary
      - Encode binary mask to RLE via pycocotools.mask.encode(np.asfortranarray(mask))
      - Create SegmentationResult
    - Returns list of list of SegmentationResult

    Method: `get_model() -> nn.Module`
    - Returns underlying torchvision model (for training script to access)

    Dataclass: `SegmentationResult`
    - bbox: tuple[int, int, int, int]  # (x1, y1, x2, y2)
    - mask_rle: dict  # pycocotools RLE dict {"counts": bytes, "size": [h, w]}
    - confidence: float
    - label: int  # always 1 for fish

    **Tests:**
    - test_dataset.py:
      - Create tiny COCO JSON fixture (2 images, 3 annotations) with synthetic masks
      - Test __getitem__ returns correct tensor shapes
      - Test empty annotation (negative frame) returns empty targets
      - Test augmentation doesn't crash and produces valid outputs
      - Test bbox format is [x1, y1, x2, y2]
    - test_model.py:
      - Test model construction (no GPU needed — just architecture check)
      - Test predict with tiny random images produces list of SegmentationResult
      - Test confidence filtering (mock model output with known scores)
      - Test RLE encoding produces valid pycocotools format

    Update __init__.py to export CropDataset, MaskRCNNSegmentor, SegmentationResult.
  </action>
  <verify>
    - `hatch run test tests/unit/segmentation/test_dataset.py tests/unit/segmentation/test_model.py` — all pass
    - `hatch run lint` — clean
    - `hatch run typecheck` — clean on new files
  </verify>
  <done>
    CropDataset loads COCO JSON and produces 256x256 crops with augmentation. MaskRCNNSegmentor wraps torchvision Mask R-CNN with proper head replacement and produces RLE-encoded SegmentationResult objects. Tests pass with synthetic fixtures.
  </done>
</task>

<task type="auto">
  <name>Task 2: Training script and pipeline API</name>
  <files>
    src/aquapose/segmentation/training.py
    src/aquapose/segmentation/__init__.py
  </files>
  <action>
    **training.py — Training entry point:**

    Function: `train(coco_json: Path, image_root: Path, output_dir: Path, *, epochs: int = 40, batch_size: int = 4, lr: float = 0.005, val_split: float = 0.2, crop_size: int = 256, device: str | None = None) -> Path`
    - Auto-detect device if None (cuda > cpu)
    - Load CropDataset(coco_json, image_root, crop_size, augment=True)
    - Random 80/20 split using torch.utils.data.random_split (seeded for reproducibility, seed=42)
    - Create validation dataset wrapper with augment=False
    - DataLoader for train (batch_size, shuffle=True, collate_fn=list — Mask R-CNN expects list of tuples)
    - DataLoader for val (batch_size=1, shuffle=False, same collate_fn)
    - Build MaskRCNNSegmentor(num_classes=2)
    - Optimizer: SGD(params, lr=lr, momentum=0.9, weight_decay=0.0005)
    - LR scheduler: StepLR(optimizer, step_size=int(epochs * 0.8), gamma=0.1) — decay at 80%
    - Training loop:
      - For each epoch:
        - Train: model.train(), forward pass returns loss dict, sum losses, backward, step
        - Validate every 5 epochs: compute mean mask IoU on val set using pycocotools
        - Log epoch, train_loss, val_iou to stdout
        - Save best model (by val IoU) to output_dir/best_model.pth
      - Save final model to output_dir/final_model.pth
    - Returns path to best model

    Function: `evaluate(model_path: Path, coco_json: Path, image_root: Path, *, crop_size: int = 256, device: str | None = None) -> dict`
    - Load trained model from model_path
    - Load validation dataset
    - Run inference on all images
    - Compute per-image mask IoU using pycocotools
    - Return {"mean_iou": float, "per_image_iou": list[float], "num_images": int}

    Custom collate_fn: `def collate_fn(batch): return tuple(zip(*batch))`
    - Mask R-CNN in torchvision expects list[Tensor], list[dict] not batched tensors

    Update __init__.py to export train and evaluate functions, plus all previously exported symbols.

    Ensure the final __init__.py exports the complete public API:
    - MOG2Detector, Detection (from detector)
    - SAMPseudoLabeler, FrameAnnotation, AnnotatedFrame (from pseudo_labeler)
    - export_to_label_studio, import_from_label_studio, to_coco_dataset (from label_studio)
    - CropDataset (from dataset)
    - MaskRCNNSegmentor, SegmentationResult (from model)
    - train, evaluate (from training)
  </action>
  <verify>
    - `hatch run lint` — clean
    - `hatch run typecheck` — clean
    - `python -c "from aquapose.segmentation import train, evaluate, MaskRCNNSegmentor, CropDataset"` — imports succeed
  </verify>
  <done>
    Training script fine-tunes Mask R-CNN from ImageNet weights on annotated crops. Evaluation computes mask IoU. Pipeline accepts N fish detections and returns list of RLE-encoded masks per frame. All public API symbols exported from __init__.py.
  </done>
</task>

</tasks>

<verification>
- All unit tests pass: `hatch run test tests/unit/segmentation/`
- All public symbols importable from `aquapose.segmentation`
- Training script runnable on small synthetic dataset (may need GPU for real training)
- Model inference produces SegmentationResult with RLE masks
</verification>

<success_criteria>
CropDataset produces properly formatted 256x256 training samples from COCO JSON. MaskRCNNSegmentor uses torchvision (not Detectron2) with proper head replacement for 2-class output. Training script handles train/val split, SGD optimization, LR scheduling, and model checkpointing. Evaluation computes mask IoU. Complete segmentation module public API exported.
</success_criteria>

<output>
After completion, create `.planning/phases/02-segmentation-pipeline/02-03-SUMMARY.md`
</output>
