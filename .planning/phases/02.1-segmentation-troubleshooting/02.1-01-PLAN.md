---
phase: 02.1-segmentation-troubleshooting
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/test_mog2.py
autonomous: true
requirements:
  - SEG-1
must_haves:
  truths:
    - "MOG2 detector achieves >=78% recall proxy (>=7/9 fish) averaged across all 13 cameras"
    - "Side-by-side annotated stills exist for every sampled frame showing detections + raw MOG2 mask"
    - "Numeric per-camera detection statistics are printed and interpretable"
  artifacts:
    - path: "scripts/test_mog2.py"
      provides: "Consolidated MOG2 diagnostic script with numeric metrics and visual output"
      contains: "recall_proxy"
  key_links:
    - from: "scripts/test_mog2.py"
      to: "src/aquapose/segmentation/detector.py"
      via: "MOG2Detector import and detect() calls"
      pattern: "from aquapose.segmentation.detector import MOG2Detector"
---

<objective>
Consolidate the two existing MOG2 diagnostic scripts into a single test script, run it on all 13 cameras, and evaluate whether the existing 2-stage shadow fix achieves sufficient recall to unblock downstream SAM2 evaluation.

Purpose: MOG2 is the foundation of the segmentation pipeline. If it misses fish, SAM2 never sees them and Mask R-CNN training data is incomplete. This plan validates the existing implementation on real data with numeric metrics.

Output: `scripts/test_mog2.py` producing annotated stills + recall proxy metrics; output images in `output/test_mog2/`
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02.1-segmentation-troubleshooting/02.1-RESEARCH.md

@scripts/diagnose_mog2.py
@scripts/verify_mog2_recall.py
@src/aquapose/segmentation/detector.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Consolidate MOG2 diagnostic scripts and run on all 13 cameras</name>
  <files>scripts/test_mog2.py</files>
  <action>
Create `scripts/test_mog2.py` by merging logic from `scripts/diagnose_mog2.py` and `scripts/verify_mog2_recall.py`. The consolidated script must:

1. **Per-camera sampling** (from verify_mog2_recall.py):
   - Accept `--data-root`, `--output-dir`, `--warmup-frames` (default 500), `--sample-count` (default 10) CLI args
   - Discover all 13 camera MP4 files in `{data-root}/raw_videos/`
   - For each camera: warm up MOG2, feed intermediate frames to keep model updated, sample `sample_count` evenly-spaced frames after warmup
   - Call `detector.detect(frame)` on each sampled frame

2. **Side-by-side visualization** (from diagnose_mog2.py):
   - For each sampled frame, produce a side-by-side image:
     - Left panel: original frame with detection bbox overlays (green rectangles) and detection count text
     - Right panel: colorized raw MOG2 mask (green=foreground 255, blue-ish=shadow 127, colored outlines=detection contours)
   - Use `detector._mog2.apply(frame, learningRate=0)` for the raw mask visualization (as diagnose_mog2.py already does — this is acceptable in a diagnostic script)
   - Scale both panels to half-size and hstack; save as JPEG to `{output-dir}/{camera_id}/frame_{idx:06d}.jpg`

3. **Numeric recall proxy metrics**:
   - For each frame: `recall_proxy = min(num_detections, 9) / 9.0`
   - Print per-frame results as they run
   - At the end, print a summary table: per-camera avg/min/max detections and avg recall proxy
   - Print overall mean recall proxy across all cameras
   - Print PASS/FAIL against threshold of 0.78 (>=7/9 fish)

4. **Use default MOG2Detector parameters** (history=500, var_threshold=12, min_area=200) — these are the production defaults. Do NOT use the tuned parameters from verify_pseudo_labels.py (those were specific to that script's needs).

5. **Exit code**: exit 0 if PASS, exit 1 if FAIL (enables CI usage).

After creating the script, run it:
```
hatch run python scripts/test_mog2.py --data-root "C:/Users/tucke/Desktop/Aqua/AquaPose" --output-dir output/test_mog2
```

If recall proxy is >= 0.78, the MOG2 component passes and no fix is needed.
If recall proxy is < 0.78, investigate the per-camera breakdown, identify worst cameras, and tune MOG2 parameters or fix the detector code. Re-run until passing or document why it cannot pass.

Document in the plan summary: (a) the existing 2-stage shadow fix in detector.py and what it does, (b) the numeric recall results, (c) whether any parameter changes were needed.
  </action>
  <verify>
- `scripts/test_mog2.py` exists and runs without error
- Output images exist in `output/test_mog2/{camera_id}/` for all 13 cameras
- Numeric summary is printed with per-camera stats and overall recall proxy
- Script prints PASS or FAIL with the 0.78 threshold
  </verify>
  <done>
MOG2 detection validated on real data: recall proxy >= 0.78 (or documented why not achievable and what was tried). Side-by-side annotated stills saved for all 13 cameras. Numeric per-camera statistics computed and printed.
  </done>
</task>

</tasks>

<verification>
- `hatch run python scripts/test_mog2.py --help` runs without import errors
- Output directory contains 13 camera subdirectories with JPEG stills
- Overall recall proxy metric printed to stdout
- PASS/FAIL printed against 0.78 threshold
</verification>

<success_criteria>
MOG2 detection recall proxy >= 0.78 averaged across all 13 cameras on real aquarium footage, OR a documented explanation of why and what was attempted. Visual evidence (annotated stills) saved for human review.
</success_criteria>

<output>
After completion, create `.planning/phases/02.1-segmentation-troubleshooting/02.1-01-SUMMARY.md`
</output>
