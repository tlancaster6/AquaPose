# Phase 02.1: Segmentation Troubleshooting - Research

**Researched:** 2026-02-20
**Domain:** Computer vision diagnostics — MOG2 background subtraction, SAM2 pseudo-labeling, Mask R-CNN instance segmentation
**Confidence:** HIGH (code is in-repo and inspectable; no external library uncertainty for the diagnostic work)

<user_constraints>
## User Constraints (from CONTEXT.md)

### Locked Decisions

#### Systematic test-then-fix structure
- Each component (MOG2, SAM2, Mask R-CNN) gets two tasks: test and fix
- If the test task passes thresholds, the fix task is checked off as complete
- Sequential dependency: MOG2 → SAM2 → Mask R-CNN (Mask R-CNN training data comes from SAM pseudo-labels + Label Studio corrections)

#### MOG2 testing (already in progress)
- Full video from all 13 cameras is available for testing
- Two existing scripts need cleanup: `scripts/diagnose_mog2.py` and `scripts/verify_mog2_recall.py` — combine and improve into a single diagnostic script
- Shadow misclassification already diagnosed: fish were being classified as shadows by MOG2
- 2-stage fix already implemented in `detector.py`: (1) merge shadow + foreground masks, (2) split merged blobs using foreground-only pixels (255) as seeds for watershed/distance-transform separation
- The MOG2 test task should summarize this prior diagnostic and fix work alongside new test results

#### SAM2 testing
- User will manually annotate a small handful of images as ground truth for SAM2 evaluation
- Test evaluates SAM2 pseudo-label quality against these manual annotations

#### Mask R-CNN testing
- Training data comes from SAM pseudo-labels corrected in Label Studio — depends on SAM pipeline working first
- Test evaluates trained model on held-out frames

#### Test output format
- All test scripts produce both visual output (annotated images showing detections/masks) AND numeric metrics
- Visual output allows human inspection of what's happening

#### Pass/fail thresholds
- Relaxed thresholds for now (lower bar than Phase 2 targets) to unblock Phase 4
- Phase 2 targets (95% MOG2 recall, 0.90 mean IoU) are aspirational — will tighten later
- Exact relaxed values: Claude's discretion based on what's achievable

### Claude's Discretion
- Specific relaxed threshold values per component
- Test script architecture (how to combine the two MOG2 scripts)
- Metric computation details
- Visual output format (overlay style, side-by-side, etc.)

### Deferred Ideas (OUT OF SCOPE)
None — discussion stayed within phase scope
</user_constraints>

---

## Summary

Phase 02.1 is a diagnostic and repair sprint, not a feature build. All three segmentation components (MOG2, SAM2, Mask R-CNN) already have complete implementations in `src/aquapose/segmentation/`. The phase exists because the implementations were written and unit-tested against synthetic data in Phase 2, but never validated on real aquarium footage. The task structure is test-then-fix for each component, sequentially.

The MOG2 shadow-misclassification problem is already diagnosed and a 2-stage fix (merge shadow+foreground, watershed-split merged blobs) is already implemented in `detector.py`. Two diagnostic scripts exist (`scripts/diagnose_mog2.py` produces annotated video; `scripts/verify_mog2_recall.py` samples frames from all 13 cameras and saves annotated stills). These need to be consolidated into a single improved script that also computes numeric recall. Additionally, the pseudo-label pipeline (`scripts/verify_pseudo_labels.py`) already ran successfully and produced 58 frames across 13 cameras with SAM2 masks (in `output/verify_pseudo_labels/`). The planner should account for work that is already substantially done.

The critical insight for planning: MOG2 is the foundation — if it misses fish, SAM2 never sees them, and Mask R-CNN training data is incomplete. The sequential gate is real and must be enforced in plan dependencies. Relaxed thresholds (recommended below) should be set at a level that unblocks Phase 4 rather than reaching production quality.

**Primary recommendation:** Plan three pairs of tasks (test + fix) with hard sequential gates: MOG2 pair must complete before SAM2 pair starts, SAM2 pair before Mask R-CNN pair. MOG2 test consolidates existing scripts and validates the existing shadow fix. SAM2 test evaluates the already-generated `output/verify_pseudo_labels/` output against manual ground truth. Mask R-CNN depends entirely on SAM2 annotations.

---

## Current State of Code (HIGH confidence — direct code inspection)

### What already exists and is complete

| Component | File | Status |
|-----------|------|--------|
| MOG2 detector with 2-stage shadow fix | `src/aquapose/segmentation/detector.py` | Complete, unit-tested |
| SAM2 pseudo-labeler | `src/aquapose/segmentation/pseudo_labeler.py` | Complete |
| Label Studio export/import | `src/aquapose/segmentation/label_studio.py` | Complete |
| COCO dataset loader | `src/aquapose/segmentation/dataset.py` | Complete |
| Mask R-CNN model wrapper | `src/aquapose/segmentation/model.py` | Complete |
| Mask R-CNN training + eval | `src/aquapose/segmentation/training.py` | Complete |
| MOG2 video diagnostic script | `scripts/diagnose_mog2.py` | Exists, needs consolidation |
| MOG2 recall verification script | `scripts/verify_mog2_recall.py` | Exists, needs consolidation |
| SAM2 pseudo-label generation script | `scripts/verify_pseudo_labels.py` | Exists and already ran |

### What already ran

- `scripts/verify_pseudo_labels.py generate` has already produced 58 frames with SAM2 masks in `output/verify_pseudo_labels/` (images + Label Studio tasks JSON)
- MOG2 diagnostic videos exist: `output/mog2_diagnostic_baseline.mp4` and `output/mog2_diagnostic_v2.mp4`
- MOG2 recall sampling ran: `output/verify_mog2_defaults/` contains per-camera subdirectories (13 cameras: e3v8250, e3v829d, e3v82e0, e3v82f9, e3v831e, e3v832e, e3v8334, e3v83e9, e3v83eb, e3v83ee, e3v83ef, e3v83f0, e3v83f1)

### What is NOT done yet

- No numeric recall metric computed from the annotated MOG2 frames (only visual review was possible)
- No ground truth annotations for SAM2 IoU measurement
- No Label Studio round-trip completed (no corrected annotations imported)
- No Mask R-CNN training run on real data
- The two MOG2 scripts are separate and each partially functional — need consolidation

---

## Architecture Patterns

### Pattern 1: Test-Then-Fix Task Pairs

Each component follows this structure:
```
Task A (test): run diagnostic script → produce metrics + visuals → evaluate against threshold
Task B (fix): IF metrics fail → modify implementation → re-run test to verify
```

Task B is a conditional no-op: if Task A passes, Task B is marked complete without doing work. This prevents forcing unnecessary refactoring.

### Pattern 2: Consolidated Diagnostic Script

Both `scripts/diagnose_mog2.py` (video output) and `scripts/verify_mog2_recall.py` (still frames + per-camera stats) should merge into a single `scripts/test_mog2.py` that:
- Processes all 13 cameras
- Produces annotated still frames per camera (for visual inspection)
- Produces a side-by-side MOG2 mask visualization (from diagnose_mog2.py)
- Computes and prints numeric recall-proxy metrics (detection counts, per-camera stats)
- Reports detection count consistency across frames as a recall proxy

**Key architectural observation from code review:** `verify_mog2_recall.py` samples frames, detects, saves annotated stills, and prints per-camera detection counts. `diagnose_mog2.py` produces a side-by-side video with mask visualization. Both use `MOG2Detector` from the library. The merge is a straightforward combination — keep the per-camera sampling from `verify_mog2_recall.py` and add the mask colorization logic from `diagnose_mog2.py` to the still frames.

### Pattern 3: SAM2 Evaluation Against Manual Ground Truth

The user will manually annotate a small number of images from `output/verify_pseudo_labels/images/` (already 58 frames available). The test script:
1. Loads ground-truth binary masks (user-annotated, likely as simple PNG files or a small JSON)
2. Loads SAM2 pseudo-label masks from the already-generated output
3. Matches predictions to GT by IoU (greedy or Hungarian)
4. Reports per-frame and mean mask IoU

Ground truth format recommendation: simple binary PNG files (one per fish per frame) named consistently, e.g., `{frame_id}_gt_{n}.png`. This is the minimal-friction format for the user to create in any image editor.

### Pattern 4: Mask R-CNN Test Depends on Label Studio Round-Trip

The Mask R-CNN component cannot be tested until:
1. SAM2 masks are reviewed/corrected in Label Studio
2. Corrected annotations are exported from Label Studio
3. `scripts/verify_pseudo_labels.py import --ls-export <path>` produces a COCO JSON
4. `training.py:train()` runs on that COCO JSON
5. `training.py:evaluate()` runs on a held-out split

The `training.py` module already implements both `train()` and `evaluate()` functions. The Mask R-CNN test script is essentially a thin wrapper that calls these, prints results, and saves visual overlays.

---

## Standard Stack

All libraries are already installed and in use. No new dependencies for this phase.

### Core (all already in pyproject.toml)
| Library | Purpose | Usage in Phase |
|---------|---------|---------------|
| `opencv-python` (cv2) | Video processing, mask ops, annotation drawing | MOG2 test script |
| `numpy` | Array math, metric computation | All test scripts |
| `sam2` (facebook/sam2) | SAM2 predictor, already wrapped | SAM2 test |
| `torch` / `torchvision` | Mask R-CNN train/eval, already wrapped | Mask R-CNN test |
| `pycocotools` | IoU computation via `mask_util.iou()`, COCO format | SAM2 + Mask R-CNN metrics |

### Metric Computation Tools

**IoU between two binary masks:**
```python
import pycocotools.mask as mask_util
import numpy as np

def mask_iou(pred: np.ndarray, gt: np.ndarray) -> float:
    """Compute IoU between two binary uint8 masks."""
    pred_f = np.asfortranarray((pred > 0).astype(np.uint8))
    gt_f = np.asfortranarray((gt > 0).astype(np.uint8))
    pred_rle = mask_util.encode(pred_f)
    gt_rle = mask_util.encode(gt_f)
    iou = mask_util.iou([pred_rle], [gt_rle], [False])
    return float(iou[0, 0])
```

**Recall proxy for MOG2 (no ground truth):**
Since there are no annotated ground-truth bounding boxes for MOG2, recall is measured as a proxy: mean detections per frame across all cameras, with 9 expected fish. A frame with N detections where N < 9 implies at least (9-N) missed fish. Recall proxy = min(N, 9) / 9 per frame, averaged across sampled frames.

---

## Recommended Relaxed Thresholds (Claude's Discretion)

Based on domain knowledge: the system must detect enough fish to drive Phase 4 single-fish reconstruction. The target is unblocking, not production quality.

| Component | Phase 2 Target | Recommended Relaxed Threshold | Rationale |
|-----------|---------------|-------------------------------|-----------|
| MOG2 detection count | ≥95% recall | ≥7/9 fish detected per frame (≥78% proxy recall) on average | Missing 1-2 fish is acceptable if those are stationary females in worst-case lighting; Phase 4 needs only 1 good fish |
| SAM2 mask IoU | ≥0.90 mean IoU | ≥0.70 mean IoU against manual GT | SAM2 at 0.70 produces usable pseudo-labels for Label Studio correction workflow |
| Mask R-CNN mask IoU | ≥0.90 mean IoU | ≥0.70 mean IoU on held-out frames | Consistent with SAM2 threshold; sufficient for silhouette-based fitting |

These thresholds are intentionally permissive. If the system achieves ≥0.70 IoU at every stage, Phase 4 can proceed — the optimizer has tolerance for imperfect masks.

---

## Don't Hand-Roll

| Problem | Don't Build | Use Instead |
|---------|-------------|-------------|
| Mask IoU computation | Custom pixel-by-pixel IoU | `pycocotools.mask.iou()` — handles RLE, vectorized |
| Prediction-to-GT matching | Custom Hungarian implementation | Simple greedy by score (acceptable for small N); or `scipy.optimize.linear_sum_assignment` if needed |
| Mask R-CNN training loop | Custom training from scratch | `training.py:train()` already exists — call it |
| Mask R-CNN evaluation | Custom eval loop | `training.py:evaluate()` already exists — call it |
| Video writing | Custom frame-by-frame encoding | `cv2.VideoWriter` (already used in `diagnose_mog2.py`) |

---

## Common Pitfalls

### Pitfall 1: Double-Counting the MOG2 Fix as "Not Yet Tested"

**What goes wrong:** The 2-stage shadow fix is in `detector.py` and unit-tested, but no real-data validation has been done. Plans might incorrectly treat this as requiring a new implementation when it just needs validation.
**How to avoid:** The MOG2 test task must explicitly say "validate the existing implementation on real data" — not "implement a fix." The fix task is a conditional no-op if recall is sufficient.

### Pitfall 2: MOG2 apply() called twice per frame

**What goes wrong:** In `diagnose_mog2.py`, line 104 calls `detector._mog2.apply(frame, learningRate=0)` a second time to get the raw mask for visualization — after `detector.detect(frame)` already called it with `learningRate > 0`. This means the background model was updated once but not twice, which is correct. However, it accesses a private attribute (`_mog2`). The consolidated script should expose a `detect_with_raw_mask()` method or accept the current pattern as a diagnostic-only script (acceptable since it lives in `scripts/`, not in the library).
**How to avoid:** In the consolidated MOG2 test script, use `learningRate=0` for the second call (as already done) — document this explicitly.

### Pitfall 3: SAM2 Evaluation Requires Exact Frame Correspondence

**What goes wrong:** The user annotates a subset of frames as GT. The test script must match SAM2 masks to GT masks by frame ID, not by index. If frame IDs don't match exactly, the IoU scores will be meaningless.
**How to avoid:** Use the `frame_id` field from `FrameAnnotation` (already in the tasks JSON: `{camera_id}_frame_{frame_idx:06d}`) as the key. GT files should be named with the same scheme.

### Pitfall 4: Label Studio Round-Trip Depends on Local File Serving

**What goes wrong:** The Label Studio tasks JSON uses `/data/local-files/?d=images/...` paths, which require Label Studio to be configured with a local file serving root pointing at `output/verify_pseudo_labels/`. If this isn't configured, images won't load in Label Studio.
**How to avoid:** Document the Label Studio local storage setup in the SAM2 fix/annotation task. This is an operational concern, not a code bug. The images directory is `output/verify_pseudo_labels/images/` — the user must set Label Studio's LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT to the parent of `images/`.

### Pitfall 5: Mask R-CNN Training Data Volume

**What goes wrong:** 58 frames × ~9 fish/frame = ~522 potential annotations, but not all frames will have all fish (especially if MOG2 missed some). After Label Studio correction and any rejections, the actual training set may be small (< 200 annotated instances). Training Mask R-CNN from ImageNet pretraining on < 200 instances is achievable but requires careful augmentation.
**How to avoid:** The existing `dataset.py` already implements augmentation (flips, rotation, brightness/contrast jitter). Ensure `augment=True` is used in the training call. If training set is very small (< 50 instances), report it as a concern in the Mask R-CNN test task.

### Pitfall 6: MOG2 Detection Count as Recall Proxy Is Imprecise

**What goes wrong:** A frame showing 9 detections where 2 are false positives and 2 fish are missed still reports "9 detections" and would naively pass. Numeric recall without ground-truth boxes is always a proxy.
**How to avoid:** Supplement numeric counts with visual inspection of annotated stills. The test tasks require BOTH numeric metrics AND visual output for this reason.

---

## Code Examples

### Consolidated MOG2 Test Script Structure

The new `scripts/test_mog2.py` should combine sampling (from `verify_mog2_recall.py`) with mask colorization (from `diagnose_mog2.py`):

```python
# Source: direct from verify_mog2_recall.py and diagnose_mog2.py
def process_camera(video_path, output_dir, warmup, samples):
    """Sample frames, detect, save side-by-side annotated stills."""
    camera_id = parse_camera_id(video_path.stem)
    detector = MOG2Detector()
    # warm up...
    results = []
    for target_idx in sample_indices(warmup, total, samples):
        # feed intermediate frames to keep model updated
        detections = detector.detect(frame)
        raw_mask = detector._mog2.apply(frame, learningRate=0)  # visualize only

        # Left panel: frame + bbox/mask overlays
        left = draw_detections(frame, detections)
        # Right panel: colorized raw MOG2 mask
        right = colorize_mask(raw_mask, detections)
        side_by_side = np.hstack([
            cv2.resize(left, (w//2, h//2)),
            cv2.resize(right, (w//2, h//2))
        ])
        cv2.imwrite(str(output_dir / camera_id / f"frame_{target_idx:06d}.jpg"), side_by_side)

        results.append({
            "camera_id": camera_id,
            "frame_idx": target_idx,
            "num_detections": len(detections),
            "recall_proxy": min(len(detections), 9) / 9.0,
        })
    return results
```

### SAM2 IoU Evaluation Pattern

```python
# GT masks loaded as PNG files named {frame_id}_gt_{n}.png
def evaluate_sam2(pseudo_labels_dir, gt_dir, threshold=0.70):
    ious = []
    for gt_path in sorted(gt_dir.glob("*_gt_*.png")):
        frame_id = "_".join(gt_path.stem.split("_")[:-2])  # strip _gt_N
        gt_mask = cv2.imread(str(gt_path), cv2.IMREAD_GRAYSCALE)
        # Find corresponding SAM2 mask (matched by frame_id)
        # ... load from tasks JSON or regenerate ...
        iou = mask_iou(pred_mask, gt_mask)
        ious.append(iou)
    mean_iou = np.mean(ious) if ious else 0.0
    print(f"SAM2 mean IoU: {mean_iou:.3f} (threshold: {threshold})")
    return mean_iou >= threshold
```

### Mask R-CNN Training + Eval (Thin Wrapper)

```python
# Source: training.py already implements this — test script just calls it
from aquapose.segmentation.training import train, evaluate

best_model = train(
    coco_json=Path("output/verify_pseudo_labels/coco_annotations.json"),
    image_root=Path("output/verify_pseudo_labels/source_frames"),
    output_dir=Path("output/maskrcnn"),
    epochs=40,
)
metrics = evaluate(best_model, coco_json, image_root)
print(f"Mask R-CNN mean IoU: {metrics['mean_iou']:.3f}")
```

---

## Phase Structure Recommendation

Based on the locked decisions (test + fix pairs, sequential), the planner should create 3 plan files:

### Plan 02.1-01: MOG2 Detection Test and Fix
**Tasks:**
1. Consolidate `diagnose_mog2.py` + `verify_mog2_recall.py` into `scripts/test_mog2.py` with numeric metrics and side-by-side visual output
2. Run `test_mog2.py` on all 13 cameras, review output, compute recall proxy
3. Document shadow fix in test summary (existing 2-stage fix in `detector.py`)
4. FIX TASK (conditional): if recall proxy < 0.78, tune MOG2 parameters or fix the detector; re-run until passing

### Plan 02.1-02: SAM2 Pseudo-Label Test and Fix
**Tasks:**
1. User annotates a small handful of frames from `output/verify_pseudo_labels/images/` as GT (operational step — must produce GT mask files)
2. Write `scripts/test_sam2.py` that computes IoU between existing SAM2 output and GT masks, saves visual overlays (side-by-side: GT mask vs SAM2 mask)
3. Run test, evaluate mean IoU against 0.70 threshold
4. FIX TASK (conditional): if IoU < 0.70, investigate SAM2 prompt strategy (box vs mask prompt, model variant), fix, re-run

### Plan 02.1-03: Mask R-CNN Training and Test
**Tasks:**
1. Complete Label Studio round-trip: user reviews `output/verify_pseudo_labels/` in Label Studio, exports corrections
2. Run `scripts/verify_pseudo_labels.py import` to produce COCO JSON
3. Write `scripts/test_maskrcnn.py` that trains on COCO JSON (calls `training.train()`) and evaluates on held-out split (calls `training.evaluate()`), saves visual overlays
4. Run test, evaluate mean IoU against 0.70 threshold
5. FIX TASK (conditional): if IoU < 0.70, investigate training data quality, augmentation, epochs; re-train

---

## Open Questions

1. **How does the user want to provide SAM2 ground truth?**
   - What we know: user will "manually annotate a small handful of images"
   - What's unclear: which tool (Label Studio, Paint, GIMP?), what file format
   - Recommendation: simplest is binary PNG files (one per fish per fish per frame, named `{frame_id}_gt_{n}.png`). The SAM2 test task should document this format requirement clearly.

2. **Is the Label Studio instance already set up and running?**
   - What we know: tasks JSON and images are already in `output/verify_pseudo_labels/`
   - What's unclear: whether Label Studio has been configured with local file serving
   - Recommendation: The SAM2 fix / Mask R-CNN setup task must include Label Studio configuration as an explicit step, not an assumption.

3. **What is the 13-camera video set size / processing time?**
   - What we know: 13 cameras, videos exist at `C:/Users/tucke/Desktop/Aqua/AquaPose/raw_videos/`, frame count unknown
   - What's unclear: how long does `test_mog2.py` take to run with warmup=500 + sample=10 per camera?
   - Recommendation: keep default sample count at 10 per camera (130 total frames) — fast enough for iteration.

4. **Has the MOG2 shadow fix actually improved recall on real data?**
   - What we know: the fix is implemented, two diagnostic videos were produced, but no numeric comparison to pre-fix behavior exists
   - What's unclear: whether the fix is sufficient or if more tuning is needed
   - Recommendation: the MOG2 test task must explicitly compare numeric results to the threshold — if it fails, the fix task triggers a tuning loop.

---

## Sources

### Primary (HIGH confidence — direct code inspection)
- `src/aquapose/segmentation/detector.py` — MOG2Detector, 2-stage shadow fix implementation
- `src/aquapose/segmentation/pseudo_labeler.py` — SAMPseudoLabeler implementation
- `src/aquapose/segmentation/label_studio.py` — export/import, RLE format
- `src/aquapose/segmentation/training.py` — train() and evaluate() functions
- `src/aquapose/segmentation/model.py` — MaskRCNNSegmentor
- `src/aquapose/segmentation/dataset.py` — CropDataset
- `scripts/diagnose_mog2.py` — existing video diagnostic script
- `scripts/verify_mog2_recall.py` — existing recall verification script
- `scripts/verify_pseudo_labels.py` — existing SAM2 pipeline script
- `output/verify_pseudo_labels/` — already-produced SAM2 output (58 frames, tasks JSON)
- `output/verify_mog2_defaults/` — already-produced MOG2 recall output (13 cameras)
- `.planning/phases/02.1-segmentation-troubleshooting/02.1-CONTEXT.md` — user decisions

### Secondary (MEDIUM confidence — project STATE.md)
- `.planning/STATE.md` — confirmed decisions: shadow threshold at 254, mask full-frame for SAM, torchvision instead of Detectron2, custom collate_fn
- `.planning/REQUIREMENTS.md` — SEG-01 through SEG-05, aspirational targets (95% recall, 0.90 IoU)

---

## Metadata

**Confidence breakdown:**
- Current state of code: HIGH — direct file inspection
- What has already run: HIGH — output directories inspected, file counts verified
- Relaxed thresholds: MEDIUM — based on domain reasoning, not empirical measurement on this rig
- Pitfalls: HIGH — identified from direct code reading and known OpenCV/Label Studio behaviors
- Plan structure: HIGH — follows locked decisions from CONTEXT.md exactly

**Research date:** 2026-02-20
**Valid until:** No expiry — all findings are based on in-repo code, which doesn't change unless committed. Re-verify if detector.py or scripts are modified.
