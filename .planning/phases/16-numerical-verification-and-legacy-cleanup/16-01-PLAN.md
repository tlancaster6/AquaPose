---
phase: 16-numerical-verification-and-legacy-cleanup
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/regression/__init__.py
  - tests/regression/conftest.py
  - tests/regression/test_per_stage_regression.py
  - tests/regression/test_end_to_end_regression.py
  - pyproject.toml
  - scripts/generate_golden_data.py
autonomous: true
requirements: [VER-03]

must_haves:
  truths:
    - "Running `pytest tests/regression/ -m regression` executes per-stage and end-to-end numerical regression tests against golden data"
    - "Each stage's output from the new PosePipeline matches golden data within per-stage tolerances (detection ~1e-6, midline ~1e-6, tracking ~1e-6, reconstruction ~1e-3)"
    - "End-to-end pipeline run produces final 3D midlines matching golden_triangulation.pt within 1e-3 tolerance"
    - "Tests fail hard on tolerance violations — no silent degradation"
    - "Tests are marked @pytest.mark.regression and excluded from fast test loop"
    - "generate_golden_data.py is updated to use PosePipeline instead of v1.0 functions"
    - "All nondeterministic operations are seeded for reproducibility"
  artifacts:
    - path: "tests/regression/__init__.py"
      provides: "Regression test package"
    - path: "tests/regression/conftest.py"
      provides: "Shared fixtures for loading golden data and running PosePipeline"
    - path: "tests/regression/test_per_stage_regression.py"
      provides: "Per-stage numerical comparison tests (5 stages)"
    - path: "tests/regression/test_end_to_end_regression.py"
      provides: "Full pipeline end-to-end regression test"
    - path: "pyproject.toml"
      provides: "regression marker registered in pytest config"
  key_links:
    - "tests/regression/conftest.py loads golden data from tests/golden/ using same fixtures as tests/golden/conftest.py"
    - "tests/regression/conftest.py runs PosePipeline via build_stages(config) + PosePipeline.run() to get new outputs"
    - "Per-stage tests compare PipelineContext fields against corresponding golden .pt files"
    - "generate_golden_data.py uses build_stages() + PosePipeline instead of pipeline.stages functions"
---

<objective>
Write numerical regression tests that run the new PosePipeline on the golden-data clip and compare every stage's output to the committed golden reference data. Also update generate_golden_data.py to use PosePipeline.

Purpose: VER-03 requires confirmed numerical equivalence between the migrated pipeline and v1.0. This is the acceptance bar for the entire refactor — if the numbers don't match, the port has bugs.

Output: A `tests/regression/` package with per-stage and end-to-end regression tests, a `regression` pytest marker, and an updated golden data generation script.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16-numerical-verification-and-legacy-cleanup/16-CONTEXT.md

@src/aquapose/engine/pipeline.py (PosePipeline + build_stages factory)
@src/aquapose/engine/stages.py (PipelineContext fields)
@src/aquapose/engine/config.py (PipelineConfig, load_config)
@tests/golden/conftest.py (existing golden data loading fixtures)
@tests/golden/test_stage_harness.py (existing structural tests — reference for golden data shapes)
@scripts/generate_golden_data.py (v1.0 script to update)

<interfaces>
From src/aquapose/engine/stages.py:
```python
@dataclass
class PipelineContext:
    frame_count: int | None = None
    camera_ids: list[str] | None = None
    detections: list[dict[str, list]] | None = None
    annotated_detections: list[dict[str, list]] | None = None
    associated_bundles: list[list] | None = None
    tracks: list[list] | None = None
    midlines_3d: list[dict] | None = None
    stage_timing: dict[str, float] = field(default_factory=dict)
```

From src/aquapose/engine/pipeline.py:
```python
def build_stages(config: PipelineConfig) -> list[Stage]: ...
class PosePipeline:
    def __init__(self, stages, config, observers=None): ...
    def run(self) -> PipelineContext: ...
```

Golden data files in tests/golden/:
- golden_detection.pt -> list[dict[str, list[Detection]]]
- golden_segmentation.pt.gz -> list[dict[str, list[tuple[ndarray, CropRegion]]]]
- golden_midline_extraction.pt -> list[dict[int, dict[str, Midline2D]]]
- golden_tracking.pt -> list[list[FishTrack]]
- golden_triangulation.pt -> list[dict[int, Midline3D]]
- metadata.pt -> dict with seed, stop_frame, camera_ids, etc.

NOTE: The v1.0 golden data has different stage boundaries than the new 5-stage model:
- v1.0 detection -> new Stage 1 (Detection): context.detections
- v1.0 segmentation + midline -> new Stage 2 (Midline): context.annotated_detections
- v1.0 tracking -> new Stage 4 (Tracking): context.tracks
- v1.0 triangulation -> new Stage 5 (Reconstruction): context.midlines_3d
- Stage 3 (Association) has no v1.0 golden data (was internal to tracker)
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create regression test infrastructure and per-stage tests</name>
  <files>
    tests/regression/__init__.py
    tests/regression/conftest.py
    tests/regression/test_per_stage_regression.py
    pyproject.toml
  </files>
  <action>
1. Register a `regression` marker in `pyproject.toml` under `[tool.pytest.ini_options]` markers list: `"regression: marks tests as regression (deselect with '-m \"not regression\"')"`. Also update the `test` script to exclude regression: `test = "pytest tests/ -m \"not slow and not regression\" {args}"`. Add a `test-regression` script: `test-regression = "pytest tests/regression/ -m regression {args}"`.

2. Create `tests/regression/__init__.py` with a module docstring.

3. Create `tests/regression/conftest.py` with:
   - Reuse golden data loading from `tests/golden/conftest.py` (import GOLDEN_DIR, or replicate the load logic — prefer importing the fixtures directly by re-exporting).
   - A session-scoped `pipeline_context` fixture that:
     a. Reads `metadata.pt` to get seed, stop_frame, detector_kind, max_fish, camera_ids.
     b. Sets deterministic seeds (random, numpy, torch, CUDA) using the same `_set_deterministic_seeds` helper.
     c. Constructs a `PipelineConfig` via `load_config()` with settings matching the golden data generation (same video_dir, calibration path, YOLO weights, U-Net weights, stop_frame, seed, max_fish, detector_kind). Use the paths from metadata or from well-known defaults (the test should skip if paths don't exist — `pytest.skip("Real data not available")`).
     d. Calls `build_stages(config)` and `PosePipeline(stages, config).run()` to produce a `PipelineContext`.
     e. Returns the context for comparison.
   - Tolerance constants: `DET_ATOL = 1e-6`, `SEG_ATOL = 1e-6`, `MID_ATOL = 1e-6`, `TRK_ATOL = 1e-6`, `RECON_ATOL = 1e-3`.

4. Create `tests/regression/test_per_stage_regression.py` with these tests, all marked `@pytest.mark.regression` and `@pytest.mark.slow`:

   a. `test_detection_regression(pipeline_context, golden_detections)` — Compare `context.detections` against `golden_detections`. For each frame and camera, compare detection count, bbox coordinates (np.allclose with DET_ATOL), and confidence values.

   b. `test_midline_regression(pipeline_context, golden_midlines)` — The new pipeline's Stage 2 produces `context.annotated_detections` (AnnotatedDetection objects with .midline). Extract Midline2D.points from annotated_detections and compare against golden_midline_extraction.pt. Match by fish_id and camera_id. Use MID_ATOL. NOTE: The mapping requires understanding how v1.0 MidlineSet (dict[fish_id, dict[cam_id, Midline2D]]) relates to the new annotated_detections structure — the new pipeline extracts midlines before tracking, so fish_id won't be available. Compare midline point arrays by camera within each frame using array-level allclose. If the structures diverge too much, use xfail with reason.

   c. `test_tracking_regression(pipeline_context, golden_tracks)` — Compare `context.tracks` against `golden_tracks`. For each frame, compare fish_id sets, and for each matching fish_id compare positions[-1] with TRK_ATOL.

   d. `test_reconstruction_regression(pipeline_context, golden_triangulation)` — Compare `context.midlines_3d` against `golden_triangulation`. For each frame and fish_id, compare Midline3D.control_points with np.allclose(atol=RECON_ATOL).

   Each test should produce a clear assertion message identifying which frame/camera/fish diverged and by how much.

  </action>
  <verify>
    <automated>hatch run python -m pytest tests/regression/ --collect-only 2>&1 | grep "test session starts"</automated>
    Verify that pytest discovers the regression test files and all tests are collected. Full regression run requires real data.
  </verify>
  <done>
    - tests/regression/ package exists with conftest.py and test_per_stage_regression.py
    - `regression` marker registered in pyproject.toml
    - `test` script excludes regression tests
    - All regression tests are marked @pytest.mark.regression and @pytest.mark.slow
    - Per-stage tolerances match CONTEXT.md decisions (1e-6 everywhere, 1e-3 for reconstruction)
    - Tests skip gracefully when golden data or real data paths are unavailable
  </done>
</task>

<task type="auto">
  <name>Task 2: Create end-to-end regression test and update golden data generator</name>
  <files>
    tests/regression/test_end_to_end_regression.py
    scripts/generate_golden_data.py
  </files>
  <action>
1. Create `tests/regression/test_end_to_end_regression.py` with:

   a. `test_end_to_end_3d_output(pipeline_context, golden_triangulation)` — marked `@pytest.mark.regression` and `@pytest.mark.slow`. This is the top-level acceptance test: compare the final 3D midline output from the full pipeline run against golden_triangulation.pt. For every frame with 3D midlines, assert that:
      - The set of fish_ids matches (or document divergence)
      - control_points arrays are np.allclose(atol=1e-3) for matching fish_ids
      - n_cameras values match
      Report total fish-frames compared and max absolute deviation.

   b. `test_pipeline_completes_all_stages(pipeline_context)` — marked `@pytest.mark.regression`. Assert that all PipelineContext fields are populated (not None): detections, annotated_detections, associated_bundles, tracks, midlines_3d, frame_count, camera_ids. Assert frame_count > 0.

   c. `test_pipeline_determinism(golden_metadata)` — marked `@pytest.mark.regression` and `@pytest.mark.slow`. Run the pipeline TWICE with the same seed/config and assert outputs are bit-identical. This validates the reproducibility contract. Use the same config construction as conftest.py but run PosePipeline.run() twice. Compare final midlines_3d control_points arrays with atol=0 (exact match). If CUDA nondeterminism makes this impossible, xfail with reason.

2. Update `scripts/generate_golden_data.py`:
   - Replace the stage-by-stage v1.0 execution with PosePipeline.
   - After `_set_deterministic_seeds(args.seed)`, construct a `PipelineConfig` via `load_config()` with the CLI args.
   - Call `build_stages(config)` and `PosePipeline(stages, config).run()` to get a `PipelineContext`.
   - Extract context fields and save them in the same format as before:
     - `context.detections` -> `golden_detection.pt`
     - The segmentation/mask data needs extraction from `context.annotated_detections` — save masks separately if the annotated_detection structure allows, or save the annotated_detections directly and update the golden data format comment.
     - `context.tracks` -> `golden_tracking.pt`
     - Extract midlines from annotated_detections -> `golden_midline_extraction.pt`
     - `context.midlines_3d` -> `golden_triangulation.pt`
   - Keep metadata.pt generation (same keys).
   - Remove the old per-stage v1.0 imports (pipeline.stages functions).
   - Keep the same CLI interface so existing invocation commands still work.
   - Add a note in the docstring that this now uses PosePipeline (v2.0 engine).

  </action>
  <verify>
    <automated>hatch run python -m pytest tests/regression/test_end_to_end_regression.py --collect-only 2>&1 | grep "test session starts"</automated>
    Verify test collection. Full regression run requires real data.
  </verify>
  <done>
    - test_end_to_end_regression.py exists with 3 tests (e2e 3D output, all-stages check, determinism)
    - generate_golden_data.py uses PosePipeline (build_stages + run) instead of v1.0 stage functions
    - generate_golden_data.py still produces the same .pt file names and structure
    - generate_golden_data.py CLI interface unchanged (same arguments work)
    - No imports from aquapose.pipeline.stages remain in generate_golden_data.py
  </done>
</task>

</tasks>

<verification>
1. `hatch run python -m pytest tests/regression/ --collect-only` — discovers all regression tests
2. `hatch run lint` — no lint errors in new files
3. `hatch run typecheck` — type annotations correct
4. `hatch run test` — fast test suite still passes (regression tests excluded)
5. `grep -r "regression" pyproject.toml` — marker registered
6. `grep -r "from aquapose.pipeline.stages" scripts/generate_golden_data.py` — no v1.0 imports remain
</verification>

<success_criteria>
- Regression test suite exists and is discoverable by pytest
- Per-stage tolerance thresholds match CONTEXT.md decisions
- Tests are isolated from the fast test loop via @pytest.mark.regression
- generate_golden_data.py uses the new PosePipeline engine
- All tests skip gracefully when real data/golden data is unavailable
</success_criteria>

<output>
After completion, create `.planning/phases/16-numerical-verification-and-legacy-cleanup/16-01-SUMMARY.md`
</output>
