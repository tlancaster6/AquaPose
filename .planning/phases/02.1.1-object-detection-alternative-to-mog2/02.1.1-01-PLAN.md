---
phase: 02.1.1-object-detection-alternative-to-mog2
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/sample_yolo_frames.py
  - data/yolo_fish/README.md
autonomous: false
requirements:
  - SEG-01
must_haves:
  truths:
    - "150 diverse frames (10 per ring camera + 30 center) are exported as JPEGs ready for annotation"
    - "Frame sampling is guided by MOG2 detection counts to ensure hard cases (0, 1, 2, many fish) are represented"
    - "Label Studio project with RectangleLabels is configured for bounding box annotation"
    - "User has annotated all 150 frames with fish bounding boxes in Label Studio"
    - "YOLO-format dataset directory (images/train, images/val, labels/train, labels/val) exists with 80/20 stratified split"
  artifacts:
    - path: "scripts/sample_yolo_frames.py"
      provides: "MOG2-guided frame sampling and export for YOLO annotation"
    - path: "data/yolo_fish/dataset.yaml"
      provides: "YOLO dataset configuration for ultralytics training"
    - path: "data/yolo_fish/images/train/"
      provides: "120 training images"
    - path: "data/yolo_fish/images/val/"
      provides: "30 validation images"
  key_links:
    - from: "scripts/sample_yolo_frames.py"
      to: "src/aquapose/segmentation/detector.py"
      via: "Uses MOG2Detector to count detections per frame for diversity sampling"
      pattern: "MOG2Detector.*detect"
---

<objective>
Sample 150 diverse frames from the 13-camera aquarium rig using MOG2 detection counts for diversity, export to Label Studio for bounding box annotation, then organize the annotated YOLO export into the ultralytics dataset directory structure.

Purpose: The YOLO detector needs training data. This plan produces the annotated dataset that Plan 02 will consume for training.
Output: `data/yolo_fish/` directory with train/val split ready for `model.train(data=...)`.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02.1.1-object-detection-alternative-to-mog2/02.1.1-CONTEXT.md
@.planning/phases/02.1.1-object-detection-alternative-to-mog2/02.1.1-RESEARCH.md
@src/aquapose/segmentation/detector.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build MOG2-guided frame sampling script</name>
  <files>scripts/sample_yolo_frames.py</files>
  <action>
Create `scripts/sample_yolo_frames.py` that:

1. Accepts CLI arguments: `--video-dir` (path to directory containing per-camera video files), `--output-dir` (default `data/yolo_fish_raw/`), `--warmup-frames` (default 200), `--sample-stride` (default 100, check every Nth frame for efficiency).

2. For each of the 13 cameras:
   - Open the video file, warm up MOG2Detector with `--warmup-frames` frames.
   - Scan remaining frames at `--sample-stride` intervals, running `detector.detect()` on each.
   - Record `(camera_id, frame_idx, mog2_count, frame_path)` for each scanned frame.

3. Per camera, bin frames by MOG2 detection count into buckets: 0, 1, 2, 3+ fish.
   - For ring cameras (12): sample 10 frames total, distributing evenly across available bins (at least 1 per non-empty bin, fill remainder from largest bin).
   - For center camera (1): sample 30 frames with the same bin-balancing strategy.

4. Export sampled frames as JPEG files to `--output-dir/images/` with naming convention `{camera_id}_frame_{frame_idx:06d}.jpg`.

5. Print a summary table: per-camera frame counts and bin distribution.

Use `from aquapose.segmentation import MOG2Detector` for detection. The script must identify which camera is the center camera — accept `--center-camera` CLI arg (camera ID string) to distinguish it from ring cameras.

The script does NOT need Label Studio import — the user will manually import the exported JPEGs into Label Studio via the web UI.
  </action>
  <verify>
Run `python scripts/sample_yolo_frames.py --help` to confirm CLI arguments are correct.
Verify the script imports and parses arguments without error.
  </verify>
  <done>
Script exists, is runnable, accepts the documented CLI arguments, and uses MOG2Detector for detection-count-based frame sampling.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Run frame sampling, annotate in Label Studio, build YOLO dataset</name>
  <files>data/yolo_fish/dataset.yaml, data/yolo_fish/images/, data/yolo_fish/labels/</files>
  <action>
This is a human verification checkpoint. Claude has automated the frame sampling script. The user must run sampling, annotate bounding boxes, and organize the dataset.

What was built:
- MOG2-guided frame sampling script (`scripts/sample_yolo_frames.py`)
- Exports 150 diverse frames (10 per ring camera + 30 center) as JPEGs

Steps for user:
1. Run the sampling script:
   `python scripts/sample_yolo_frames.py --video-dir /path/to/videos --center-camera e3v8340`
   Adjust `--center-camera` to match your center camera ID.

2. Verify 150 frames were exported to `data/yolo_fish_raw/images/` (120 ring + 30 center).

3. Create a new Label Studio project with RectangleLabels labeling config:
   ```xml
   <View>
     <Image name="image" value="$image"/>
     <RectangleLabels name="label" toName="image">
       <Label value="fish" background="green"/>
     </RectangleLabels>
   </View>
   ```

4. Import the 150 JPEGs into Label Studio and annotate all frames with bounding boxes around each fish.

5. Export from Label Studio as "YOLO" format. Extract the zip.

6. Organize into YOLO dataset directory structure with stratified 80/20 train/val split per camera:
   `data/yolo_fish/{images,labels}/{train,val}/`

7. Create `data/yolo_fish/dataset.yaml` with absolute path, nc=1, names: {0: fish}.
  </action>
  <verify>
`ls data/yolo_fish/images/train/ | wc -l` shows ~120 images.
`ls data/yolo_fish/images/val/ | wc -l` shows ~30 images.
Each image has a corresponding `.txt` label file in `labels/`.
  </verify>
  <done>
150 annotated frames organized into YOLO-format dataset at `data/yolo_fish/` with stratified train/val split, ready for `model.train(data=...)`.
Type "done" when the YOLO dataset is ready, or describe any issues.
  </done>
</task>

</tasks>

<verification>
- `scripts/sample_yolo_frames.py` exists and runs without import errors
- `data/yolo_fish/dataset.yaml` exists with correct paths
- `data/yolo_fish/images/train/` contains ~120 images
- `data/yolo_fish/images/val/` contains ~30 images
- `data/yolo_fish/labels/train/` and `labels/val/` contain matching `.txt` files
</verification>

<success_criteria>
150 annotated frames are organized into a YOLO-format dataset with stratified train/val split, ready to be consumed by `model.train(data="data/yolo_fish/dataset.yaml")` in Plan 02.
</success_criteria>

<output>
After completion, create `.planning/phases/02.1.1-object-detection-alternative-to-mog2/02.1.1-01-SUMMARY.md`
</output>
