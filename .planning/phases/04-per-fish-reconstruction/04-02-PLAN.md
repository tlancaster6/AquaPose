---
phase: 04-per-fish-reconstruction
plan: 02
type: execute
wave: 2
depends_on: [04-01]
files_modified:
  - src/aquapose/optimization/optimizer.py
  - src/aquapose/optimization/__init__.py
  - tests/unit/optimization/test_optimizer.py
autonomous: true
requirements: [RECON-03, RECON-04]
must_haves:
  truths:
    - "First-frame optimization runs 2-start (forward + 180-degree flip) and selects the lower-loss result"
    - "Warm-start optimization uses constant-velocity prediction from previous 2 frames to initialize the next frame"
    - "Optimizer converges within iteration cap with early-stop when loss delta is below threshold for 3 consecutive steps"
    - "Each fish is optimized independently per frame using Adam on FishState parameters with gradient clipping"
  artifacts:
    - path: "src/aquapose/optimization/optimizer.py"
      provides: "FishOptimizer with 2-start, warm-start, and convergence logic"
      exports: ["FishOptimizer", "make_optimizable_state", "warm_start_from_velocity"]
    - path: "tests/unit/optimization/test_optimizer.py"
      provides: "Tests for 2-start selection, warm-start, convergence, and gradient clipping"
  key_links:
    - from: "src/aquapose/optimization/optimizer.py"
      to: "src/aquapose/optimization/renderer.py"
      via: "FishOptimizer.optimize calls renderer.render() each iteration"
      pattern: "renderer\\.render\\("
    - from: "src/aquapose/optimization/optimizer.py"
      to: "src/aquapose/optimization/loss.py"
      via: "FishOptimizer.optimize calls multi_objective_loss each iteration"
      pattern: "multi_objective_loss\\("
    - from: "src/aquapose/optimization/optimizer.py"
      to: "src/aquapose/mesh/builder.py"
      via: "FishOptimizer builds mesh from state each iteration"
      pattern: "build_fish_mesh\\("
---

<objective>
Build the FishOptimizer with 2-start first-frame initialization, warm-start frame-to-frame optimization, and convergence logic.

Purpose: The optimizer is the outer loop that drives analysis-by-synthesis — it calls the renderer and loss from Plan 01 each iteration, updates FishState via Adam, and manages the multi-start and warm-start strategies specified in user decisions.

Output: `optimizer.py` (FishOptimizer, make_optimizable_state, warm_start_from_velocity) with unit tests covering all optimization strategies.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-per-fish-reconstruction/04-RESEARCH.md
@.planning/phases/04-per-fish-reconstruction/04-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement FishOptimizer with 2-start and warm-start strategies</name>
  <files>
    src/aquapose/optimization/optimizer.py
    src/aquapose/optimization/__init__.py
  </files>
  <action>
  **Step 1: Create `src/aquapose/optimization/optimizer.py`.**

  Implement utility functions:

  `make_optimizable_state(init_state: FishState, device: str = "cpu") -> FishState`:
  - Clone all 5 tensors with `.detach().clone().requires_grad_(True).to(device)`
  - Return new FishState with gradient-enabled tensors

  `get_state_params(state: FishState) -> list[Tensor]`:
  - Return `[state.p, state.psi, state.theta, state.kappa, state.s]`

  `warm_start_from_velocity(state_t1: FishState, state_t2: FishState | None) -> FishState`:
  - If `state_t2` is None, return detached copy of `state_t1`
  - Otherwise: constant-velocity extrapolation for p and psi: `p_pred = p_t1 + (p_t1 - p_t2)`, `psi_pred = psi_t1 + (psi_t1 - psi_t2)`
  - theta, kappa, s: copy from state_t1 (no velocity model for these)
  - All tensors detached (no autograd from prior frames)

  Implement `FishOptimizer`:
  - `__init__(self, renderer, loss_weights, camera_weights, lr=1e-3, max_iters_first=300, max_iters_warmstart=50, early_exit_iters=50, convergence_delta=1e-4, convergence_patience=3, grad_clip_norm=1.0)` — store all config; renderer is a RefractiveSilhouetteRenderer instance
  - Internal `_run_optimization_loop(self, state, target_masks, crop_regions, cameras, camera_ids, n_iters) -> tuple[FishState, float]`:
    1. `state = make_optimizable_state(state)`
    2. Create Adam optimizer with `get_state_params(state)`, lr from config. Use param groups: `[{"params": [state.p], "lr": lr * 5}, {"params": [state.psi, state.theta, state.kappa, state.s], "lr": lr}]` (position benefits from larger LR per RESEARCH.md)
    3. Loop for `n_iters`:
       a. `optimizer.zero_grad()`
       b. `meshes = build_fish_mesh([state])`
       c. `alpha_maps = self.renderer.render(meshes, cameras, camera_ids)`
       d. `losses = multi_objective_loss(state, alpha_maps, target_masks, crop_regions, self.camera_weights, self.loss_weights)`
       e. `losses["total"].backward()`
       f. `clip_grad_norm_(get_state_params(state), self.grad_clip_norm)`
       g. `optimizer.step()`
       h. Track `loss_val = losses["total"].item()` in history list
       i. Check convergence: if last `convergence_patience + 1` loss values have delta < `convergence_delta`, break early
    4. Return `(state, final_loss_val)`

  - `optimize_first_frame(self, init_state, target_masks, crop_regions, cameras, camera_ids) -> FishState`:
    1. Start A: original init_state
    2. Start B: clone init_state but flip psi by pi: `flipped_psi = init_state.psi.detach() + torch.pi`
    3. Run `_run_optimization_loop` on both for `early_exit_iters` iterations
    4. Pick winner (lower loss)
    5. Run winner for remaining `max_iters_first - early_exit_iters` iterations
    6. Return winner state

  - `optimize_frame(self, init_state, target_masks, crop_regions, cameras, camera_ids) -> FishState`:
    1. Run `_run_optimization_loop` with `max_iters_warmstart` iterations
    2. Return optimized state

  - `optimize_sequence(self, first_frame_state, frames_data: list[dict], cameras, camera_ids) -> list[FishState]`:
    1. First frame: call `optimize_first_frame`
    2. Subsequent frames: call `warm_start_from_velocity` using last 2 results, then `optimize_frame`
    3. Return list of optimized FishState per frame
    4. Each `frames_data[i]` dict has keys `"target_masks"` and `"crop_regions"`

  **Step 2: Update `src/aquapose/optimization/__init__.py`** adding `FishOptimizer`, `make_optimizable_state`, `warm_start_from_velocity` to `__all__`.
  </action>
  <verify>
  ```bash
  python -c "from aquapose.optimization import FishOptimizer; print('OK')"
  ```
  </verify>
  <done>
  - FishOptimizer implements 2-start first-frame and warm-start subsequent-frame optimization
  - Adam optimizer with per-parameter learning rates and gradient clipping
  - Convergence early-stop when loss delta below threshold for patience steps
  - warm_start_from_velocity extrapolates position and heading from last 2 frames
  - make_optimizable_state creates gradient-enabled FishState copies
  </done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for optimizer strategies</name>
  <files>
    tests/unit/optimization/test_optimizer.py
  </files>
  <action>
  Create `tests/unit/optimization/test_optimizer.py` with the following tests. Tests should use synthetic/mock setups to avoid requiring real video data or GPU-heavy rendering.

  **Helper setup:** Create a mock renderer that returns a fixed alpha map (or a simple differentiable function of vertex positions) so optimizer tests don't require full pytorch3d rendering. Alternatively, if Plan 01's renderer works on CPU with small image sizes (e.g., 32x32), use a tiny real renderer.

  Tests:

  - `test_make_optimizable_state_requires_grad`: All 5 tensors have `requires_grad=True`
  - `test_make_optimizable_state_detached`: Changing the original state doesn't affect the optimizable copy
  - `test_warm_start_no_previous`: With `state_t2=None`, returns detached copy of state_t1
  - `test_warm_start_constant_velocity`: With 2 prior states, predicted position extrapolates linearly: `p_pred = p_t1 + (p_t1 - p_t2)`
  - `test_warm_start_detached`: All returned tensors have `requires_grad=False`
  - `test_optimize_first_frame_selects_lower_loss`: Create two init states where one is clearly better (closer to a known target). Verify the optimizer returns the one that started closer (lower loss after early exit).
  - `test_optimize_first_frame_flips_psi`: Verify the second start has `psi = init_psi + pi`
  - `test_convergence_early_stop`: Set `convergence_delta` high enough that optimizer stops well before `max_iters`. Verify the loop terminates early (check iteration count or timing).
  - `test_gradient_clipping_prevents_explosion`: Manually set large gradients on state params, run one step with clipping, verify param change is bounded
  - `test_optimize_sequence_uses_warm_start`: Run a 3-frame sequence. Verify frame 2+ init position differs from frame 1's result (constant-velocity prediction applied).
  - `test_optimizer_loss_decreases`: Run optimizer for a few iterations on a simple case, verify final loss < initial loss

  For mock renderer: Create a class with a `render()` method that produces a simple differentiable alpha map (e.g., a Gaussian blob centered at the projected fish position). This allows testing optimizer logic without full pytorch3d rendering.
  </action>
  <verify>
  ```bash
  hatch run test tests/unit/optimization/test_optimizer.py -v
  hatch run test  # all tests pass
  ```
  </verify>
  <done>
  - 2-start optimizer selects the lower-loss initialization
  - Warm-start constant-velocity prediction correctly extrapolates from 2 prior frames
  - Convergence early-stop terminates before max_iters when loss plateaus
  - Gradient clipping bounds parameter updates
  - Sequence optimization chains warm-start across frames
  - All optimizer tests pass
  </done>
</task>

</tasks>

<verification>
```bash
# All Plan 02 tests
hatch run test tests/unit/optimization/test_optimizer.py -v

# All optimization module tests (Plan 01 + Plan 02)
hatch run test tests/unit/optimization/ -v

# Full test suite
hatch run test
```
</verification>

<success_criteria>
- FishOptimizer.optimize_first_frame runs 2-start and returns the lower-loss result
- FishOptimizer.optimize_frame runs warm-start optimization with convergence early-stop
- FishOptimizer.optimize_sequence chains first-frame + warm-start across a frame sequence
- Gradient clipping prevents explosion through refractive projection Newton-Raphson chain
- Loss decreases during optimization (optimizer actually improves the pose)
- All tests pass including existing tests (no regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/04-per-fish-reconstruction/04-02-SUMMARY.md`
</output>
