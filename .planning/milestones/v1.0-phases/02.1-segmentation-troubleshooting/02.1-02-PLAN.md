---
phase: 02.1-segmentation-troubleshooting
plan: 02
type: execute
wave: 2
depends_on:
  - 02.1-01
files_modified:
  - scripts/test_sam2.py
autonomous: false
requirements:
  - SEG-2
  - SEG-3
must_haves:
  truths:
    - "SAM2 pseudo-label masks (generated via YOLO detection) achieve >= 0.70 mean IoU against manual ground truth"
    - "Visual overlay images show SAM2 mask vs ground truth side-by-side for inspection"
    - "Numeric per-frame IoU is printed with overall mean and PASS/FAIL"
  artifacts:
    - path: "scripts/test_sam2.py"
      provides: "SAM2 evaluation script comparing YOLO-sourced pseudo-labels to manual GT"
      contains: "mask_iou"
  key_links:
    - from: "scripts/test_sam2.py"
      to: "output/pseudo_labels/"
      via: "loads SAM2 masks generated by run_pseudo_labels.py --detector yolo"
      pattern: "pseudo_labels"
    - from: "scripts/run_pseudo_labels.py"
      to: "src/aquapose/segmentation/detector.py"
      via: "make_detector('yolo') creates YOLODetector for first-stage detection"
      pattern: "make_detector"
---

<objective>
Generate YOLO-sourced SAM2 pseudo-labels using `scripts/run_pseudo_labels.py --detector yolo`, then evaluate pseudo-label quality against manually annotated ground truth. User annotates GT, then the test script computes mask IoU.

Purpose: SAM2 quality determines whether pseudo-labels are usable as starting points for Label Studio correction. Using YOLO instead of MOG2 as the first-stage detector is expected to provide better bounding boxes (higher recall, no warmup needed), leading to better SAM2 masks.

Output: `scripts/test_sam2.py` producing IoU metrics and visual comparison images; output in `output/test_sam2/`
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02.1-segmentation-troubleshooting/02.1-RESEARCH.md
@.planning/phases/02.1-segmentation-troubleshooting/02.1-01-SUMMARY.md
@.planning/phases/02.1.1-object-detection-alternative-to-mog2/02.1.1-03-SUMMARY.md

@scripts/run_pseudo_labels.py
@src/aquapose/segmentation/pseudo_labeler.py
@src/aquapose/segmentation/label_studio.py
@src/aquapose/segmentation/detector.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create SAM2 evaluation script using YOLO-sourced pseudo-labels</name>
  <files>scripts/test_sam2.py</files>
  <action>
Create `scripts/test_sam2.py` that evaluates SAM2 pseudo-label quality against manual ground truth masks.

**Prerequisite â€” generate YOLO-sourced pseudo-labels:** Before evaluation can run, pseudo-labels must be generated using the YOLO pipeline from Phase 02.1.1:
```
hatch run python scripts/run_pseudo_labels.py \
  --video-dir "C:/Users/tucke/Desktop/Aqua/AquaPose/raw_videos" \
  --detector yolo \
  --yolo-weights runs/detect/output/yolo_fish/train_v1/weights/best.pt \
  --output-dir output/pseudo_labels \
  --frame-stride 100 \
  --max-frames-per-camera 5
```
This produces `output/pseudo_labels/tasks.json` (Label Studio format with RLE masks) and `output/pseudo_labels/images/` (source frames as JPEGs). The test script Task 1 creates will evaluate these YOLO-sourced SAM2 masks. If `output/pseudo_labels/tasks.json` already exists, skip regeneration.

**Ground truth format:** Binary PNG files (white=fish, black=background) placed in a GT directory. One file per fish per frame, named `{frame_id}_gt_{n}.png` where `frame_id` matches the frame IDs from `output/pseudo_labels/` (e.g., `e3v831e_frame_001234_gt_0.png`).

**Script behavior:**
1. Accept CLI args: `--pseudo-labels-dir` (default `output/pseudo_labels`), `--gt-dir` (required -- path to directory of GT binary PNGs), `--output-dir` (default `output/test_sam2`), `--threshold` (default 0.70)

2. Load SAM2 pseudo-label masks from the tasks JSON in `pseudo-labels-dir`:
   - Parse `{pseudo-labels-dir}/tasks.json` (Label Studio format)
   - Decode RLE masks from the predictions (use `aquapose.segmentation.label_studio` utilities or `pycocotools.mask`)

3. Load GT binary PNG masks from `--gt-dir`

4. Match SAM2 predictions to GT per frame:
   - Group GT masks and SAM2 masks by `frame_id`
   - For each frame: compute pairwise IoU matrix between all GT masks and all SAM2 prediction masks
   - Use greedy matching (sort by IoU descending, match one-to-one) -- `scipy.optimize.linear_sum_assignment` is overkill for ~9 fish
   - Unmatched GT masks count as IoU=0 (missed)
   - Unmatched SAM2 masks are false positives (report count but don't penalize IoU)

5. Compute IoU using `pycocotools.mask.iou()` as described in RESEARCH.md (encode both masks as RLE, compute IoU)

6. Visual output: For each frame, save a side-by-side image to `output-dir/`:
   - Left: original image with GT mask contours (red) overlaid
   - Right: original image with SAM2 mask contours (green) overlaid
   - Print IoU for each matched pair on the image

7. Print summary:
   - Per-frame: frame_id, num_gt, num_pred, num_matched, mean_iou
   - Overall: mean IoU across all matched pairs, PASS/FAIL against threshold
   - Note in output that detections came from YOLO (not MOG2) for traceability
   - Exit 0 if PASS, exit 1 if FAIL

Note: The ground truth does not exist yet. The script must be written first, then the user will annotate images before the script can be run. The pseudo-labels in `output/pseudo_labels/` must also be generated via `run_pseudo_labels.py --detector yolo` before evaluation (see prerequisite above).
  </action>
  <verify>
- `scripts/test_sam2.py` exists and `hatch run python scripts/test_sam2.py --help` runs without error
- Script correctly handles the case where GT directory is empty (prints a message and exits)
  </verify>
  <done>
SAM2 evaluation script created and tested for import/help correctness. Ready for user to generate YOLO pseudo-labels and provide GT annotations.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: User generates YOLO pseudo-labels, annotates GT, and runs SAM2 evaluation</name>
  <files>N/A -- verification only</files>
  <action>
    This is a human verification checkpoint. Claude has automated the SAM2 evaluation script. The user generates YOLO-sourced pseudo-labels, provides ground truth annotations, and runs the evaluation.

    What was built:
    - SAM2 evaluation script (`scripts/test_sam2.py`) comparing pseudo-labels to manual GT
    - Loads SAM2 masks from `output/pseudo_labels/` (generated via YOLO detection)
    - Computes per-frame and mean mask IoU against GT binary PNGs

    Steps for user:

    **Step A: Generate YOLO-sourced pseudo-labels** (if `output/pseudo_labels/tasks.json` does not already exist)
    1. Run: `hatch run python scripts/run_pseudo_labels.py --video-dir "C:/Users/tucke/Desktop/Aqua/AquaPose/raw_videos" --detector yolo --yolo-weights runs/detect/output/yolo_fish/train_v1/weights/best.pt --output-dir output/pseudo_labels --max-frames-per-camera 5`
    2. Verify `output/pseudo_labels/tasks.json` and `output/pseudo_labels/images/` exist with generated frames

    **Step B: Create ground truth annotations**
    1. Select 3-5 frames from `output/pseudo_labels/images/` (pick frames from different cameras, including at least one with a difficult/low-contrast fish)
    2. For each selected frame, create binary PNG ground truth masks (one per fish):
       - Use any image editor (Paint, GIMP, etc.)
       - White (255) = fish pixels, Black (0) = background
       - Name files: `{frame_id}_gt_0.png`, `{frame_id}_gt_1.png`, etc.
       - The `frame_id` is the filename stem of the source frame (e.g., `e3v831e_frame_001234`)
    3. Place all GT PNGs in a single directory (e.g., `output/sam2_gt/`)

    **Step C: Run evaluation**
    1. Run: `hatch run python scripts/test_sam2.py --gt-dir output/sam2_gt`
    2. Review per-frame IoU and side-by-side images in `output/test_sam2/`
  </action>
  <verify>
    - Does the script print per-frame IoU without errors?
    - Does the overall mean IoU meet >= 0.70?
    - Do the side-by-side images in `output/test_sam2/` look reasonable?
  </verify>
  <done>
    SAM2 IoU >= 0.70 confirmed by user (using YOLO-sourced detections), or issues described for fix task. Type "approved" if passing, or describe failures.
  </done>
</task>

</tasks>

<verification>
- `scripts/run_pseudo_labels.py --detector yolo` generates pseudo-labels in `output/pseudo_labels/`
- `scripts/test_sam2.py` runs without error when GT directory exists with properly named files
- Side-by-side visual comparisons saved to `output/test_sam2/`
- Mean IoU >= 0.70 against manual GT, or issues documented for fix
</verification>

<success_criteria>
SAM2 pseudo-labels generated via YOLO detection achieve >= 0.70 mean mask IoU against manual ground truth on a representative sample of frames. Visual evidence and numeric metrics available for review.
</success_criteria>

<output>
After completion, create `.planning/phases/02.1-segmentation-troubleshooting/02.1-02-SUMMARY.md`
</output>
