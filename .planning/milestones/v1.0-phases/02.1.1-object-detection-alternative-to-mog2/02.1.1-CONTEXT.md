# Phase 02.1.1: Object-detection alternative to MOG2 - Context

**Gathered:** 2026-02-20
**Status:** Ready for planning

<domain>
## Phase Boundary

Replace MOG2 background subtraction with a YOLO object detector as an alternative first-stage fish localizer. Train on manually annotated bounding boxes, evaluate against MOG2, and wire into the existing SAM2 → Mask R-CNN pipeline. MOG2 remains as a runtime-configurable alternative path.

</domain>

<decisions>
## Implementation Decisions

### Detector choice
- YOLOv8 (nano or small variant) via ultralytics
- Bounding-box detection only (not instance segmentation)
- Fine-tune from COCO pretrained weights — no zero-shot baseline needed
- Single-class detection: "fish"

### Training data strategy
- 150 manually annotated frames with bounding boxes
- Camera distribution: 10 frames per ring camera (12 × 10 = 120) + 30 frames from center camera
- MOG2-guided frame sampling: pick frames with varying fish counts (0, 1, 2, many) to ensure hard cases are represented
- Annotation tool: Label Studio (already set up from Phase 02.1)
- Train/val split stratified by camera: 80% train / 20% val per camera (8 train + 2 val per ring, 24 train + 6 val center → 120 train / 30 val total)

### Pipeline integration
- MOG2 and YOLO are runtime-configurable alternatives — user chooses one or the other
- YOLODetector class lives in `segmentation/detector.py` alongside MOG2Detector, sharing the Detection dataclass
- YOLO boxes feed into SAM2 as box prompts for mask generation (same role MOG2 plays today)
- Full pipeline wiring included in this phase: YOLO → SAM2 → Mask R-CNN path must be usable end-to-end

### Evaluation criteria
- Primary metrics: recall and precision (both tracked, recall is most important)
- Success bar: YOLO must beat MOG2's recall on the same val set (relative comparison, not absolute threshold)
- Visual comparison: side-by-side annotated frames showing YOLO boxes vs MOG2 boxes on the same frames
- Evaluation on stratified val set (30 frames, 2 per ring camera + 6 center)

### Claude's Discretion
- Exact YOLO model variant (v8n vs v8s) based on training results. Start with v8n
- Training hyperparameters (epochs, augmentation, learning rate).
- MOG2-guided sampling algorithm (how to select diverse frames)
- Frame export script design for Label Studio import
- Runtime configuration mechanism (config file, CLI flag, etc.)

</decisions>

<specifics>
## Specific Ideas

- MOG2 already shows weaknesses: stationary fish missed, over-splitting blobs, shadow artifacts — YOLO should handle these better since it learns fish appearance, not just motion
- The 150-frame annotation budget is deliberate: 10 per ring camera ensures every viewpoint is represented, 30 from center camera because it has a unique top-down view
- Stratified val split ensures evaluation covers all camera angles, not just the easiest views

</specifics>

<deferred>
## Deferred Ideas

None — discussion stayed within phase scope

</deferred>

---

*Phase: 02.1.1-object-detection-alternative-to-mog2*
*Context gathered: 2026-02-20*
