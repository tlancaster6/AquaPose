---
phase: 02.1-segmentation-troubleshooting
plan: 02
type: execute
wave: 2
depends_on:
  - 02.1-01
files_modified:
  - scripts/test_sam2.py
autonomous: false
requirements:
  - SEG-2
  - SEG-3
must_haves:
  truths:
    - "SAM2 pseudo-label masks achieve >= 0.70 mean IoU against manual ground truth"
    - "Visual overlay images show SAM2 mask vs ground truth side-by-side for inspection"
    - "Numeric per-frame IoU is printed with overall mean and PASS/FAIL"
  artifacts:
    - path: "scripts/test_sam2.py"
      provides: "SAM2 evaluation script comparing pseudo-labels to manual GT"
      contains: "mask_iou"
  key_links:
    - from: "scripts/test_sam2.py"
      to: "output/verify_pseudo_labels/"
      via: "loads existing SAM2 masks from prior generate run"
      pattern: "verify_pseudo_labels"
---

<objective>
Evaluate SAM2 pseudo-label quality against manually annotated ground truth on a small set of frames already generated in `output/verify_pseudo_labels/`. User annotates GT, then the test script computes mask IoU.

Purpose: SAM2 quality determines whether pseudo-labels are usable as starting points for Label Studio correction. If IoU is too low, the Label Studio workflow becomes impractically labor-intensive.

Output: `scripts/test_sam2.py` producing IoU metrics and visual comparison images; output in `output/test_sam2/`
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02.1-segmentation-troubleshooting/02.1-RESEARCH.md
@.planning/phases/02.1-segmentation-troubleshooting/02.1-01-SUMMARY.md

@scripts/verify_pseudo_labels.py
@src/aquapose/segmentation/pseudo_labeler.py
@src/aquapose/segmentation/label_studio.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create SAM2 evaluation script</name>
  <files>scripts/test_sam2.py</files>
  <action>
Create `scripts/test_sam2.py` that evaluates SAM2 pseudo-label quality against manual ground truth masks.

**Ground truth format:** Binary PNG files (white=fish, black=background) placed in a GT directory. One file per fish per frame, named `{frame_id}_gt_{n}.png` where `frame_id` matches the frame IDs from `output/verify_pseudo_labels/` (e.g., `e3v831e_frame_001234_gt_0.png`).

**Script behavior:**
1. Accept CLI args: `--pseudo-labels-dir` (default `output/verify_pseudo_labels`), `--gt-dir` (required — path to directory of GT binary PNGs), `--output-dir` (default `output/test_sam2`), `--threshold` (default 0.70)

2. Load SAM2 pseudo-label masks from the tasks JSON in `pseudo-labels-dir`:
   - Parse `output/verify_pseudo_labels/tasks.json` (Label Studio format)
   - Decode RLE masks from the predictions (use `aquapose.segmentation.label_studio` utilities or `pycocotools.mask`)

3. Load GT binary PNG masks from `--gt-dir`

4. Match SAM2 predictions to GT per frame:
   - Group GT masks and SAM2 masks by `frame_id`
   - For each frame: compute pairwise IoU matrix between all GT masks and all SAM2 prediction masks
   - Use greedy matching (sort by IoU descending, match one-to-one) — `scipy.optimize.linear_sum_assignment` is overkill for ~9 fish
   - Unmatched GT masks count as IoU=0 (missed)
   - Unmatched SAM2 masks are false positives (report count but don't penalize IoU)

5. Compute IoU using `pycocotools.mask.iou()` as described in RESEARCH.md (encode both masks as RLE, compute IoU)

6. Visual output: For each frame, save a side-by-side image to `output-dir/`:
   - Left: original image with GT mask contours (red) overlaid
   - Right: original image with SAM2 mask contours (green) overlaid
   - Print IoU for each matched pair on the image

7. Print summary:
   - Per-frame: frame_id, num_gt, num_pred, num_matched, mean_iou
   - Overall: mean IoU across all matched pairs, PASS/FAIL against threshold
   - Exit 0 if PASS, exit 1 if FAIL

Note: The ground truth does not exist yet. The script must be written first, then the user will annotate images before the script can be run.
  </action>
  <verify>
- `scripts/test_sam2.py` exists and `hatch run python scripts/test_sam2.py --help` runs without error
- Script correctly handles the case where GT directory is empty (prints a message and exits)
  </verify>
  <done>
SAM2 evaluation script created and tested for import/help correctness. Ready for user to provide GT annotations.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: User annotates GT and runs SAM2 evaluation</name>
  <files>N/A — verification only</files>
  <action>
    This is a human verification checkpoint. Claude has automated the SAM2 evaluation script. The user provides ground truth annotations and runs the evaluation.

    What was built:
    - SAM2 evaluation script (`scripts/test_sam2.py`) comparing pseudo-labels to manual GT
    - Loads SAM2 masks from `output/verify_pseudo_labels/`
    - Computes per-frame and mean mask IoU against GT binary PNGs

    Steps for user:
    1. Select 3-5 frames from `output/verify_pseudo_labels/source_frames/` (pick frames from different cameras, including at least one with a difficult/low-contrast fish)
    2. For each selected frame, create binary PNG ground truth masks (one per fish):
       - Use any image editor (Paint, GIMP, etc.)
       - White (255) = fish pixels, Black (0) = background
       - Name files: `{frame_id}_gt_0.png`, `{frame_id}_gt_1.png`, etc.
       - The `frame_id` is the filename stem of the source frame (e.g., `e3v831e_frame_001234`)
    3. Place all GT PNGs in a single directory (e.g., `output/sam2_gt/`)
    4. Run: `hatch run python scripts/test_sam2.py --gt-dir output/sam2_gt`
    5. Review per-frame IoU and side-by-side images in `output/test_sam2/`
  </action>
  <verify>
    - Does the script print per-frame IoU without errors?
    - Does the overall mean IoU meet >= 0.70?
    - Do the side-by-side images in `output/test_sam2/` look reasonable?
  </verify>
  <done>
    SAM2 IoU >= 0.70 confirmed by user, or issues described for fix task. Type "approved" if passing, or describe failures.
  </done>
</task>

</tasks>

<verification>
- `scripts/test_sam2.py` runs without error when GT directory exists with properly named files
- Side-by-side visual comparisons saved to `output/test_sam2/`
- Mean IoU >= 0.70 against manual GT, or issues documented for fix
</verification>

<success_criteria>
SAM2 pseudo-labels achieve >= 0.70 mean mask IoU against manual ground truth on a representative sample of frames. Visual evidence and numeric metrics available for review.
</success_criteria>

<output>
After completion, create `.planning/phases/02.1-segmentation-troubleshooting/02.1-02-SUMMARY.md`
</output>
