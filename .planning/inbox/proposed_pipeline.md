# AquaPose: Detailed Pipeline Overview

## System Context

- **Rig**: 13 cameras (12 in a ring at ~0.6m radius + 1 center), mounted ~1m above a cylindrical tank (2m diameter, 1m tall), all oriented straight down through a flat water surface.
- **Subjects**: 9 fish (3 male, 6 female), ~10cm body length, in clear water with controlled diffuse lighting.
- **Capture**: 30 fps, synchronized.
- **Calibration**: Refraction-aware, sub-millimeter reprojection error, via custom library.
- **Geometry**: 25–50° best triangulation angle across the tank volume; 3–5 camera coverage everywhere; X-Y reconstruction strong, Z (depth) weaker but adequate.

---

## Core Approach: Analysis-by-Synthesis

Rather than triangulating sparse keypoints, AquaPose defines a 3D parametric fish model and optimizes its state until its refractive projection matches observed multi-view silhouettes. This sidesteps the fragility of point correspondence under refraction and leverages the full silhouette as a dense constraint.

### Fish State Vector

$$\mathbf{S} = \{\mathbf{p}, \psi, \boldsymbol{\kappa}, s\}$$

| Symbol | Description |
|---|---|
| $\mathbf{p} \in \mathbb{R}^3$ | Centroid position $(x, y, z)$ in world coordinates |
| $\psi \in [0, 2\pi)$ | Heading (yaw). Pitch and roll are regularized toward zero by a gravity prior but not locked. |
| $\boldsymbol{\kappa} \in \mathbb{R}^K$ | Midline curvature parameters: control points of a deformable spline from head to tail |
| $s \in \mathbb{R}^+$ | Body scale (overall length; cross-sectional proportions are species-fixed) |

The fish mesh $\mathcal{M}(\mathbf{S})$ is generated by sweeping species-specific cross-sectional ellipses along the midline spline. Each cross-section is locally symmetric (left-right across the body axis), which holds even during turning maneuvers. Global bilateral symmetry is **not** assumed.

---

## Phase 0: Calibration (Offline, Pre-Existing)

Assumed complete. The custom calibration library provides:

- Per-camera intrinsics (including distortion, particularly important for the wide-angle center camera).
- Extrinsics in a common world frame.
- Flat-plane refractive model parameters (air-glass-water interface geometry).
- The refractive projection function $\Pi_{\text{ref}}$: maps a 3D world point to a 2D image point via Snell's law at the flat air-water interface. Implemented in AquaKit with clean, differentiable gradients. Newton-Raphson solver with bisection fallback at geometric extremes.

---

## Phase I: Multi-View Instance Segmentation

### Goal

Produce a binary body mask $\mathbf{M}_i^{(j)}$ for each fish $j$ in each camera $i$, per frame. Fins are excluded; only the compact body is segmented.

### Method

1. **Base model**: Start with SAM 2 (`segment-anything-2`, Meta) in video mode for temporal consistency, or Mask R-CNN via **Detectron2** (`detectron2`) for per-frame instance segmentation.
2. **Annotation strategy**: Use SAM in zero-shot mode to generate pseudo-labels across many frames. Human annotators **correct** these pseudo-labels rather than annotating from scratch (3–5× faster). Focus correction effort on hard cases: overlapping fish and edge-on views where the silhouette thins. A few hundred corrected frames per camera viewpoint should suffice.
3. **Fine-tuning**: Train a species-specific instance segmentation model on the corrected dataset. Detectron2 with a ResNet-50-FPN backbone is a practical default.
4. **Output per camera per frame**: Binary masks $\mathbf{M}_i^{(j)}$ and coarse 2D keypoints (head centroid, body centroid, tail tip) extracted from mask geometry (e.g., PCA of mask pixels for major axis, endpoints for head/tail).

### Libraries

| Library | Role |
|---|---|
| `segment-anything-2` | Zero-shot pseudo-label generation |
| `detectron2` | Instance segmentation model training and inference |
| `supervision` (Roboflow) | Annotation format conversion, mask utilities, visualization |
| `labelstudio` or `cvat` | Human-in-the-loop annotation correction |

### Notes

- The center camera covers the entire tank and always sees all 9 fish, but at lower per-fish resolution. It should still be segmented — its masks are valuable for tracking association even if less useful for silhouette-based pose fitting.
- Segmentation quality is the single largest upstream risk. Invest in annotation quality and model validation before building downstream phases.

---

## Phase II: 3D Initialization

### Goal

Provide a biologically plausible initial estimate of $\mathbf{S}$ for each fish, avoiding local minima in the Phase III optimizer.

### Method: Epipolar Consensus (Primary)

For each detected fish, cast refractive rays from the coarse 2D keypoints (head, center, tail) through $\Pi_{\text{ref}}^{-1}$ (inverse projection) from each camera that sees it. Find the 3D point minimizing total ray distance across views.

**Centroid initialization** — solve for $\mathbf{p}$:

$$\mathbf{p}^* = \arg\min_{\mathbf{p}} \sum_{i \in \text{views}} d(\mathbf{p}, \; \mathbf{r}_i^{\text{center}})^2$$

where $\mathbf{r}_i^{\text{center}}$ is the refracted ray from camera $i$ through the body centroid keypoint, and $d(\mathbf{p}, \mathbf{r})$ is the point-to-ray distance. This is a standard least-squares problem solvable in closed form.

**Heading initialization** — repeat for head and tail keypoints to get $\mathbf{p}_{\text{head}}^*$ and $\mathbf{p}_{\text{tail}}^*$:

$$\psi^* = \text{atan2}(p_{\text{head},y}^* - p_{\text{tail},y}^*, \; p_{\text{head},x}^* - p_{\text{tail},x}^*)$$

**Scale initialization**:

$$s^* = \|\mathbf{p}_{\text{head}}^* - \mathbf{p}_{\text{tail}}^*\|$$

**Curvature initialization**: Set $\boldsymbol{\kappa} = \mathbf{0}$ (straight body) for the first frame; warm-start from the previous frame thereafter.

### Fallback: Local Voxel Carving

When epipolar consensus fails (e.g., ambiguous keypoints, severe occlusion), fall back to a coarse-to-fine voxel carving approach in a bounding box around the estimated centroid.

1. Define a local volume (e.g., 20cm cube) at coarse resolution (5mm voxels).
2. For each voxel $\mathbf{V}$, project through $\Pi_{\text{ref}}$ into all views and check mask occupancy:

$$\text{Occ}(\mathbf{V}) = \begin{cases} 1 & \text{if } \sum_{i} \mathbb{1}\!\left(\Pi_{\text{ref}}(\mathbf{V}) \in \mathbf{M}_i\right) \geq 3 \\ 0 & \text{otherwise} \end{cases}$$

3. Refine occupied regions at 1–2mm resolution.
4. Fit the midline spline to the carved volume via PCA of occupied voxels.

### After Frame 1: Warm Start

After the first frame, initialization is trivially handled by using the previous frame's optimized $\mathbf{S}$ as the starting point. At 30 fps with typical swimming speeds, fish move ~1–3mm per frame (<3% of body length), so the previous solution is an excellent seed. Epipolar consensus and voxel carving are only needed for the first frame of each track, or after a track is lost and re-acquired.

### Libraries

| Library | Role |
|---|---|
| `scipy.optimize.least_squares` | Epipolar consensus point optimization |
| `numpy` | Ray geometry, linear algebra |
| `open3d` | Voxel grid operations (if carving fallback is needed) |

---

## Phase III: Differentiable Pose Refinement

### Goal

Iteratively refine $\mathbf{S}$ for each fish by minimizing a multi-objective loss through gradient descent, using differentiable refractive projection.

### Mesh Generation

Given state $\mathbf{S}$, the fish mesh $\mathcal{M}(\mathbf{S})$ is constructed by:

1. Evaluating the midline spline defined by heading $\psi$ and curvature parameters $\boldsymbol{\kappa}$ at $N$ evenly spaced points.
2. At each point, placing a cross-sectional ellipse with species-fixed height-to-width ratio, scaled by $s$ and a longitudinal profile (wider at mid-body, tapering at head and tail).
3. Connecting adjacent ellipses to form a watertight triangle mesh.

This is implemented in **PyTorch** for end-to-end differentiability.

### Loss Function

$$\mathcal{L}_{\text{total}} = \lambda_{\text{sil}}\,\mathcal{L}_{\text{sil}} + \lambda_{\text{grav}}\,\mathcal{L}_{\text{grav}} + \lambda_{\text{shape}}\,\mathcal{L}_{\text{shape}} + \lambda_{\text{temp}}\,\mathcal{L}_{\text{temp}}$$

#### 1. Refractive Silhouette Loss ($\mathcal{L}_{\text{sil}}$)

Compares the projected 3D mesh silhouette against the observed 2D masks:

$$\mathcal{L}_{\text{sil}} = \sum_{i \in \text{views}} w_i \left(1 - \text{IoU}\!\left(\text{Render}(\mathcal{M}(\mathbf{S}), \Pi_{\text{ref}}^{(i)}), \; \mathbf{M}_i\right)\right)$$

where $\text{Render}$ is a differentiable silhouette renderer (via **PyTorch3D**) composited with the refractive projection from **AquaKit**, and $w_i$ is a per-camera weight.

**Camera weighting**: Weight each camera's contribution by angular diversity relative to the fish. Nearby ring cameras with near-identical viewing angles should not collectively dominate. A practical scheme:

$$w_i = \frac{1}{\max\!\left(\epsilon, \; \sum_{j \neq i} \exp(-\alpha \, \|\mathbf{v}_i - \mathbf{v}_j\|) \right)}$$

where $\mathbf{v}_i$ is the unit viewing direction from camera $i$ to the fish centroid, $\alpha$ controls the angular bandwidth, and $\epsilon$ prevents division by zero. This downweights cameras that are clustered and upweights geometrically isolated views (e.g., the center camera, or ring cameras on the far side of the tank).

#### 2. Gravity Prior ($\mathcal{L}_{\text{grav}}$)

Penalizes deviation from upright orientation (dorsal side up):

$$\mathcal{L}_{\text{grav}} = \|1 - \mathbf{u}_{\text{dorsal}} \cdot \mathbf{u}_{\text{world}}\|^2$$

where $\mathbf{u}_{\text{dorsal}}$ is the fish's dorsal-pointing unit vector and $\mathbf{u}_{\text{world}}$ is the world vertical. This should be a **soft** prior — fish do tilt during turns — with $\lambda_{\text{grav}}$ set low enough to allow 20–30° deviations without heavy penalty.

#### 3. Morphological Constraint ($\mathcal{L}_{\text{shape}}$)

Penalizes deviations of body proportions from species norms. This primarily constrains the dorso-ventral axis where the top-down camera geometry provides the weakest signal.

$$\mathcal{L}_{\text{shape}} = \sum_{k} \left(\frac{h_k(\mathbf{S})}{w_k(\mathbf{S})} - \rho_k\right)^2$$

where $h_k / w_k$ is the height-to-width ratio at cross-section $k$ and $\rho_k$ is the species-specific target ratio. Ideally, $\rho_k$ values are learned from a small set of manually verified 3D reconstructions rather than set from anatomical tables alone.

**Prior strength**: With 25–50° triangulation angles across the tank, the geometry provides real depth signal. The prior supplements rather than replaces geometric information. Set $\lambda_{\text{shape}}$ to be active but not dominant — validate by comparing reconstructions with and without the prior and confirming that depth estimates shift by less than ~5mm.

#### 4. Temporal Smoothness ($\mathcal{L}_{\text{temp}}$)

Penalizes physically implausible accelerations, active once frame-to-frame associations exist:

$$\mathcal{L}_{\text{temp}} = \left\|\mathbf{p}_t - 2\mathbf{p}_{t-1} + \mathbf{p}_{t-2}\right\|^2 + \lambda_{\kappa} \left\|\boldsymbol{\kappa}_t - \boldsymbol{\kappa}_{t-1}\right\|^2$$

The first term penalizes acceleration (second derivative of position); the second penalizes rapid changes in body curvature. Both allow smooth motion while resisting segmentation-error-induced jumps.

**Note on anisotropy**: Given the top-down rig geometry, Z estimates are noisier than X-Y. Consider applying stronger temporal smoothness in Z:

$$\mathcal{L}_{\text{temp}}^{\text{pos}} = \|(\mathbf{p}_t - 2\mathbf{p}_{t-1} + \mathbf{p}_{t-2}) \odot \mathbf{w}_{\text{axis}}\|^2, \quad \mathbf{w}_{\text{axis}} = (1, 1, w_z), \; w_z > 1$$

### Optimization

- **Optimizer**: Adam (via PyTorch) with a per-frame budget of ~50–100 iterations.
- **Warm start**: Initialize from previous frame's solution (primary) or Phase II output (first frame / track recovery).
- **Per-fish parallelism**: Each fish's optimization is independent given fixed masks. Batch across fish on GPU.

### Libraries

| Library | Role |
|---|---|
| `torch` | Autograd, optimizer, tensor operations |
| `pytorch3d` | Differentiable mesh rendering (silhouette mode), mesh data structures |
| AquaKit | Refractive projection $\Pi_{\text{ref}}$ and its Jacobian |
| `kornia` | Differentiable image operations (IoU, morphological ops) if needed |

---

## Phase IV: Tracking and Identity

### Goal

Maintain consistent identity labels for all 9 fish across the full recording, including through close interactions and brief occlusions.

### Architecture: Predict → Associate → Refine

#### Step 1: Motion Prediction (3D Extended Kalman Filter)

Maintain a per-fish EKF with state $[\mathbf{p}, \dot{\mathbf{p}}]$ (position and velocity in 3D).

**Process noise**: Anisotropic — higher uncertainty in Z than X-Y, reflecting the camera geometry:

$$\mathbf{Q} = \text{diag}(\sigma_{xy}^2, \sigma_{xy}^2, \sigma_z^2), \quad \sigma_z > \sigma_{xy}$$

At each frame, the EKF predicts each fish's expected position, producing a predicted centroid $\hat{\mathbf{p}}_t^{(j)}$ and covariance $\hat{\mathbf{P}}_t^{(j)}$ for each track $j$.

#### Step 2: Detection-to-Track Association (Hungarian Algorithm)

Construct a cost matrix $\mathbf{C}$ between predicted track positions and Phase III–optimized detections:

$$C_{jk} = \left(\mathbf{p}_t^{(k)} - \hat{\mathbf{p}}_t^{(j)}\right)^T \left(\hat{\mathbf{P}}_t^{(j)}\right)^{-1} \left(\mathbf{p}_t^{(k)} - \hat{\mathbf{p}}_t^{(j)}\right)$$

This is the Mahalanobis distance, which accounts for direction-dependent uncertainty. Solve the assignment problem via the Hungarian algorithm with a gating threshold (reject associations above a maximum Mahalanobis distance).

**Sex classification boost**: Extract a simple color histogram from the masked region across views. Classify as male or female. Add a large penalty to the cost matrix for associations that would change a track's sex classification:

$$C_{jk}' = C_{jk} + \lambda_{\text{sex}} \cdot \mathbb{1}(\text{sex}(j) \neq \text{sex}(k))$$

#### Step 3: Close-Interaction Handling (Merge-and-Split)

When two or more fish detections become inseparable (e.g., masks merge in enough views that individual pose estimation fails):

1. **Merge**: Combine the involved tracks into a single "interaction event." Record the entry state (positions, velocities, identities) of all participants.
2. **Track the blob**: Maintain a single aggregate detection for the group.
3. **Split**: When individuals re-separate, assign identities using:
   - **Trajectory continuity**: Match exit velocities to entry velocities. Fish exiting an interaction typically maintain momentum consistent with their entry direction.
   - **Sex classification**: If the interacting pair includes a male and a female, assignment is unambiguous.
   - **Confidence flag**: Mark the identity assignment with a confidence score. Same-sex pairs with crossing trajectories get low confidence.

#### Step 4: Global Consistency Check

Enforce the hard constraint that exactly 9 fish exist at all times. If a track is lost and a new detection appears in the same frame window, link them. The topological constraint (fixed population) resolves many apparent identity losses without any re-identification model.

### Libraries

| Library | Role |
|---|---|
| `filterpy` | Extended Kalman Filter implementation |
| `scipy.optimize.linear_sum_assignment` | Hungarian algorithm for association |
| `numpy` | Cost matrix construction, Mahalanobis distance |
| `scikit-learn` | Color histogram features, simple male/female classifier |

---

## Phase V: Output and Visualization

### Per-Frame Output

For each fish $j$ at each frame $t$:

| Field | Type | Description |
|---|---|---|
| `fish_id` | `int` | Global identity label |
| `position` | `float[3]` | Centroid $(x, y, z)$ in mm, world frame |
| `heading` | `float` | Yaw angle $\psi$ in radians |
| `midline` | `float[K, 3]` | 3D midline spline sample points |
| `curvature` | `float[K]` | Midline curvature parameters $\boldsymbol{\kappa}$ |
| `scale` | `float` | Body length in mm |
| `sex` | `str` | Male / Female classification |
| `confidence` | `float` | Identity confidence (reduced during/after merge-split events) |
| `n_cameras` | `int` | Number of cameras contributing to this estimate |
| `silhouette_loss` | `float` | Final $\mathcal{L}_{\text{sil}}$ value (reconstruction quality metric) |

### Storage

Export as HDF5 (via `h5py`) for efficient random access to trajectories, or as a columnar format (`pandas` / `pyarrow`) for analysis. Include per-frame metadata: which cameras saw each fish, merge-split event logs, and per-camera silhouette IoU values for post-hoc quality filtering.

### Visualization

- **2D overlay**: Project optimized 3D meshes back into each camera view via $\Pi_{\text{ref}}$; overlay on original video for visual QA.
- **3D scene**: Render fish meshes in the tank volume in real time using **PyVista** or **rerun** (`rerun-sdk`), with trajectory trails and identity color coding.
- **Trajectory analysis**: 3D trajectory plots, speed/acceleration profiles, inter-fish distance matrices, interaction event timelines.

### Libraries

| Library | Role |
|---|---|
| `h5py` | Trajectory data storage |
| `pandas` / `pyarrow` | Tabular analysis and export |
| `rerun-sdk` | Real-time 3D visualization and debugging |
| `pyvista` | Publication-quality 3D renders |
| `matplotlib` | Trajectory plots, analysis figures |
| `opencv-python` | 2D video overlay rendering |

---

## Implementation Sequencing

The following order prioritizes early validation and avoids building on unverified assumptions:

### Stage 1: Foundation (Validate Geometry)

1. Take a short recording (a few hundred frames) with known conditions.
2. Implement Phase I segmentation (SAM pseudo-labels → manual correction → Detectron2 fine-tuning) on a subset of cameras.
3. Implement Phase II epipolar consensus initialization using AquaKit's refractive rays.
4. Validate: project the estimated 3D centroids back into all camera views. Do they land on the fish? This tests the full calibration + refraction + initialization chain with minimal code.

### Stage 2: Core Reconstruction (Single Fish)

1. Build the parametric fish mesh and differentiable rendering pipeline (Phase III) for a single fish in a subset of frames.
2. Tune loss weights ($\lambda_{\text{sil}}, \lambda_{\text{grav}}, \lambda_{\text{shape}}$) against manually measured ground truth (e.g., fish position estimated by hand in multiple views).
3. Benchmark: reconstruction accuracy vs. triangulation-of-keypoints baseline. The analysis-by-synthesis approach should win on depth precision — if it doesn't, diagnose before scaling.

### Stage 3: Multi-Fish and Tracking

1. Scale Phase III to all 9 fish per frame.
2. Implement Phase IV EKF + Hungarian matching.
3. Add temporal smoothness loss.
4. Evaluate track continuity: how many identity swaps per minute of video? Where do they occur (spatially)?

### Stage 4: Robustness and Identity

1. Implement merge-and-split interaction handling.
2. Add sex classification and integrate into association cost.
3. Add global population constraint (exactly 9 fish).
4. Stress-test on high-interaction sequences.

### Stage 5: Scale and Polish

1. Optimize runtime (GPU batching, caching of refractive projections).
2. Full-recording processing pipeline with checkpointing.
3. Visualization and export tooling.
4. Documentation and reproducibility.
