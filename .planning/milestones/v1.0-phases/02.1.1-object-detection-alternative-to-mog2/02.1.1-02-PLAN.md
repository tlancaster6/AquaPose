---
phase: 02.1.1-object-detection-alternative-to-mog2
plan: 02
type: execute
wave: 2
depends_on:
  - 02.1.1-01
files_modified:
  - pyproject.toml
  - src/aquapose/segmentation/detector.py
  - src/aquapose/segmentation/__init__.py
  - scripts/train_yolo.py
  - scripts/eval_yolo_vs_mog2.py
  - tests/unit/segmentation/test_detector.py
autonomous: false
requirements:
  - SEG-01
  - SEG-04
must_haves:
  truths:
    - "YOLOv8n model is trained on the 150-frame fish dataset and weights are saved"
    - "YOLODetector class produces Detection objects interchangeable with MOG2Detector output"
    - "YOLO detections feed into SAM2 pseudo-labeler without modification"
    - "YOLO recall beats MOG2 recall on the same 30-frame validation set"
    - "Runtime detector selection via make_detector('mog2') or make_detector('yolo', model_path=...)"
    - "Side-by-side visual comparison of YOLO vs MOG2 boxes exists for validation frames"
  artifacts:
    - path: "src/aquapose/segmentation/detector.py"
      provides: "YOLODetector class and make_detector factory"
      contains: "class YOLODetector"
    - path: "scripts/train_yolo.py"
      provides: "YOLO training script"
    - path: "scripts/eval_yolo_vs_mog2.py"
      provides: "Comparative evaluation script with visual output"
    - path: "tests/unit/segmentation/test_detector.py"
      provides: "Unit tests for YOLODetector"
  key_links:
    - from: "src/aquapose/segmentation/detector.py"
      to: "ultralytics"
      via: "Lazy import of YOLO in YOLODetector.__init__"
      pattern: "from ultralytics import YOLO"
    - from: "scripts/eval_yolo_vs_mog2.py"
      to: "src/aquapose/segmentation/detector.py"
      via: "Uses both MOG2Detector and YOLODetector on same frames"
      pattern: "make_detector"
    - from: "src/aquapose/segmentation/detector.py"
      to: "src/aquapose/segmentation/pseudo_labeler.py"
      via: "YOLODetector.detect() returns list[Detection] compatible with SAMPseudoLabeler.predict()"
      pattern: "Detection\\(bbox=.*mask=.*area=.*confidence="
---

<objective>
Add `ultralytics` dependency, implement `YOLODetector` class in `detector.py`, train YOLOv8n on the annotated dataset from Plan 01, evaluate recall vs MOG2, and wire YOLO into the pipeline as a runtime-configurable alternative detector.

Purpose: Replace MOG2 (which misses stationary fish and over-splits) with a learned detector that should achieve higher recall on the validation set.
Output: Trained YOLO weights, `YOLODetector` class, `make_detector()` factory, comparison eval results.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02.1.1-object-detection-alternative-to-mog2/02.1.1-CONTEXT.md
@.planning/phases/02.1.1-object-detection-alternative-to-mog2/02.1.1-RESEARCH.md
@.planning/phases/02.1.1-object-detection-alternative-to-mog2/02.1.1-01-SUMMARY.md
@src/aquapose/segmentation/detector.py
@src/aquapose/segmentation/__init__.py
@tests/unit/segmentation/test_detector.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add ultralytics dependency, implement YOLODetector and make_detector factory</name>
  <files>
    pyproject.toml
    src/aquapose/segmentation/detector.py
    src/aquapose/segmentation/__init__.py
    tests/unit/segmentation/test_detector.py
  </files>
  <action>
1. **pyproject.toml**: Add `"ultralytics>=8.0"` to the `dependencies` array.

2. **src/aquapose/segmentation/detector.py**: Add `YOLODetector` class and `make_detector` factory function after the existing `MOG2Detector` class.

   `YOLODetector` implementation (follow the pattern from RESEARCH.md Pattern 2):
   - `__init__(self, model_path: str | Path, conf_threshold: float = 0.25, padding_fraction: float = 0.15)` — lazy-import `from ultralytics import YOLO` inside `__init__` (consistent with SAM2 lazy import pattern). Store `self._model = YOLO(str(model_path))`.
   - `detect(self, frame: np.ndarray) -> list[Detection]` — call `self._model.predict(frame, conf=self._conf, verbose=False)`, iterate over `results[0].boxes`, extract xyxy coordinates, apply padding (clamped to frame bounds), construct full-frame mask (zeros with bbox region filled to 255), compute area, build `Detection(bbox=(x,y,w,h), mask=mask, area=area, confidence=conf)`.
   - Add `from pathlib import Path` import at top of file.

   `make_detector` factory:
   ```python
   def make_detector(kind: str, **kwargs: Any) -> MOG2Detector | YOLODetector:
       """Create a fish detector by name.

       Args:
           kind: Detector type — ``"mog2"`` or ``"yolo"``.
           **kwargs: Forwarded to the detector constructor. For ``"yolo"``,
               ``model_path`` is required.

       Returns:
           Configured detector instance.

       Raises:
           ValueError: If *kind* is not recognized.
       """
       if kind == "mog2":
           return MOG2Detector(**kwargs)
       if kind == "yolo":
           return YOLODetector(**kwargs)
       raise ValueError(f"Unknown detector kind: {kind!r}")
   ```
   Add `from typing import Any` to imports.

3. **src/aquapose/segmentation/__init__.py**: Add `YOLODetector` and `make_detector` to imports and `__all__`.

4. **tests/unit/segmentation/test_detector.py**: Add tests for `YOLODetector`:
   - Test `YOLODetector` can be instantiated with a mock model path (monkeypatch `ultralytics.YOLO` to return a mock that produces empty results).
   - Test `detect()` returns `list[Detection]` with correct bbox format `(x, y, w, h)`, full-frame mask shape, and confidence in [0, 1].
   - Test `make_detector("mog2")` returns `MOG2Detector` instance.
   - Test `make_detector("yolo", model_path="dummy.pt")` returns `YOLODetector` instance (with mocked YOLO).
   - Test `make_detector("bad")` raises `ValueError`.
   - Use `@pytest.fixture` and `monkeypatch` for YOLO mock — do NOT require actual ultralytics model weights.

Run `hatch run test` to verify all tests pass.
  </action>
  <verify>
`hatch run test` passes (all existing + new tests).
`python -c "from aquapose.segmentation import YOLODetector, make_detector; print('OK')"` succeeds.
  </verify>
  <done>
YOLODetector class and make_detector factory exist in detector.py, are exported from the segmentation package, and unit tests pass with mocked ultralytics.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create YOLO training script, evaluation script, and run training</name>
  <files>
    scripts/train_yolo.py
    scripts/eval_yolo_vs_mog2.py
  </files>
  <action>
1. **scripts/train_yolo.py**: Training script that wraps `model.train()`.

   CLI arguments:
   - `--data` (default `data/yolo_fish/dataset.yaml`)
   - `--model` (default `yolov8n.pt` — COCO pretrained)
   - `--epochs` (default 100)
   - `--imgsz` (default 1600)
   - `--batch` (default 2)
   - `--patience` (default 20)
   - `--project` (default `output/yolo_fish`)
   - `--name` (default `train_v1`)
   - `--device` (default `0`)

   Implementation:
   ```python
   from ultralytics import YOLO
   model = YOLO(args.model)
   results = model.train(
       data=str(Path(args.data).resolve()),
       epochs=args.epochs,
       imgsz=args.imgsz,
       batch=args.batch,
       device=args.device,
       patience=args.patience,
       single_cls=True,
       augment=True,
       project=args.project,
       name=args.name,
   )
   ```
   Print path to best weights at the end: `{project}/{name}/weights/best.pt`.

2. **scripts/eval_yolo_vs_mog2.py**: Comparative evaluation script.

   CLI arguments:
   - `--yolo-weights` (path to trained YOLO best.pt)
   - `--data` (default `data/yolo_fish/dataset.yaml`)
   - `--video-dir` (path to per-camera video files, for MOG2 warmup)
   - `--output-dir` (default `output/yolo_eval/`)
   - `--warmup-frames` (default 200)
   - `--center-camera` (center camera ID)

   Implementation:
   a. Load the validation image list from `data/yolo_fish/images/val/`.
   b. Load corresponding ground-truth bounding boxes from `data/yolo_fish/labels/val/`.
   c. For each val image:
      - Run `YOLODetector(model_path=weights).detect(frame)` → YOLO detections.
      - Run `MOG2Detector().detect(frame)` → MOG2 detections. (Note: MOG2 needs video context. For fair comparison, warm up MOG2 on frames preceding the val frame from the same camera's video. If warmup is impractical for every val frame, run MOG2 without warmup and note this limitation.)
      - Match detections to GT boxes using IoU >= 0.5 threshold.
      - Track per-frame TP, FP, FN for both detectors.
   d. Compute aggregate recall and precision for both YOLO and MOG2.
   e. Save side-by-side annotated images to `--output-dir`: each image shows GT boxes (green), YOLO boxes (blue), MOG2 boxes (red) overlaid.
   f. Print summary table:
      ```
      Detector | Recall | Precision | TP | FP | FN
      ---------|--------|-----------|----|----|---
      YOLO     | 0.XXX  | 0.XXX     | XX | XX | XX
      MOG2     | 0.XXX  | 0.XXX     | XX | XX | XX
      ```

   Note on MOG2 comparison: MOG2 requires temporal context (video warmup) to work properly. For val frames extracted from video, the script should load frames from the source video, warm up MOG2 on preceding frames, then run detection on the target frame. Use the camera ID parsed from the filename (format `{camera_id}_frame_{frame_idx:06d}.jpg`) to find the source video and frame index.

After creating scripts, run training:
```bash
python scripts/train_yolo.py
```

If OOM at `imgsz=1600, batch=2`, retry with `--imgsz 1280` or `--batch 1`. Document the final resolution used.

After training completes, run evaluation:
```bash
python scripts/eval_yolo_vs_mog2.py --yolo-weights output/yolo_fish/train_v1/weights/best.pt --video-dir /path/to/videos --center-camera e3v8340
```
  </action>
  <verify>
`python scripts/train_yolo.py --help` shows correct arguments.
`python scripts/eval_yolo_vs_mog2.py --help` shows correct arguments.
Training produces weights at `output/yolo_fish/train_v1/weights/best.pt`.
Evaluation prints recall/precision comparison table and saves annotated images to `output/yolo_eval/`.
  </verify>
  <done>
YOLO model trained on 150-frame dataset. Evaluation shows recall and precision for both YOLO and MOG2 on the same 30-frame val set. Side-by-side annotated images saved for visual comparison.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 3: Verify YOLO recall beats MOG2 and visual quality is acceptable</name>
  <files>output/yolo_eval/</files>
  <action>
This is a human verification checkpoint. Claude has automated YOLO training and evaluation. The user reviews results.

What was built:
- Trained YOLOv8n fish detector on the 150-frame annotated dataset
- Comparative evaluation against MOG2 on the 30-frame stratified validation set
- Side-by-side annotated images in `output/yolo_eval/`

Steps for user:
1. Check the evaluation summary table printed by `eval_yolo_vs_mog2.py`:
   - YOLO recall should be higher than MOG2 recall (success criterion).
   - Check precision — if very low, may cause false detections downstream.

2. Review side-by-side annotated images in `output/yolo_eval/`:
   - Green boxes = ground truth, Blue = YOLO, Red = MOG2.
   - Verify YOLO catches stationary fish that MOG2 misses.
   - Verify YOLO does not massively over-detect.

3. If YOLO recall is NOT higher than MOG2:
   - Consider training with `yolov8s.pt` (larger model) or adding more frames.
   - Report the issue for re-training with adjusted parameters.
  </action>
  <verify>
Evaluation table shows YOLO recall > MOG2 recall on the 30-frame val set.
Side-by-side images visually confirm YOLO detects fish MOG2 misses.
  </verify>
  <done>
YOLO recall exceeds MOG2 recall on stratified val set. Visual quality approved.
Type "approved" or describe issues to address.
  </done>
</task>

</tasks>

<verification>
- `ultralytics` is listed in `pyproject.toml` dependencies
- `YOLODetector` class in `detector.py` returns `list[Detection]` with correct field shapes
- `make_detector("yolo", model_path=...)` returns a `YOLODetector`
- `make_detector("mog2")` returns a `MOG2Detector`
- `hatch run test` passes (including new YOLODetector unit tests)
- `output/yolo_fish/train_v1/weights/best.pt` exists (trained model)
- `output/yolo_eval/` contains side-by-side comparison images
- Evaluation table shows YOLO recall > MOG2 recall
</verification>

<success_criteria>
YOLOv8n is trained, achieves higher recall than MOG2 on the stratified 30-frame val set, and is wired into the segmentation package as a runtime-configurable alternative detector via `make_detector()`.
</success_criteria>

<output>
After completion, create `.planning/phases/02.1.1-object-detection-alternative-to-mog2/02.1.1-02-SUMMARY.md`
</output>
